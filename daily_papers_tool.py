import os
os.environ['OPENAI_API_KEY'] = 'Your-API-Key'
import sys
import argparse
from datetime import datetime

# Add current directory to path to import local modules
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from fetch_papers import fetch_daily_papers
from download_papers import download_all_papers
from summarize_papers import extract_text_from_pdf, summarize_paper

def generate_markdown_report(summaries, date_str, model_name):
    """Generates a comprehensive Markdown report from the paper summaries."""
    report = f"# Daily Hugging Face Paper Digest - {date_str}\n\n"
    report += f"This report was automatically generated by the Manus AI tool on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} using model `{model_name}`.\n\n"
    report += "## Summary of Papers\n\n"
    
    for i, summary_data in enumerate(summaries, 1):
        paper = summary_data['paper']
        summary = summary_data['summary']
        
        report += f"--- \n\n"
        report += f"## {i}. {paper['title']}\n\n"
        report += f"**Authors:** {', '.join(paper['authors'])}\n\n"
        report += f"**Published:** {paper['published_at'][:10]}\n\n"
        
        report += summary.strip() + "\n\n"
        
        links = [
            ("Hugging Face", paper.get('hf_url', 'N/A')),
            ("ArXiv Abstract", paper.get('arxiv_url', 'N/A')),
            ("PDF Download", paper.get('pdf_url', 'N/A'))
        ]
        
        github_link = "N/A"
        for line in summary.split('\n'):
            if 'Github:' in line:
                github_link = line.split('Github:')[1].strip()
                break
        if github_link != "N/A":
            links.append(("GitHub", github_link))

        report += "### Related Links\n\n"
        report += "| Platform | Link |\n"
        report += "| :--- | :--- |\n"
        for platform, link in links:
            if platform == "GitHub" and "Github:" in link:
                link = link.replace("Github:", "").strip()
            
            if link.startswith('http'):
                report += f"| {platform} | [{link}]({link}) |\n"
            else:
                report += f"| {platform} | {link} |\n"
        report += "\n"
        
    return report

def run_daily_digest(date_str, model="gpt-4.1-mini"):
    """Main function to run the daily paper digest process."""
    print(f"--- Starting Daily Paper Digest for {date_str} using {model} ---")
    
    # 1. Get the paper list
    print("Phase 1: Fetching paper list...")
    papers = fetch_daily_papers(date_str)
    if not papers:
        print("No papers found or error in fetching. Exiting.")
        return
    print(f"Found {len(papers)} papers.")
    
    # 2. Download all the papers into a date-specific folder
    print(f"Phase 2: Downloading PDFs into papers/{date_str}...")
    download_dir = os.path.join("papers", date_str)
    download_all_papers(papers, download_dir=download_dir)
    
    # 3 & 4. Read and Summarize each paper
    print("Phase 3 & 4: Reading and Summarizing papers...")
    summaries = []
    for i, paper in enumerate(papers):
        print(f"Processing paper {i+1}/{len(papers)}: {paper['title']}")
        
        if 'local_path' not in paper or not os.path.exists(paper['local_path']):
            print(f"Skipping paper {paper['id']}: PDF not found.")
            continue
            
        text = extract_text_from_pdf(paper['local_path'])
        summary_text = summarize_paper(paper, text, model=model)
        
        summaries.append({
            'paper': paper,
            'summary': summary_text
        })
        
        # Delete the PDF file after processing
        try:
            os.remove(paper['local_path'])
            print(f"Deleted temporary file: {paper['local_path']}")
        except Exception as e:
            print(f"Error deleting file {paper['local_path']}: {e}")
        
    # 5. Make a comprehension Markdown Report
    print("Phase 5: Generating Markdown Report...")
    markdown_report = generate_markdown_report(summaries, date_str, model)
    
    if not os.path.exists("summaries"):
        os.makedirs("summaries", exist_ok=True)
    report_filename = f"summaries/daily_digest_{date_str}.md"
    with open(report_filename, 'w') as f:
        f.write(markdown_report)
        
    # Clean up the date folder if empty
    try:
        if os.path.exists(download_dir) and not os.listdir(download_dir):
            os.rmdir(download_dir)
            print(f"Removed empty directory: {download_dir}")
    except Exception as e:
        print(f"Error removing directory {download_dir}: {e}")
        
    print(f"--- Digest Complete. Report saved to {report_filename} ---")
    return report_filename

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Hugging Face Daily Paper Digest Tool")
    parser.add_argument("--date", type=str, help="Date in YYYY-MM-DD format", default=datetime.now().strftime("%Y-%m-%d"))
    parser.add_argument("--model", type=str, choices=["gpt-4.1-mini", "gemini-2.5-flash"], default="gpt-4.1-mini", 
                        help="LLM model to use for summarization")
    
    args = parser.parse_args()
    run_daily_digest(args.date, model=args.model)
