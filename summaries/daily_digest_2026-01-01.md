# Daily Hugging Face Paper Digest - 2026-01-01

This report was automatically generated by the Manus AI tool on 2026-01-04 17:01:48 using model `gemini-2.5-flash`.

## Summary of Papers

--- 

## 1. mHC: Manifold-Constrained Hyper-Connections

**Authors:** Zhenda Xie, Yixuan Wei, Huanqi Cao, Chenggang Zhao, Chengqi Deng, Jiashi Li, Damai Dai, Huazuo Gao, Jiang Chang, Liang Zhao, Shangyan Zhou, Zhean Xu, Zhengyan Zhang, Wangding Zeng, Shengding Hu, Yuqing Wang, Jingyang Yuan, Lean Wang, Wenfeng Liang

**Published:** 2025-12-31

-   **Main Problem:** Hyper-Connections (HC) improve deep learning model performance by expanding the residual stream width and diversifying connectivity patterns. However, this diversification compromises the crucial identity mapping property of residual connections, leading to severe training instability, restricted scalability, and significant memory access overhead, especially when training at scale.

-   **Main Idea:** The paper proposes Manifold-Constrained Hyper-Connections (mHC) to address these challenges. mHC projects the residual connection space of HC onto a specific manifold to restore the identity mapping property. This is achieved by employing the Sinkhorn-Knopp algorithm to project the `Hres_l` matrices onto the Birkhoff polytope, ensuring they become doubly stochastic matrices. This constraint guarantees that feature means are conserved and signal norms are regularized, preventing vanishing or exploding signals. Furthermore, mHC incorporates rigorous infrastructure optimizations for efficiency, including kernel fusion, mixed precision kernels, selective recomputing, and overlapping communication within the DualPipe schedule.

-   **Main Results:** Empirical experiments show that mHC is effective for large-scale training, delivering tangible performance improvements and superior scalability compared to HC. Large-scale in-house training demonstrates that mHC supports training at scale with exceptional stability, introducing only a 6.7% additional time overhead when the expansion rate `n=4`.

-   **Conclusion & Future Works:** mHC offers a flexible and practical extension of HC, resolving its stability and scalability limitations while retaining its performance advantages. The authors anticipate that mHC will contribute to a deeper understanding of topological architecture design and open up promising directions for the evolution of foundational models.

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.24880](https://huggingface.co/papers/2512.24880) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.24880](https://arxiv.org/abs/2512.24880) |
| PDF Download | [https://arxiv.org/pdf/2512.24880.pdf](https://arxiv.org/pdf/2512.24880.pdf) |

--- 

## 2. Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models

**Authors:** Junru Lu, Jiarui Qin, Lingfeng Qiao, Yinghui Li, Xinyi Dai, Bo Ke, Jianfeng He, Ruizhi Qiao, Di Yin, Xing Sun, Yunsheng Wu, Yinsong Liu, Shuangyin Liu, Mingkong Tang, Haodong Lin, Jiayi Kuang, Fanxu Meng, Xiaojuan Tang, Yunjia Xi, Junjie Huang, Haotong Yang, Zhenyi Shen, Yangning Li, Qianwen Zhang, Yifei Yu, Siyu An, Junnan Dong, Qiufeng Wang, Jie Wang, Keyu Chen, Wei Wen, Taian Guo, Zhifeng Shen, Daohai Yu, Jiahao Li, Ke Li, Zongyi Li, Xiaoyu Tan

**Published:** 2025-12-31

-   **Main Problem:** Current Large Language Models (LLMs) achieving state-of-the-art performance are typically very large, leading to substantial computational, financial, and environmental costs that limit their accessibility and real-world deployment, especially in resource-constrained settings. Existing lightweight LLMs (sub-2B parameters) often lack robust reasoning, planning, and generalization capabilities for complex "agentic" tasks because they rely on distillation or post-hoc alignment rather than systematically cultivating intrinsic cognitive abilities during pre-training.

-   **Main Idea:** The Youtu-LLM project introduces a lightweight (1.96B parameters) language model pre-trained from scratch to natively unlock agentic intelligence. The core idea is that strong agentic performance in lightweight LLMs is achievable by injecting agent-oriented signals early and systematically through a principled agentic pre-training process. Key advancements include: 1) a compact Multi-Latent Attention architecture with a STEM-oriented vocabulary supporting a 128k context window; 2) a multi-stage training curriculum (Commonsense-STEM-Agent) on an 11T token corpus, progressively focusing on STEM and agentic tasks; and 3) scalable agentic mid-training utilizing diverse, high-quality synthesized trajectory data (e.g., math, coding, tool-use, deep research) to internalize planning and reflection behaviors, including a refined Agentic-CoT paradigm.

-   **Main Results:** Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. It demonstrates competitive performance against much larger models on general benchmarks and significantly surpasses existing SOTA baselines on agent-specific tasks. The research provides the first systematic evidence that agentic pre-training can effectively unlock native agentic potential in lightweight LLMs, revealing phenomena such as the scalable growth of agent capabilities.

-   **Conclusion & Future Works:** The paper concludes that lightweight models can possess strong intrinsic agentic capabilities when trained with a principled, agent-centric pre-training paradigm. Youtu-LLM successfully harmonizes high computational efficiency with native agentic intelligence, demonstrating that pre-training can lead to scalable growth of agentic abilities in smaller models. No specific future works are detailed in the provided text.

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.24618](https://huggingface.co/papers/2512.24618) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.24618](https://arxiv.org/abs/2512.24618) |
| PDF Download | [https://arxiv.org/pdf/2512.24618.pdf](https://arxiv.org/pdf/2512.24618.pdf) |

--- 

## 3. Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem

**Authors:** Weixun Wang, XiaoXiao Xu, Wanhe An, Fangwen Dai, Wei Gao, Yancheng He, Ju Huang, Qiang Ji, Hanqi Jin, Xiaoyang Li, Yang Li, Zhongwen Li, Shirong Lin, Jiashun Liu, Zenan Liu, Tao Luo, Dilxat Muhtar, Yuanbin Qu, Jiaqiang Shi, Qinghui Sun, Yingshui Tan, Hao Tang, Runze Wang, Yi Wang, Zhaoguo Wang, Yanan Wu, Shaopan Xiong, Binchen Xu, Xander Xu, Yuchi Xu, Qipeng Zhang, Xixia Zhang, Haizhou Zhao, Jie Zhao, Shuaibing Zhao, Baihui Zheng, Jianhui Zheng, Suhang Zheng, Yanni Zhu, Mengze Cai, Kerui Cao, Xitong Chen, Yue Dai, Lifan Du, Tao Feng, Tao He, Jin Hu, Yijie Hu, Ziyu Jiang, Cheng Li, Xiang Li, Jing Liang, Chonghuan Liu, ZhenDong Liu, Haodong Mi, Yanhu Mo, Junjia Ni, Shixin Pei, Jingyu Shen, XiaoShuai Song, Cecilia Wang, Chaofan Wang, Kangyu Wang, Pei Wang, Tao Wang, Wei Wang, Ke Xiao, Mingyu Xu, Tiange Xu, Nan Ya, Siran Yang, Jianan Ye, Yaxing Zang, Duo Zhang, Junbo Zhang, Boren Zheng, Wanxi Deng, Ling Pan, Lin Qu, Wenbo Su, Jiamang Wang, Wei Wang, Hu Wei, Minggang Wu, Cheng Yu, Bing Zhao, Zhicheng Zheng, Bo Zheng

**Published:** 2025-12-31

Here's a summary of the research paper:

-   **Main Problem:** Existing Large Language Models (LLMs) struggle with "agentic crafting" – complex, multi-turn tasks requiring iterative refinement and real-world interaction with environments. This is largely due to the absence of a scalable, end-to-end agentic ecosystem that can effectively manage data generation, agent execution, and policy optimization for such tasks.

-   **Main Idea:** The paper introduces the **Agentic Learning Ecosystem (ALE)**, a foundational, full-stack infrastructure designed to streamline the end-to-end production pipeline for agent LLMs. ALE comprises three synergistic components:
    1.  **ROLL (Reinforcement Learning Optimization for Large-Scale Learning):** A scalable RL training framework for post-training and policy optimization.
    2.  **ROCK (Reinforcement Open Construction Kit):** A secure, sandboxed environment execution engine for interaction trajectory synthesis, execution, and validation.
    3.  **iFlow CLI:** An agent framework for configurable and efficient context engineering and managing real-world workflows.
    Grounded in ALE, the authors also release **ROME (ROME is Obviously an Agentic ModEl)**, an open-source agent LLM trained on over one million trajectories using novel data composition protocols (spanning isolated snippets to complex agentic behaviors with built-in verification) and a new policy optimization algorithm called **IPA (Interaction-Perceptive Agentic Policy Optimization)**, which assigns credit over semantic interaction chunks for improved training stability. They also propose **Terminal Bench Pro** for more rigorous evaluation.

-   **Main Results:** ROME demonstrates strong performance across diverse agentic benchmarks:
    *   Achieved 24.72% on Terminal-Bench 2.0 and 57.40% accuracy on SWE-bench Verified, outperforming similarly sized models and rivaling models with over 100B parameters.
    *   Maintained competitive performance on the newly introduced and more rigorous Terminal Bench Pro, showcasing strong generalization and stability.
    *   Successfully deployed in production via iFlow CLI, validating its practical effectiveness and the robustness of ALE.

-   **Conclusion & Future Works:** The paper presents ALE as a reliable, cost-effective, secure, and user-friendly training ecosystem for building customized agent models. Beyond a technical stack, the authors advocate for a shift in community priorities towards the co-design of training infrastructure, executable environments, and evaluation protocols. They hope this work will catalyze collaborative efforts to establish standardized benchmarks, execution environments, and reproducible training pipelines, which are deemed essential for the next generation of general-purpose agents.

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.24873](https://huggingface.co/papers/2512.24873) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.24873](https://arxiv.org/abs/2512.24873) |
| PDF Download | [https://arxiv.org/pdf/2512.24873.pdf](https://arxiv.org/pdf/2512.24873.pdf) |

--- 

## 4. A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers

**Authors:** Mohammad Nasirzadeh, Jafar Tahmoresnezhad, Parviz Rashidi-Khazaee

**Published:** 2025-12-29

Here is a summary of the research paper based on the provided text:

-   **Main Problem:**
    Log anomaly detection is crucial for system security, but existing methods face significant challenges. Unimodal methods ignore diverse log data modalities, losing valuable information. Multimodal methods, while attempting to use multiple modalities, often struggle with handling interactions between modalities, high dimensionality, noise sensitivity, data complexity, feature compatibility, network complexity, balancing contributions, lack of shared information, redundancy, and limited improvement. Additionally, most existing work focuses on detecting only one type of log anomaly (point or collective) and lacks a unified framework for both.

-   **Main Idea:**
    The paper proposes **CoLog**, a unified framework for detecting both point and collective anomalies in operating system logs. Inspired by multimodal sentiment analysis, CoLog collaboratively encodes logs by interpreting anomalies as "negative sentiments" and normal samples as "positive sentiments." It utilizes **collaborative transformers** and **multi-head impressed attention** to learn interactions among semantic and sequence log modalities. To manage the heterogeneity from these interactions, CoLog incorporates a **modality adaptation layer**. This approach allows CoLog to learn nuanced patterns and dependencies, providing an end-to-end supervised solution that explicitly learns relationships between modalities.

-   **Main Results:**
    CoLog demonstrates superiority over existing state-of-the-art methods. Across seven benchmark datasets for log-based anomaly detection, CoLog achieved a mean precision of 99.63%, a mean recall of 99.59%, and a mean F1 score of 99.61% for detecting both point and collective anomalies.

-   **Conclusion & Future Works:**
    CoLog represents a significant advancement in log anomaly detection, offering a sophisticated and effective solution to the complex challenges of detecting both point and collective anomalies through a unified framework. Its comprehensive detection capabilities make it highly suitable for cybersecurity, system monitoring, and operational efficiency. The implementation is also provided publicly. (The provided text cuts off before explicit future work is detailed).

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.23380](https://huggingface.co/papers/2512.23380) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.23380](https://arxiv.org/abs/2512.23380) |
| PDF Download | [https://arxiv.org/pdf/2512.23380.pdf](https://arxiv.org/pdf/2512.23380.pdf) |

--- 

## 5. GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction

**Authors:** Yi-Chuan Huang, Hao-Jen Chien, Chin-Yang Lin, Ying-Huan Chen, Yu-Lun Liu

**Published:** 2025-12-31

-   **Main Problem:** Reconstructing high-quality 3D scenes from a limited number of input views (sparse views) remains challenging. Existing diffusion-based methods for sparse-view 3D reconstruction, which often generate novel views to augment data, suffer from three critical limitations: inadequate scene coverage beyond known view peripheries (leading to holes and ghosting), geometric inconsistencies across generated views, and computationally expensive pipelines.

-   **Main Idea:** The paper introduces GaMO (Geometry-aware Multi-view Diffusion Outpainting), a framework that reformulates sparse-view reconstruction by employing *multi-view outpainting* instead of generating entirely new viewpoints. GaMO expands the Field of View (FOV) of existing input camera poses, which inherently preserves geometric consistency while providing broader scene coverage. The method leverages multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner (without training). The pipeline involves three stages: (a) Coarse 3D Initialization using an initial 3DGS model to obtain geometry priors (opacity mask, coarse render), (b) GaMO, which uses these priors to guide a multi-view diffusion model to generate outpainted views with enlarged FOV, and (c) Refined Reconstruction, where these outpainted views are used to enhance the final 3D Gaussian Splatting (3DGS) reconstruction.

-   **Main Results:** GaMO achieves state-of-the-art reconstruction quality on Replica and ScanNet++ datasets across 3, 6, and 9 input views. It significantly outperforms prior methods in quantitative metrics like PSNR and LPIPS, effectively mitigating artifacts such as holes, ghosting, and inconsistent geometry. Furthermore, GaMO demonstrates a substantial computational advantage, achieving a 25x speedup over state-of-the-art diffusion-based methods, completing the reconstruction process in under 10 minutes. The paper's motivation section quantitatively shows that outpainting consistently improves both geometric (SSIM) and perceptual quality (LPIPS), unlike adding more novel views, which can degrade quality.

-   **Conclusion & Future Works:** The paper concludes that multi-view outpainting, as demonstrated by GaMO, is a superior paradigm for sparse-view 3D reconstruction. It effectively eliminates common issues like holes, ghosting artifacts, and geometric inconsistencies that plague existing methods, providing a fast, geometry-aware, and zero-shot approach. The extracted text does not explicitly detail future works, focusing instead on establishing the superiority and efficiency of the proposed GaMO method.

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.25073](https://huggingface.co/papers/2512.25073) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.25073](https://arxiv.org/abs/2512.25073) |
| PDF Download | [https://arxiv.org/pdf/2512.25073.pdf](https://arxiv.org/pdf/2512.25073.pdf) |

--- 

## 6. GR-Dexter Technical Report

**Authors:** Ruoshi Wen, Guangzeng Chen, Zhongren Cui, Min Du, Yang Gou, Zhigang Han, Liqun Huang, Mingyu Lei, Yunfei Li, Zhuohang Li, Wenlei Liu, Yuxiao Liu, Xiao Ma, Hao Niu, Yutao Ouyang, Zeyu Ren, Haixin Shi, Wei Xu, Haoxiang Zhang, Jiajun Zhang, Xiao Zhang, Liwei Zheng, Weiheng Zhong, Yifei Zhou, Zhengming Zhu, Hang Li

**Published:** 2025-12-30

-   **Main Problem:** Current vision-language-action (VLA) models for robot manipulation are mostly limited to gripper-based systems. Scaling these VLA policies to bimanual robots with high degree-of-freedom (DoF) dexterous hands presents significant challenges due to the expanded action space, frequent hand-object occlusions, and the high cost and difficulty of collecting diverse, high-quality real-robot data for such complex systems.

-   **Main Idea:** The paper introduces GR-Dexter, a comprehensive framework encompassing hardware, model, and data for VLA-based generalist manipulation using a bimanual dexterous-hand robot. This includes:
    1.  **Hardware Design:** A compact, 21-DoF anthropomorphic robotic hand called ByteDexter V2, featuring integrated actuators, tactile sensors, and an additional thumb DoF for enhanced dexterity. This hand is deployed on a bimanual system using two Franka Research 3 arms, resulting in a 56-DoF robot.
    2.  **Data Collection:** An intuitive bimanual teleoperation system (using a Meta Quest VR headset and Manus gloves) designed to efficiently collect real-robot trajectories for complex dexterous and bimanual tasks.
    3.  **Training Recipe:** A VLA model based on a Mixture-of-Transformer architecture (GR-Dexter, 4B parameters) which is co-trained on a diverse "data pyramid" including teleoperated robot trajectories, large-scale web vision-language data, carefully curated cross-embodiment robot datasets, and human trajectory data, with a unified preprocessing and retargeting pipeline to handle diverse data sources.

-   **Main Results:** GR-Dexter demonstrates strong performance in real-world evaluations. It achieves:
    *   **High In-Domain Performance:** Successfully completes long-horizon everyday manipulation tasks (e.g., "Bread Serving with Tongs," "Makeup Table Decluttering").
    *   **Improved Generalization:** Shows robustness to out-of-distribution scenarios, including novel objects and previously unseen language instructions (e.g., "pick up the darkest object").
    *   This robust performance and generalization are attributed to the co-training approach leveraging diverse data sources.

-   **Conclusion & Future Works:** GR-Dexter represents a practical step towards achieving generalist dexterous-hand robotic manipulation, showcasing the feasibility of scaling VLA policies to complex bimanual systems. The paper does not explicitly detail future work in the provided extract, but the overall aim is to advance general-purpose operation in cluttered, human-centered environments.

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.24210](https://huggingface.co/papers/2512.24210) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.24210](https://arxiv.org/abs/2512.24210) |
| PDF Download | [https://arxiv.org/pdf/2512.24210.pdf](https://arxiv.org/pdf/2512.24210.pdf) |

--- 

## 7. AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents

**Authors:** Jiafeng Liang, Hao Li, Chang Li, Jiaqi Zhou, Shixin Jiang, Zekun Wang, Changkai Ji, Zhihao Zhu, Runxuan Liu, Tao Ren, Jinlan Fu, See-Kiong Ng, Xia Liang, Ming Liu, Bing Qin

**Published:** 2025-12-29

Summary generation failed.

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.23343](https://huggingface.co/papers/2512.23343) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.23343](https://arxiv.org/abs/2512.23343) |
| PDF Download | [https://arxiv.org/pdf/2512.23343.pdf](https://arxiv.org/pdf/2512.23343.pdf) |

--- 

## 8. PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation

**Authors:** Yuanhao Cai, Kunpeng Li, Menglin Jia, Jialiang Wang, Junzhe Sun, Feng Liang, Weifeng Chen, Felix Juefei-Xu, Chu Wang, Ali Thabet, Xiaoliang Dai, Xuan Ju, Alan Yuille, Ji Hou

**Published:** 2025-12-31

Summary generation failed.

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.24551](https://huggingface.co/papers/2512.24551) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.24551](https://arxiv.org/abs/2512.24551) |
| PDF Download | [https://arxiv.org/pdf/2512.24551.pdf](https://arxiv.org/pdf/2512.24551.pdf) |

--- 

## 9. Pretraining Frame Preservation in Autoregressive Video Memory Compression

**Authors:** Lvmin Zhang, Shengqu Cai, Muyang Li, Chong Zeng, Beijia Lu, Anyi Rao, Song Han, Gordon Wetzstein, Maneesh Agrawala

**Published:** 2025-12-29

-   **Main Problem:** Autoregressive video generation models struggle with handling long-form content due to a critical trade-off between context quality and length. Storing extensive video history (e.g., 60 seconds) leads to prohibitively large context lengths (e.g., 561,600 tokens), creating computational bottlenecks for both training and inference on typical hardware. Existing compression methods often sacrifice either long-range consistency or high-frequency image details, or still incur significant computational overhead.

-   **Main Idea:** The paper proposes a novel framework centered on pretraining a neural network to compress long videos into short contexts while explicitly preserving high-frequency details of individual frames at arbitrary temporal positions. The approach has two main stages:
    1.  **Pretraining Memory Compression Model:** A dedicated model is trained to compress long video histories (e.g., 20 seconds into a ~5k length context) with the objective of maximizing the quality of reconstructing randomly selected frames from that history. This pretraining task uses a retrieval objective to minimize the feature distance between original and reconstructed frames.
    2.  **Fine-tuning for Autoregressive Generation:** The pretrained compression model then serves as an efficient history memory encoder for autoregressive video diffusion models (like Diffusion Transformers, DiTs). This allows the generation models to access long-range context without the prohibitive computational cost.
    The memory compression model itself is a lightweight neural structure incorporating 3D convolutions, SiLU, and attention mechanisms.

-   **Main Results:**
    *   The proposed method effectively compresses a 20-second video into a context length of approximately 5k, a significant reduction.
    *   The pretrained memory encoder demonstrates high-quality reconstruction of arbitrary history frames, as quantitatively measured by PSNR/SSIM.
    *   When integrated into autoregressive video diffusion models, the encoder enables long-range consistency in generated videos.
    *   The framework achieves a balanced and optimized point in the context length-quality trade-off, leading to a practical autoregressive video model with a short context length and perceptually high consistency over long histories.
    *   It significantly reduces video model training costs and facilitates a stronger neural representation for detailed frame retrieval.

-   **Conclusion & Future Works:** The paper successfully introduces a framework that directly addresses the context length-quality trade-off in autoregressive video generation through an explicit pretraining objective focused on frame preservation. This leads to a practical memory compression model that, when fine-tuned, enhances the performance of autoregressive video models by providing efficient long-range context. Future work includes conducting extensive experiments (quantitative, qualitative, ablation studies) to analyze different compression designs and providing pretrained models for broader application in video generation and editing.

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.23851](https://huggingface.co/papers/2512.23851) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.23851](https://arxiv.org/abs/2512.23851) |
| PDF Download | [https://arxiv.org/pdf/2512.23851.pdf](https://arxiv.org/pdf/2512.23851.pdf) |

--- 

## 10. JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation

**Authors:** Kai Liu, Jungang Li, Yuchong Sun, Shengqiong Wu, Jianzhang Gao, Daoan Zhang, Wei Zhang, Sheng Jin, Sicheng Yu, Geng Zhan, Jiayi Ji, Fan Zhou, Liang Zheng, Shuicheng Yan, Hao Fei, Tat-Seng Chua

**Published:** 2025-12-28

-   **Main Problem:** Current multimodal large language models (MLLMs) struggle with the truly unified understanding and generation of *sounding videos* (videos with synchronized audio). Existing approaches either focus on image-text, treat video and audio as separate modalities, or fail to achieve precise spatio-temporal alignment between audio and visual streams during both comprehension and generation, leading to desynchronized outputs and limited effectiveness in complex, temporally-dependent scenarios.

-   **Main Idea:** JavisGPT is introduced as the first unified MLLM specifically designed for joint audio-video (JAV) comprehension and generation of sounding videos. It utilizes a concise encoder-LLM-decoder architecture (based on Qwen2.5-VL-7B-Instruct), integrating a novel **SyncFusion module** for explicit spatio-temporal audio-video feature fusion and synchrony-aware learnable queries. This design allows for unified comprehension of synchronized audio-video content and enables high-quality, synchronized sounding video generation by bridging the LLM's intent to a pretrained JAV-DiT generator. The system is trained using an effective three-stage pipeline (multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning) and supported by **JavisInst-Omni**, a new high-quality dataset of over 200K GPT-4o-curated audio-video-text dialogues covering diverse comprehension and generation scenarios.

-   **Main Results:** Extensive experiments demonstrate that JavisGPT achieves state-of-the-art performance across various JAV comprehension and generation benchmarks. It significantly outperforms existing MLLMs, particularly in tasks requiring complex and temporally synchronized audio-video understanding and generation, producing high-quality and diverse sounding videos with enhanced synchronization.

-   **Conclusion & Future Works:** JavisGPT successfully addresses the critical challenge of unified sounding-video comprehension and generation. Its innovative architecture, including the SyncFusion module for explicit spatio-temporal alignment, and its comprehensive multi-stage training with a dedicated instruction dataset, enable it to achieve superior performance in handling synchronized audio-video content. The paper highlights JavisGPT's effectiveness in both understanding and producing coherent and synchronized sounding videos. (Future work is not explicitly mentioned in the provided text.)

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.22905](https://huggingface.co/papers/2512.22905) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.22905](https://arxiv.org/abs/2512.22905) |
| PDF Download | [https://arxiv.org/pdf/2512.22905.pdf](https://arxiv.org/pdf/2512.22905.pdf) |

--- 

## 11. Scaling Open-Ended Reasoning to Predict the Future

**Authors:** Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping

**Published:** 2025-12-31

-   **Main Problem:** The paper addresses the challenge of training Language Models (LLMs) to perform open-ended forecasting of future world events under uncertainty. Current methods struggle with scaling training data (due to human creation and slow resolution), rely heavily on binary prediction market questions (which provide noisy rewards and skewed distributions), and risk leakage of future information. The goal is to enable LLMs to reason about "unknown unknowns" and generate novel, correct hypotheses for complex future events.

-   **Main Idea:** The authors propose a novel, fully automated methodology to train LLMs for open-ended forecasting. Key aspects include:
    1.  **Data Synthesis:** Creating a large-scale dataset, OpenForesight (~50,000 questions), by synthesizing open-ended forecasting questions from global news articles using a two-stage LLM-based recipe (one LLM generates questions from news, another filters and rewrites to prevent leakage).
    2.  **Leakage Prevention:** Utilizing an offline, static news corpus (CommonCrawl News) for both data generation and retrieval, avoiding online search engines, and strictly enforcing a knowledge cutoff (training only on events until April 2025).
    3.  **Training Methodology:** Employing Reinforcement Learning (RL) with Group Relative Policy Optimization (GRPO) on Qwen3 models (4B and 8B, with thinking enabled).
    4.  **Retrieval-Augmented Generation:** Integrating dense retrieval of relevant information from the offline news corpus (up to one month before resolution) to improve forecasting.
    5.  **Improved Reward Function:** Combining accuracy (measured via LLM-based semantic matching) and an adapted multi-class Brier score for open-ended responses to incentivize both correct predictions and truthful probability reporting.

-   **Main Results:** The specialized model, OpenForecaster8B, trained using this methodology on OpenForesight, demonstrated significant improvements:
    *   It matched the performance of much larger proprietary models in terms of accuracy, calibration, and consistency on a held-out test set from May to August 2025 and an external benchmark (FutureX).
    *   RL training specifically enhanced prediction accuracy, calibration, and consistency, particularly for long-term forecasts.
    *   Calibration improvements from the forecasting training generalized to multiple out-of-distribution benchmarks.

-   **Conclusion & Future Works:** The paper concludes that open-ended forecasting systems with rigorous probabilistic predictions can significantly transform policy-making, corporate planning, and financial risk management. To foster further research in this critical area, the authors have open-sourced all their models, code, and the OpenForesight dataset.

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.25070](https://huggingface.co/papers/2512.25070) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.25070](https://arxiv.org/abs/2512.25070) |
| PDF Download | [https://arxiv.org/pdf/2512.25070.pdf](https://arxiv.org/pdf/2512.25070.pdf) |

--- 

## 12. Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process

**Authors:** Zhenyu Zhang, Shujian Zhang, John Lambert, Wenxuan Zhou, Zhangyang Wang, Mingqing Chen, Andrew Hard, Rajiv Mathews, Lun Wang

**Published:** 2025-12-30

Here's a summary of the research paper based on the provided title and extracted text:

-   **Main Problem:** Despite advancements in large language models (LLMs) and their reasoning capabilities (e.g., Chain-of-Thought), the internal mechanisms of their reasoning processes remain underexplored. Existing approaches rely on limited, human-defined, supervised concepts (like reflection or backtracking) which are difficult to define at the token level, fail to capture the full spectrum of fluid and overlapping reasoning behaviors, and are ill-suited for large-scale annotation. This restricts the interpretability and steerability of LLMs during reasoning.

-   **Main Idea:** The authors propose an unsupervised framework called RISE (Reasoning behavior Interpretability via Sparse auto-Encoder) to discover "reasoning vectors" – linear directions in the activation space that encode distinct reasoning behaviors. RISE uses Sparse Auto-Encoders (SAEs) trained on step-level activations derived from segmenting Chain-of-Thought (CoT) traces into sentence-level steps (specifically, extracting hidden representations of `\n\n` delimiters). The SAEs learn a sparse dictionary of features where individual decoder columns correspond to these interpretable reasoning behaviors, enabling the disentanglement and analysis of the reasoning process without human labels.

-   **Main Results:**
    *   RISE successfully uncovers disentangled features corresponding to human-interpretable behaviors such as reflection and backtracking.
    *   Visualization and clustering analyses show that these discovered behaviors occupy separable regions in the SAE decoder column space.
    *   Targeted interventions on SAE-derived vectors can controllably amplify or suppress specific reasoning behaviors during inference, directly altering the reasoning trajectory *without additional model retraining*.
    *   SAEs also capture structural properties, such as response length, revealing clusters for long versus short reasoning traces.
    *   The framework enables the discovery of *novel reasoning behaviors* beyond human supervision, demonstrating the ability to identify confidence-related vectors and control response confidence.

-   **Conclusion & Future Works:** The paper concludes that unsupervised latent discovery using Sparse Auto-Encoders (SAEs) offers a powerful and principled approach for both interpreting and controllably steering reasoning in LLMs. This work represents a pivot study towards uncovering the geometry of reasoning behaviors in an unsupervised manner, demonstrating the potential for directly analyzing and manipulating these internal mechanisms. While not explicitly detailed, the findings imply future research into leveraging these discovered vectors for more precise control and deeper understanding of LLM reasoning.

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.23988](https://huggingface.co/papers/2512.23988) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.23988](https://arxiv.org/abs/2512.23988) |
| PDF Download | [https://arxiv.org/pdf/2512.23988.pdf](https://arxiv.org/pdf/2512.23988.pdf) |

--- 

## 13. SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time

**Authors:** Zhening Huang, Hyeonho Jeong, Xuelin Chen, Yulia Gryaditskaya, Tuanfeng Y. Wang, Joan Lasenby, Chun-Hao Huang

**Published:** 2025-12-31

-   **Main Problem**: Current video diffusion models lack the ability to freely explore dynamic scenes across both space (camera viewpoint) and time (motion sequence) independently. Existing methods struggle to disentangle and control these factors simultaneously within the generative process, and there is a scarcity of datasets offering continuous temporal variations of the same dynamic scene alongside diverse camera movements.

-   **Main Idea**: SpaceTimePilot proposes a video diffusion model that disentangles spatial and temporal factors to enable controllable generative rendering of dynamic scenes. The core solution involves:
    1.  **Animation Time-Embedding Mechanism**: An explicit mechanism within the diffusion process to control the output video's motion sequence relative to the source.
    2.  **Temporal-Warping Training Scheme**: A novel strategy that repurposes existing multi-view datasets to simulate continuous temporal differences, thereby teaching the model temporal control without requiring specially captured paired videos.
    3.  **Improved Camera-Conditioning Mechanism**: A refined component for precise control over camera movement, effective from the very first frame.
    4.  **Cam×Time Dataset**: A new synthetic dataset specifically designed to provide dense, full-coverage space-time video trajectories, offering rich supervision for effective disentanglement.
    This unified approach allows the model to generate continuous and coherent videos with arbitrary camera movements and retimed motion sequences (e.g., slow motion, reverse, bullet time) from a single input video.

-   **Main Results**:
    *   SpaceTimePilot successfully disentangles space and time in generative rendering, enabling independent control over camera viewpoint and motion sequence.
    *   It demonstrates the ability to synthesize novel views with various temporal manipulations (slow motion, reverse motion, bullet time) from a single source video.
    *   The model achieves strong results and clear space-time disentanglement, outperforming adapted state-of-the-art baselines on both real-world and synthetic data.

-   **Conclusion & Future Works**: SpaceTimePilot is presented as the first video diffusion model to achieve continuous and controllable novel view synthesis alongside temporal control from a single video, signifying a major step towards full 4D scene exploration. The introduction of the temporal-warping strategy and the Cam×Time dataset are highlighted as critical enablers for this disentanglement and precise spatiotemporal control. This work paves the way for advanced applications in interactive scene manipulation, video editing, and generative rendering where fine-grained control over dynamic scenes across space and time is essential.

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.25075](https://huggingface.co/papers/2512.25075) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.25075](https://arxiv.org/abs/2512.25075) |
| PDF Download | [https://arxiv.org/pdf/2512.25075.pdf](https://arxiv.org/pdf/2512.25075.pdf) |

--- 

## 14. Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems

**Authors:** Song Wang, Lingdong Kong, Xiaolu Liu, Hao Shi, Wentong Li, Jianke Zhu, Steven C. H. Hoi

**Published:** 2025-12-30

-   **Main Problem**: Autonomous systems, such as self-driving vehicles and drones, require robust "Spatial Intelligence" – holistic scene understanding, reasoning, and future prediction – from diverse multi-modal onboard sensor data (cameras, LiDAR, radar, event cameras). While foundation models excel in single-modal contexts, integrating their capabilities across these heterogeneous sensors to create a unified understanding, especially given challenges like costly manual annotations, data sparsity, noise, and multi-modal alignment, remains a formidable task.

-   **Main Idea**: This paper presents a comprehensive framework and roadmap for multi-modal data pre-training to address the challenge of forging Spatial Intelligence in autonomous systems. Its central contribution is the formulation of a unified taxonomy for pre-training paradigms, covering single-modality baselines to sophisticated unified frameworks that learn holistic representations. The paper systematically analyzes state-of-the-art techniques, dissects foundational methodologies (self-supervised learning, cross-modality interaction, knowledge distillation, transfer learning), and evaluates their strengths and limitations across various autonomous platforms. It also investigates the integration of textual inputs and occupancy representations to facilitate open-world perception and planning.

-   **Main Results**: As a roadmap and survey paper, it does not present novel experimental results but rather provides an analytical and structured overview of the field. Key contributions include:
    *   A comprehensive framework and unified taxonomy for multi-modal pre-training paradigms, categorizing methods by sensor modality, interaction level, and downstream application.
    *   A systematic analysis of state-of-the-art techniques in representation learning from onboard sensor data, emphasizing multi-modal interactions and foundation model integration.
    *   An investigation into foundational learning paradigms and their applicability across diverse autonomous platforms (vehicles, drones, robots).
    *   Identification and characterization of key challenges in sensor representation learning, such as data sparsity, sensor noise, multi-modal alignment, and real-time processing demands.

-   **Conclusion & Future Works**: The paper concludes by identifying critical bottlenecks, including computational efficiency and model scalability, in current multi-modal pre-training efforts. It proposes a roadmap toward the development of general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment. Future research directions highlighted include advancing generative world models and embodied reasoning to handle dynamic, real-world environments more effectively.

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.24385](https://huggingface.co/papers/2512.24385) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.24385](https://arxiv.org/abs/2512.24385) |
| PDF Download | [https://arxiv.org/pdf/2512.24385.pdf](https://arxiv.org/pdf/2512.24385.pdf) |

--- 

## 15. Guiding a Diffusion Transformer with the Internal Dynamics of Itself

**Authors:** Xingyu Zhou, Qifan Li, Xiaobin Hu, Hai Chen, Shuhang Gu

**Published:** 2025-12-30

-   **Main Problem:** Diffusion models struggle to generate high-quality samples in low-probability data areas due to insufficient training, leading to quality degradation. Existing guidance strategies, such as Classifier-Free Guidance (CFG), often lead to over-simplified or distorted samples, or alternative methods like guiding with a "bad version" of the model require complex degradation strategies, extra training, or additional sampling steps, limiting their scalability and practical application in large-scale Diffusion Transformers.

-   **Main Idea:** The paper proposes "Internal Guidance (IG)," a simple yet effective strategy that leverages the internal dynamics of a Diffusion Transformer. During training, an auxiliary supervision loss is applied to an intermediate layer, compelling the model to produce both a "weaker" intermediate generative output and a "final" output. During the sampling stage, IG extrapolates between these intermediate and final outputs to guide the generation process. This allows the model to self-guide, akin to using a "bad version" of itself, but without the need for external degradation strategies, additional training, or extra sampling steps.

-   **Main Results:** Internal Guidance (IG) significantly improves both training efficiency and generation quality across various Diffusion Transformer architectures (DiTs, SiTs, and LightningDiT).
    *   On ImageNet 256x256, SiT-XL/2+IG achieves FID=5.31 at 80 epochs and FID=1.75 at 800 epochs, outperforming vanilla SiT-XL at 1400 epochs.
    *   LightningDiT-XL/1+IG achieves FID=1.34 at 680 epochs.
    *   When combined with Classifier-Free Guidance (CFG) and guidance interval, LightningDiT-XL/1+IG achieves a new state-of-the-art FID of 1.19.
    *   The auxiliary intermediate supervision also helps alleviate vanishing gradients, demonstrating convergence performance comparable to or better than more complex self-supervised regularization methods.

-   **Conclusion & Future Works:** Internal Guidance offers a powerful, efficient, and easily integratable solution for enhancing diffusion model generation quality by utilizing the model's inherent internal dynamics. The authors suggest that further exploring the discrepancy between intermediate and deep outputs within the loss function could inspire novel training acceleration strategies.

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.24176](https://huggingface.co/papers/2512.24176) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.24176](https://arxiv.org/abs/2512.24176) |
| PDF Download | [https://arxiv.org/pdf/2512.24176.pdf](https://arxiv.org/pdf/2512.24176.pdf) |

--- 

## 16. Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking

**Authors:** Meiqi Chen, Fandong Meng, Jie Zhou

**Published:** 2025-12-30

- **Main Problem:** Complex reasoning problems, especially those involving geometry, kinematics, or combinatorial tasks, often require understanding implicit spatial, geometric, and structural relationships that are difficult for purely text-based reasoning models to capture. Existing approaches like text-only Chain-of-Thought (CoT) struggle with global structural constraints, leading to errors. Unified multimodal models generate noisy images, and tool-augmented LVLMs are limited to predefined operations on existing images, unable to construct new, task-specific visual representations from scratch.

- **Main Idea:** The paper introduces **FIGR (Figure It Out)**, a novel approach that integrates "active visual thinking" into multi-turn reasoning. Instead of directly generating images or relying on fixed toolsets, FIGR generates *executable code* (e.g., Python plotting code) to construct and iteratively refine illustrative figures. These rendered figures provide dynamic, interpretable visual feedback that enforces geometric consistency and supports the reasoning process. FIGR is trained end-to-end using reinforcement learning (specifically, GRPO) with an adaptive reward mechanism that regulates *when* visual reasoning should be invoked, promoting selective and effective use of visual cues without needing a supervised cold-start.

- **Main Results:** Experiments on challenging mathematical reasoning benchmarks demonstrate that FIGR significantly outperforms strong text-only CoT baselines. Specifically, FIGR improves the base model's performance by 13.12% on AIME 2025 and 11.00% on BeyondAIME. This highlights the effectiveness of its figure-guided multimodal reasoning in enhancing the stability and reliability of complex problem-solving.

- **Conclusion & Future Works:** FIGR successfully demonstrates that active visual thinking, achieved through generating executable code for figure construction and guided by an adaptive reinforcement learning reward mechanism, can overcome the limitations of current reasoning models in handling complex spatial and geometric constraints. This approach leads to more stable, reliable, and geometrically consistent reasoning. (The provided text does not explicitly detail future works.)

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.24297](https://huggingface.co/papers/2512.24297) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.24297](https://arxiv.org/abs/2512.24297) |
| PDF Download | [https://arxiv.org/pdf/2512.24297.pdf](https://arxiv.org/pdf/2512.24297.pdf) |

--- 

## 17. Factorized Learning for Temporally Grounded Video-Language Models

**Authors:** Wenzheng Zeng, Difei Gao, Mike Zheng Shou, Hwee Tou Ng

**Published:** 2025-12-30

-   **Main Problem:** Recent video-language models (Video LLMs) struggle with accurate temporal grounding for event-level perception. Existing methods typically handle temporal grounding and textual response in a coupled manner, leading to sub-optimal learning objectives. Specifically, current special tokens for temporal grounding lack explicit capture of event-level visual semantics, focusing mainly on timestamp representation, which limits their utility as context for subsequent textual answers.

-   **Main Idea:** The paper proposes a "factorized learning perspective" to decouple and improve temporally grounded video-language models.
    1.  **D2VLM Framework:** Decouples temporal grounding and textual answering into a "grounding then answering with evidence referencing" paradigm. It introduces **evidence tokens (`<evi>`)** that not only pinpoint temporal locations but also emphasize capturing event-level visual semantics, providing crucial context for subsequent text generation. The model first performs pure grounding, then generates an interleaved text-evidence response, ensuring consistency and reinforcing logical coherence.
    2.  **Factorized Preference Optimization (FPO):** A novel algorithm that incorporates probabilistic temporal grounding modeling into the optimization objective, enabling explicit preference learning for both temporal grounding and textual response.
    3.  **Synthetic Dataset:** To support FPO, a synthetic dataset is constructed by introducing factorized perturbations (temporal grounding and textual response) into preferred response sequences, creating structured and controllable noise without manual annotation.

-   **Main Results:**
    *   Experiments across various tasks demonstrate a clear advantage and superiority of the proposed D2VLM framework.
    *   The paper claims its 3.8B model outperforms existing state-of-the-art methods (ranging from 3.8B to 13B in model size) across various tasks, as illustrated in Figure 1(a).
    *   The designed sequence generation objective and event-level visual semantic capture are shown to be essential for performance improvement.

-   **Conclusion & Future Works:** The paper concludes that its factorized learning approach, encompassing the D2VLM framework with evidence tokens and the FPO algorithm, effectively addresses the limitations of existing video LLMs in temporal grounding and textual response. The experimental results validate the approach's superiority. While not explicitly detailed in the extracted text, the work offers valuable insights for future model design in this area.

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.24097](https://huggingface.co/papers/2512.24097) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.24097](https://arxiv.org/abs/2512.24097) |
| PDF Download | [https://arxiv.org/pdf/2512.24097.pdf](https://arxiv.org/pdf/2512.24097.pdf) |

--- 

## 18. Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers

**Authors:** Atakan Işık, Selin Vulga Işık, Ahmet Feridun Işık, Mahşuk Taylan

**Published:** 2025-12-27

Summary generation failed.

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.22564](https://huggingface.co/papers/2512.22564) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.22564](https://arxiv.org/abs/2512.22564) |
| PDF Download | [https://arxiv.org/pdf/2512.22564.pdf](https://arxiv.org/pdf/2512.22564.pdf) |

--- 

## 19. BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts

**Authors:** Hengli Li, Zhaoxin Yu, Qi Shen, Chenxi Li, Mengmeng Wang, Tinglang Wu, Yipeng Kang, Yuxuan Wang, Song-Chun Zhu, Zixia Jia, Zilong Zheng

**Published:** 2025-12-31

-   **Main Problem:** Prior work in strategic dialogue often focuses on accurately estimating interlocutor beliefs but lacks a principled mechanism for effectively *using* those beliefs during utterance generation. Existing methods tend to relay all available belief information, which can be suboptimal for strategic objectives like shaping the interlocutor's beliefs.

-   **Main Idea:** The paper introduces BEDA (Belief Estimation as Probabilistic Constraints), a framework that bridges belief estimation and utterance generation by formalizing dialogue acts as probabilistic constraints. It defines two core strategic acts—Adversarial (introducing information unknown to the interlocutor) and Alignment (emphasizing shared knowledge)—and operationalizes them via constraints on what an agent may generate. BEDA consists of a World Set, a Belief Estimator Module (to infer the opponent's beliefs and the truthfulness of events), and a Conditional Generator that selects events consistent with the inferred beliefs and chosen dialogue act to produce utterances.

-   **Main Results:** BEDA consistently outperforms strong baselines across three distinct dialogue settings:
    *   **Conditional Keeper–Burglar (adversarial):** Improves success rate by at least 5.0 points across backbones, and by 20.6 points with GPT-4.1-nano.
    *   **Mutual Friends (cooperative):** Achieves an average improvement of 9.3 points (up to 30.4% success rate improvement).
    *   **CaSiNo (negotiation):** Achieves the optimal deal relative to all baselines.
    Case analyses further demonstrate BEDA's ability to mitigate useless information for cooperation efficiency and generate personalized misleading information in adversarial scenarios.

-   **Conclusion & Future Works:** The paper concludes that casting belief estimation as probabilistic constraints provides a simple, general, and reliable mechanism for performing strategic dialogue acts. This integration significantly enhances performance compared to existing methods, highlighting the importance of using belief estimation to constrain dialogue generation. The current work focuses on the overall paradigm with a given world set, implying future work could involve dynamic construction of the world set.

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.24885](https://huggingface.co/papers/2512.24885) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.24885](https://arxiv.org/abs/2512.24885) |
| PDF Download | [https://arxiv.org/pdf/2512.24885.pdf](https://arxiv.org/pdf/2512.24885.pdf) |

--- 

## 20. Valori: A Deterministic Memory Substrate for AI Systems

**Authors:** Varshith Gudur

**Published:** 2025-12-25

Here's a summary of the research paper "Valori: A Deterministic Memory Substrate for AI Systems":

-   **Main Problem:** Modern AI systems, particularly those using vector embeddings for memory (e.g., RAG, agent workflows), suffer from fundamental non-determinism. Due to the reliance on standard floating-point arithmetic (IEEE 754), identical models, inputs, and code can produce different memory states and retrieval results across different hardware architectures (e.g., x86 vs. ARM). This non-reproducibility prevents post-hoc verification, compromises audit trails in regulated sectors, and breaks state replayability guarantees. Empirical evidence shows bit-level divergence in embeddings generated on different platforms even with identical code.

-   **Main Idea:** Valori proposes a deterministic AI memory substrate that enforces a strict determinism boundary. It achieves this by replacing floating-point memory operations with fixed-point arithmetic (Q16.16 as default) and modeling memory as a replayable state machine. Valori normalizes non-deterministic model outputs into a deterministic memory state at the kernel boundary. The system's core logic is implemented as a `no std` Rust kernel, ensuring numerical results are bit-identical across architectures (x86, ARM, RISC-V, WASM). It also adapts indexing structures like HNSW for strict determinism by using fixed-point distance metrics and removing stochasticity in graph construction.

-   **Main Results:**
    *   **Cross-Platform Consistency:** A "Snapshot Transfer" test confirmed bit-identical memory states and internal hashes when a Valori kernel snapshot was transferred from an x86 machine to an ARM machine. This guarantees identical k-NN result ordering and scores for a fixed query and memory state across platforms.
    *   **Performance:** Valori achieved raw retrieval latency of less than 500µs for typical k-NN queries on a MacBook Pro M3, which is sufficient for real-time agentic applications.
    *   **Semantic Fidelity:** Despite using fixed-point quantization, Valori demonstrated a mean Recall@10 of 99.8% compared to a standard `f32` ANN index, indicating minimal degradation in semantic retrieval quality.

-   **Conclusion & Future Works:** The paper concludes that deterministic memory is a necessary primitive for trustworthy AI systems, enabling replayability, verifiability, and auditability. Valori provides a foundational solution by enforcing bit-identical memory states across diverse hardware. Future work includes supporting higher precision contracts like Q32.32 for enterprise applications and Q64.64/Q128 for scientific computing and defense systems, allowing architects to trade off precision for performance without sacrificing determinism.

### Related Links

| Platform | Link |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.22280](https://huggingface.co/papers/2512.22280) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.22280](https://arxiv.org/abs/2512.22280) |
| PDF Download | [https://arxiv.org/pdf/2512.22280.pdf](https://arxiv.org/pdf/2512.22280.pdf) |

