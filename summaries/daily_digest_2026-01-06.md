# ğŸ¤— Daily Hugging Face Paper Digest - 2026-01-06

BÃ¡o cÃ¡o Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng vÃ o lÃºc 2026-01-07 11:08:22 báº±ng mÃ´ hÃ¬nh `gemini-2.5-flash`.

## ğŸ“° Summary of Papers

--- 

## 1. Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits

**TÃ¡c giáº£:** Amirhosein Ghasemabadi, Di Niu

**Xuáº¥t báº£n lÃºc:** 2025-12-23

### Main Problem:
CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) cÃ³ kháº£ nÄƒng táº¡o ra cÃ¡c pháº£n há»“i trÃ´i cháº£y vÃ  phá»©c táº¡p nhÆ°ng thÆ°á»ng khÃ´ng nháº­n ra lá»—i cá»§a chÃ­nh mÃ¬nh hoáº·c cÃ¡c thÃ´ng tin bá»‹a Ä‘áº·t (hallucinations). Khoáº£ng cÃ¡ch giá»¯a kháº£ nÄƒng táº¡o vÄƒn báº£n máº¡nh máº½ vÃ  kháº£ nÄƒng tá»± xÃ¡c minh yáº¿u nÃ y háº¡n cháº¿ Ä‘á»™ tin cáº­y, an toÃ n vÃ  hiá»‡u quáº£ cá»§a viá»‡c triá»ƒn khai LLM, Ä‘áº·c biá»‡t trong cÃ¡c tÃ¬nh huá»‘ng yÃªu cáº§u suy luáº­n dÃ i háº¡n hoáº·c kiá»ƒm soÃ¡t chi phÃ­ tÃ­nh toÃ¡n. CÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n cÃ³ thÆ°á»ng dá»±a vÃ o cÃ¡c tháº©m phÃ¡n bÃªn ngoÃ i, tÃ­nh nháº¥t quÃ¡n Ä‘a máº«u hoáº·c tá»± phÃª bÃ¬nh dá»±a trÃªn vÄƒn báº£n, nhÆ°ng chÃºng Ä‘á»u phÃ¡t sinh chi phÃ­ tÃ­nh toÃ¡n bá»• sung hoáº·c tÆ°Æ¡ng quan yáº¿u vá»›i Ä‘á»™ chÃ­nh xÃ¡c thá»±c sá»±.

### Main Idea:
BÃ i nghiÃªn cá»©u giá»›i thiá»‡u Gnosis, má»™t cÆ¡ cháº¿ tá»± nháº­n thá»©c nháº¹ cho phÃ©p cÃ¡c LLM Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n (frozen LLMs) thá»±c hiá»‡n tá»± xÃ¡c minh ná»™i táº¡i báº±ng cÃ¡ch giáº£i mÃ£ tÃ­n hiá»‡u tá»« cÃ¡c tráº¡ng thÃ¡i áº©n (hidden states) vÃ  cÃ¡c máº«u chÃº Ã½ (attention patterns) trong quÃ¡ trÃ¬nh suy luáº­n. Gnosis hoáº¡t Ä‘á»™ng nhÆ° má»™t quan sÃ¡t viÃªn thá»¥ Ä‘á»™ng, nÃ©n cÃ¡c dáº¥u váº¿t ná»™i bá»™ cá»§a mÃ´ hÃ¬nh thÃ nh cÃ¡c mÃ´ táº£ cÃ³ kÃ­ch thÆ°á»›c cá»‘ Ä‘á»‹nh, sau Ä‘Ã³ dá»± Ä‘oÃ¡n Ä‘iá»ƒm sá»‘ chÃ­nh xÃ¡c. Kiáº¿n trÃºc cá»§a Gnosis Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ chi phÃ­ suy luáº­n khÃ´ng phá»¥ thuá»™c vÃ o Ä‘á»™ dÃ i chuá»—i, chá»‰ thÃªm khoáº£ng 5 triá»‡u tham sá»‘ vÃ  cÃ³ chi phÃ­ tÃ­nh toÃ¡n khÃ´ng Ä‘Ã¡ng ká»ƒ.

### Main Results:
Gnosis Ä‘Ã£ chá»©ng minh hiá»‡u suáº¥t vÆ°á»£t trá»™i so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p cÆ¡ sá»Ÿ ná»™i bá»™ máº¡nh máº½ vÃ  cÃ¡c tháº©m phÃ¡n bÃªn ngoÃ i lá»›n vá» cáº£ Ä‘á»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ hiá»‡u chuáº©n (calibration).
*   TrÃªn cÃ¡c bá»™ tiÃªu chuáº©n bao gá»“m suy luáº­n toÃ¡n há»c (Math-Reasoning), há»i Ä‘Ã¡p miá»n má»Ÿ (Open-Domain QA) vÃ  kiáº¿n thá»©c há»c thuáº­t (Academic Knowledge), Gnosis luÃ´n vÆ°á»£t trá»™i hÆ¡n cÃ¡c mÃ´ hÃ¬nh thÆ°á»Ÿng Skywork 8B vÃ  tháº©m phÃ¡n Gemini 2.5 Pro trÃªn cÃ¡c mÃ´ hÃ¬nh ná»n tá»« 1.7B Ä‘áº¿n 20B tham sá»‘.
*   Gnosis chá»‰ thÃªm khoáº£ng 5 triá»‡u tham sá»‘, nhá» hÆ¡n nhiá»u láº§n so vá»›i cÃ¡c trÃ¬nh xÃ¡c minh bÃªn ngoÃ i cÃ³ hÃ ng tá»· tham sá»‘, nhÆ°ng váº«n Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tiÃªn tiáº¿n.
*   Gnosis cÃ³ kháº£ nÄƒng khÃ¡i quÃ¡t hÃ³a zero-shot sang cÃ¡c tháº¿ há»‡ má»™t pháº§n (partial generations), cho phÃ©p phÃ¡t hiá»‡n sá»›m cÃ¡c quá»¹ Ä‘áº¡o suy luáº­n tháº¥t báº¡i vÃ  kiá»ƒm soÃ¡t tÃ i nguyÃªn tÃ­nh toÃ¡n.
*   CÃ¡c káº¿t quáº£ cho tháº¥y cÃ¡c dáº¥u hiá»‡u Ä‘Ã¡ng tin cáº­y vá» Ä‘á»™ chÃ­nh xÃ¡c lÃ  ná»™i táº¡i Ä‘á»‘i vá»›i quÃ¡ trÃ¬nh táº¡o ra pháº£n há»“i vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c trÃ­ch xuáº¥t má»™t cÃ¡ch hiá»‡u quáº£ mÃ  khÃ´ng cáº§n giÃ¡m sÃ¡t tá»« bÃªn ngoÃ i.

### Conclusion & Future Works:
CÃ¡c káº¿t quáº£ cho tháº¥y cÃ¡c dáº¥u hiá»‡u Ä‘Ã¡ng tin cáº­y vá» Ä‘á»™ chÃ­nh xÃ¡c lÃ  ná»™i táº¡i Ä‘á»‘i vá»›i quÃ¡ trÃ¬nh táº¡o ra pháº£n há»“i cá»§a LLM vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c trÃ­ch xuáº¥t má»™t cÃ¡ch hiá»‡u quáº£ mÃ  khÃ´ng cáº§n giÃ¡m sÃ¡t tá»« bÃªn ngoÃ i. Kháº£ nÄƒng tá»± nháº­n thá»©c ná»™i táº¡i nÃ y cá»§a Gnosis mang láº¡i tiá»m nÄƒng cho viá»‡c phÃ¡t hiá»‡n sá»›m cÃ¡c lá»—i suy luáº­n, má»Ÿ rá»™ng hiá»‡u quáº£ trÃªn cÃ¡c kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh vÃ  miá»n khÃ¡c nhau, cÅ©ng nhÆ° triá»ƒn khai thá»±c táº¿ cÃ¡c há»‡ thá»‘ng ngÃ´n ngá»¯ yÃªu cáº§u Ä‘á»™ tin cáº­y cao vÃ  kiá»ƒm soÃ¡t tÃ i nguyÃªn tÃ­nh toÃ¡n má»™t cÃ¡ch hiá»‡u quáº£.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.20578](https://huggingface.co/papers/2512.20578) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.20578](https://arxiv.org/abs/2512.20578) |
| PDF Download | [https://arxiv.org/pdf/2512.20578.pdf](https://arxiv.org/pdf/2512.20578.pdf) |
| Github Repository | [https://github.com/Amirhosein-gh98/Gnosis](https://github.com/Amirhosein-gh98/Gnosis) |

--- 

## 2. K-EXAONE Technical Report

**TÃ¡c giáº£:** Eunbi Choi, Kibong Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Hyunjik Jo, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Haeju Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Heuiyeen Yeen, Hwan Chang, Stanley Jungkyu Choi, Yejin Choi, Jiwon Ham, Kijeong Jeon, Geunyeong Jeong, Gerrard Jeongwon Jo, Yonghwan Jo, Jiyeon Jung, Naeun Kang, Dohoon Kim, Euisoon Kim, Hayeon Kim, Hyosang Kim, Hyunseo Kim, Jieun Kim, Minu Kim, Myoungshin Kim, Unsol Kim, Youchul Kim, YoungJin Kim, Chaeeun Lee, Chaeyoon Lee, Changhun Lee, Dahm Lee, Edward Hwayoung Lee, Honglak Lee, Jinsang Lee, Jiyoung Lee, Sangeun Lee, Seungwon Lim, Solji Lim, Woohyung Lim, Chanwoo Moon, Jaewoo Park, Jinho Park, Yongmin Park, Hyerin Seo, Wooseok Seo, Yongwoo Song, Sejong Yang, Sihoon Yang, Chang En Yea, Sihyuk Yi, Chansik Yoon, Dongkeun Yoon, Sangyeon Yoon, Hyeongu Yun

**Xuáº¥t báº£n lÃºc:** 2026-01-05

### Main Problem:
Thá»‹ trÆ°á»ng phÃ¡t triá»ƒn mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) toÃ n cáº§u Ä‘ang cÃ³ sá»± cáº¡nh tranh gay gáº¯t, vá»›i nhu cáº§u vá» cÃ¡c mÃ´ hÃ¬nh cÃ³ hiá»‡u suáº¥t vÆ°á»£t trá»™i. Tuy nhiÃªn, HÃ n Quá»‘c Ä‘á»‘i máº·t vá»›i nhá»¯ng thÃ¡ch thá»©c riÃªng biá»‡t vá» cÆ¡ sá»Ÿ háº¡ táº§ng, nhÆ° thiáº¿u trung tÃ¢m dá»¯ liá»‡u chuyÃªn dá»¥ng vÃ  chip AI, lÃ m háº¡n cháº¿ kháº£ nÄƒng phÃ¡t triá»ƒn cÃ¡c mÃ´ hÃ¬nh quy mÃ´ lá»›n. CÃ¡c ná»— lá»±c trÆ°á»›c Ä‘Ã¢y táº­p trung vÃ o cÃ¡c mÃ´ hÃ¬nh nhá» hÆ¡n, hiá»‡u quáº£ vá» chi phÃ­. Váº¥n Ä‘á» cá»‘t lÃµi lÃ  lÃ m tháº¿ nÃ o Ä‘á»ƒ xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh ná»n táº£ng máº¡nh máº½ vÃ  Ä‘Ã¡ng tin cáº­y, Ä‘áº¡t hiá»‡u suáº¥t hÃ ng Ä‘áº§u trÃªn quy mÃ´ toÃ n cáº§u, báº¥t cháº¥p nhá»¯ng háº¡n cháº¿ vá» cÆ¡ sá»Ÿ háº¡ táº§ng. NgoÃ i ra, viá»‡c má»Ÿ rá»™ng kháº£ nÄƒng Ä‘a ngÃ´n ngá»¯ vÃ  tÄƒng cÆ°á»ng nÄƒng lá»±c suy luáº­n cÅ©ng lÃ  nhá»¯ng thÃ¡ch thá»©c quan trá»ng.

### Main Idea:
LG AI Research Ä‘Ã£ phÃ¡t triá»ƒn K-EXAONE, má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘a ngÃ´n ngá»¯ quy mÃ´ lá»›n, Ä‘Æ°á»£c xÃ¢y dá»±ng trÃªn kiáº¿n trÃºc Mixture-of-Experts (MoE) vá»›i tá»•ng sá»‘ 236 tá»· tham sá»‘, trong Ä‘Ã³ 23 tá»· tham sá»‘ Ä‘Æ°á»£c kÃ­ch hoáº¡t trong quÃ¡ trÃ¬nh suy luáº­n. MÃ´ hÃ¬nh nÃ y há»— trá»£ cá»­a sá»• ngá»¯ cáº£nh 256K token vÃ  bao gá»“m sÃ¡u ngÃ´n ngá»¯: tiáº¿ng HÃ n, tiáº¿ng Anh, tiáº¿ng TÃ¢y Ban Nha, tiáº¿ng Äá»©c, tiáº¿ng Nháº­t vÃ  tiáº¿ng Viá»‡t. K-EXAONE tÃ­ch há»£p cÃ¡c cáº£i tiáº¿n kiáº¿n trÃºc nhÆ° thiáº¿t káº¿ MoE thÆ°a tinh chá»‰nh, cÆ¡ cháº¿ chÃº Ã½ lai (hybrid attention) káº¿t há»£p chÃº Ã½ toÃ n cá»¥c vÃ  chÃº Ã½ cá»­a sá»• trÆ°á»£t, vÃ  má»™t module Multi-Token Prediction (MTP) Ä‘á»ƒ há»— trá»£ Ä‘Ã o táº¡o phá»¥ trá»£ vÃ  tÄƒng tá»‘c suy luáº­n. Bá»™ mÃ£ hÃ³a (tokenizer) Ä‘Ã£ Ä‘Æ°á»£c thiáº¿t káº¿ láº¡i vá»›i kÃ­ch thÆ°á»›c tá»« vá»±ng tÄƒng lÃªn (150K) vÃ  chiáº¿n lÆ°á»£c SuperBPE Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u quáº£ mÃ£ hÃ³a vÃ  kháº£ nÄƒng má»Ÿ rá»™ng Ä‘a ngÃ´n ngá»¯. QuÃ¡ trÃ¬nh tiá»n huáº¥n luyá»‡n ba giai Ä‘oáº¡n Ä‘Æ°á»£c Ã¡p dá»¥ng, sá»­ dá»¥ng bá»™ dá»¯ liá»‡u cháº¥t lÆ°á»£ng cao, má»Ÿ rá»™ng pháº¡m vi ngÃ´n ngá»¯ thÃ´ng qua tá»•ng há»£p dá»¯ liá»‡u cÃ³ má»¥c tiÃªu vÃ  tá»•ng há»£p dá»¯ liá»‡u tÄƒng cÆ°á»ng suy luáº­n. MÃ´ hÃ¬nh cÅ©ng Ã¡p dá»¥ng quy trÃ¬nh má»Ÿ rá»™ng Ä‘á»™ dÃ i ngá»¯ cáº£nh hai giai Ä‘oáº¡n (tá»« 8K lÃªn 256K token) báº±ng cÃ¡ch sá»­ dá»¥ng táº­p dá»¯ liá»‡u Rehearsal Ä‘á»ƒ duy trÃ¬ hiá»‡u suáº¥t ngá»¯ cáº£nh ngáº¯n vÃ  táº­p dá»¯ liá»‡u Synthetic Reasoning Ä‘á»ƒ tÄƒng cÆ°á»ng kháº£ nÄƒng suy luáº­n.

### Main Results:
K-EXAONE thá»ƒ hiá»‡n hiá»‡u suáº¥t cáº¡nh tranh, ngang báº±ng vá»›i cÃ¡c mÃ´ hÃ¬nh mÃ£ nguá»“n má»Ÿ cÃ³ kÃ­ch thÆ°á»›c tÆ°Æ¡ng tá»± trÃªn má»™t bá»™ tiÃªu chuáº©n Ä‘Ã¡nh giÃ¡ toÃ n diá»‡n, bao gá»“m cÃ¡c kháº£ nÄƒng suy luáº­n, tÃ¡c tá»­, tá»•ng quÃ¡t, tiáº¿ng HÃ n vÃ  Ä‘a ngÃ´n ngá»¯. Cá»¥ thá»ƒ, mÃ´ hÃ¬nh Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ áº¥n tÆ°á»£ng trÃªn tÃ¡m háº¡ng má»¥c Ä‘Ã¡nh giÃ¡ chÃ­nh: kiáº¿n thá»©c tháº¿ giá»›i (MMLU-PRO), toÃ¡n há»c (AIME 2025), láº­p trÃ¬nh (LiveCodeBench v6), sá»­ dá»¥ng cÃ´ng cá»¥ tÃ¡c tá»­ (Ï„2-Bench), tuÃ¢n thá»§ hÆ°á»›ng dáº«n (IFBench), tiáº¿ng HÃ n (KoBALT), Ä‘a ngÃ´n ngá»¯ (MMMLU) vÃ  an toÃ n (KGC-Safety). Thiáº¿t káº¿ láº¡i bá»™ mÃ£ hÃ³a Ä‘Ã£ cáº£i thiá»‡n hiá»‡u quáº£ mÃ£ hÃ³a trung bÃ¬nh khoáº£ng 30% trÃªn cÃ¡c miá»n vÄƒn báº£n khÃ¡c nhau. Trong quÃ¡ trÃ¬nh suy luáº­n, K-EXAONE sá»­ dá»¥ng khá»‘i MTP Ä‘á»ƒ tá»± phÃ¡c tháº£o, Ä‘áº¡t Ä‘Æ°á»£c tá»‘c Ä‘á»™ giáº£i mÃ£ nhanh hÆ¡n khoáº£ng 1,5 láº§n so vá»›i giáº£i mÃ£ tá»± há»“i quy tiÃªu chuáº©n. MÃ´ hÃ¬nh cÅ©ng duy trÃ¬ hiá»‡u suáº¥t ngá»¯ cáº£nh ngáº¯n tá»‘t sau quÃ¡ trÃ¬nh má»Ÿ rá»™ng Ä‘á»™ dÃ i ngá»¯ cáº£nh lÃªn 256K token nhá» vÃ o viá»‡c sá»­ dá»¥ng Rehearsal Dataset.

### Conclusion & Future Works:
K-EXAONE lÃ  má»™t bÆ°á»›c tiáº¿n quan trá»ng cá»§a LG AI Research trong viá»‡c phÃ¡t triá»ƒn má»™t mÃ´ hÃ¬nh ná»n táº£ng Ä‘áº¡t hiá»‡u suáº¥t tiÃªn phong, giáº£i quyáº¿t cÃ¡c thÃ¡ch thá»©c vá» cÆ¡ sá»Ÿ háº¡ táº§ng AI táº¡i HÃ n Quá»‘c thÃ´ng qua sá»± há»— trá»£ cá»§a chÃ­nh phá»§. Vá»›i kiáº¿n trÃºc MoE sÃ¡ng táº¡o, kháº£ nÄƒng há»— trá»£ Ä‘a ngÃ´n ngá»¯ rá»™ng rÃ£i vÃ  ká»¹ thuáº­t huáº¥n luyá»‡n tiÃªn tiáº¿n, K-EXAONE Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cáº¡nh tranh trÃªn pháº¡m vi toÃ n cáº§u. MÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘á»‹nh vá»‹ lÃ  má»™t mÃ´ hÃ¬nh ná»n táº£ng AI Ä‘á»™c quyá»n máº¡nh máº½, sáºµn sÃ ng cho nhiá»u á»©ng dá»¥ng cÃ´ng nghiá»‡p vÃ  nghiÃªn cá»©u, nháº±m thÃºc Ä‘áº©y AI vÃ¬ má»™t cuá»™c sá»‘ng tá»‘t Ä‘áº¹p hÆ¡n. Pháº§n trÃ­ch dáº«n khÃ´ng nÃªu rÃµ cÃ¡c hÆ°á»›ng nghiÃªn cá»©u tiáº¿p theo mÃ  chá»§ yáº¿u táº­p trung vÃ o mÃ´ táº£ cÃ¡c Ä‘áº·c tÃ­nh vÃ  káº¿t quáº£ hiá»‡n táº¡i cá»§a mÃ´ hÃ¬nh.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2601.01739](https://huggingface.co/papers/2601.01739) |
| ArXiv Abstract | [https://arxiv.org/abs/2601.01739](https://arxiv.org/abs/2601.01739) |
| PDF Download | [https://arxiv.org/pdf/2601.01739.pdf](https://arxiv.org/pdf/2601.01739.pdf) |
| Github Repository | [https://github.com/LG-AI-EXAONE/K-EXAONE](https://github.com/LG-AI-EXAONE/K-EXAONE) |

--- 

## 3. NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation

**TÃ¡c giáº£:** Huichao Zhang, Liao Qu, Yiheng Liu, Hang Chen, Yangyang Song, Yongsheng Dong, Shikun Sun, Xian Li, Xu Wang, Yi Jiang, Hu Ye, Bo Chen, Yiming Gao, Peng Liu, Akide Liu, Zhipeng Yang, Qili Deng, Linjie Xing, Jiyang Liu, Zhao Wang, Yang Zhou, Mingcong Liu, Yi Zhang, Qian He, Xiwei Hu, Zhongqi Qi, Jie Shao, Zhiye Fu, Shuai Wang, Fangmin Chen, Xuezhi Chai, Zhihua Wu, Yitong Wang, Zehuan Yuan, Daniel K. Du, Xinglong Wu

**Xuáº¥t báº£n lÃºc:** 2026-01-05

### Main Problem:
CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) vÃ  mÃ´ hÃ¬nh khuáº¿ch tÃ¡n (Diffusion Models) hiá»‡n táº¡i váº«n tÃ¡ch biá»‡t, dáº«n Ä‘áº¿n viá»‡c cÃ¡c mÃ´ hÃ¬nh khuáº¿ch tÃ¡n thiáº¿u kháº£ nÄƒng suy luáº­n vÃ  há»c theo ngá»¯ cáº£nh cá»§a LLMs, trong khi cÃ¡c LLMs Ä‘a phÆ°Æ¡ng thá»©c truyá»n thá»‘ng thÆ°á»ng chá»‰ giá»›i háº¡n á»Ÿ kháº£ nÄƒng nháº­n thá»©c. CÃ¡c kiáº¿n trÃºc lai AR-Diffusion gáº·p váº¥n Ä‘á» vá» chi phÃ­ mÃ£ hÃ³a láº¡i vÃ  háº¡n cháº¿ tÃ­ch há»£p Ä‘a phÆ°Æ¡ng thá»©c sÃ¢u sáº¯c. CÃ¡c mÃ´ hÃ¬nh tá»± há»“i quy thuáº§n tÃºy nhÆ° Chameleon gáº·p pháº£i hai váº¥n Ä‘á» chÃ­nh: chi phÃ­ tÃ­nh toÃ¡n lá»›n vÃ  tá»‘c Ä‘á»™ cháº­m khi táº¡o hÃ¬nh áº£nh Ä‘á»™ phÃ¢n giáº£i cao (hÆ¡n 10 phÃºt cho áº£nh 1024x1024) do sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p quÃ©t raster truyá»n thá»‘ng, vÃ  biá»ƒu diá»…n hÃ¬nh áº£nh thiáº¿u máº­t Ä‘á»™ ngá»¯ nghÄ©a cao, lÃ m háº¡n cháº¿ hiá»‡u suáº¥t trong cÃ¡c tÃ¡c vá»¥ hiá»ƒu Ä‘a phÆ°Æ¡ng thá»©c.

### Main Idea:
BÃ i bÃ¡o giá»›i thiá»‡u NextFlow, má»™t mÃ´ hÃ¬nh transformer tá»± há»“i quy chá»‰ dá»±a trÃªn bá»™ giáº£i mÃ£ (decoder-only) Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn 6 nghÃ¬n tá»· token rá»i ráº¡c xen káº½ vÄƒn báº£n vÃ  hÃ¬nh áº£nh. NextFlow kÃ­ch hoáº¡t kháº£ nÄƒng hiá»ƒu vÃ  táº¡o Ä‘a phÆ°Æ¡ng thá»©c thá»‘ng nháº¥t, bao gá»“m chá»‰nh sá»­a hÃ¬nh áº£nh, ná»™i dung xen káº½ vÃ  táº¡o video. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» hiá»‡u quáº£, NextFlow sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p dá»± Ä‘oÃ¡n theo tá»‰ lá»‡ tiáº¿p theo (next-scale prediction) cho táº¡o hÃ¬nh áº£nh (thay vÃ¬ quÃ©t raster), táº¡o ná»™i dung hÃ¬nh áº£nh tá»« cáº¥u trÃºc thÃ´ Ä‘áº¿n chi tiáº¿t tinh vi. Äá»ƒ kháº¯c phá»¥c khoáº£ng cÃ¡ch ngá»¯ nghÄ©a, NextFlow Ã¡p dá»¥ng trÃ¬nh mÃ£ hÃ³a tokenizer dual-codebook Ä‘á»ƒ tÃ¡ch biá»‡t cÃ¡c Ä‘áº·c trÆ°ng ngá»¯ nghÄ©a vÃ  má»©c pixel. MÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i cÃ´ng thá»©c máº¡nh máº½ Ä‘á»ƒ giáº£i quyáº¿t sá»± báº¥t á»•n Ä‘á»‹nh cá»§a táº¡o Ä‘a tá»‰ lá»‡, vÃ  giá»›i thiá»‡u chiáº¿n lÆ°á»£c prefix-tuning cho há»c tÄƒng cÆ°á»ng (Reinforcement Learning) Ä‘á»ƒ tá»‘i Æ°u hÃ³a cáº¥u trÃºc tá»•ng thá»ƒ. Má»™t bá»™ giáº£i mÃ£ khuáº¿ch tÃ¡n tÃ¹y chá»n cÅ©ng Ä‘Æ°á»£c tÃ­ch há»£p Ä‘á»ƒ tinh chá»‰nh Ä‘áº§u ra rá»i ráº¡c, nÃ¢ng cao Ä‘á»™ chÃ¢n thá»±c cá»§a hÃ¬nh áº£nh.

### Main Results:
NextFlow cÃ³ thá»ƒ táº¡o ra hÃ¬nh áº£nh 1024x1024 chá»‰ trong 5 giÃ¢y, nhanh hÆ¡n nhiá»u láº§n so vá»›i cÃ¡c mÃ´ hÃ¬nh AR tÆ°Æ¡ng Ä‘Æ°Æ¡ng dá»±a trÃªn quÃ©t raster. MÃ´ hÃ¬nh Ä‘áº¡t hiá»‡u suáº¥t dáº«n Ä‘áº§u trong sá»‘ cÃ¡c mÃ´ hÃ¬nh thá»‘ng nháº¥t vÃ  cáº¡nh tranh vá»›i cÃ¡c mÃ´ hÃ¬nh khuáº¿ch tÃ¡n chuyÃªn biá»‡t vá» cháº¥t lÆ°á»£ng hÃ¬nh áº£nh. Vá»›i 7 tá»· tham sá»‘, NextFlow Ä‘áº¡t hiá»‡u suáº¥t cáº¡nh tranh trÃªn cÃ¡c tiÃªu chuáº©n vÄƒn báº£n-thÃ nh-hÃ¬nh áº£nh vÃ  vÆ°á»£t trá»™i hÆ¡n cÃ¡c mÃ´ hÃ¬nh chuyÃªn biá»‡t trong chá»‰nh sá»­a hÃ¬nh áº£nh. Kiáº¿n trÃºc thá»‘ng nháº¥t há»— trá»£ cÃ¡c tÃ¡c vá»¥ xen káº½ vÄƒn báº£n-hÃ¬nh áº£nh, cÃ³ kháº£ nÄƒng thá»±c hiá»‡n suy luáº­n Chain-of-Thought (CoT) Ä‘á»ƒ tinh chá»‰nh lá»i nháº¯c vÃ  há»c theo ngá»¯ cáº£nh (in-context learning) cho chá»‰nh sá»­a hÃ¬nh áº£nh zero-shot. NextFlow hiá»‡u quáº£ cao, yÃªu cáº§u Ã­t hÆ¡n 6 láº§n FLOPs trong quÃ¡ trÃ¬nh suy luáº­n so vá»›i cÃ¡c mÃ´ hÃ¬nh khuáº¿ch tÃ¡n dá»±a trÃªn MMDiT á»Ÿ Ä‘á»™ phÃ¢n giáº£i 1024x1024.

### Conclusion & Future Works:
NextFlow chá»©ng minh ráº±ng má»™t mÃ´ hÃ¬nh tá»± há»“i quy thá»‘ng nháº¥t cÃ³ thá»ƒ cáº¡nh tranh vá»›i cÃ¡c mÃ´ hÃ¬nh khuáº¿ch tÃ¡n tiÃªn tiáº¿n vá» cháº¥t lÆ°á»£ng hÃ¬nh áº£nh, Ä‘á»“ng thá»i giá»¯ Ä‘Æ°á»£c kháº£ nÄƒng suy luáº­n cá»§a LLMs. MÃ´ hÃ¬nh giáº£i quyáº¿t thÃ nh cÃ´ng cÃ¡c nÃºt tháº¯t vá» hiá»‡u quáº£ vÃ  máº­t Ä‘á»™ ngá»¯ nghÄ©a trong cÃ¡c mÃ´ hÃ¬nh Ä‘a phÆ°Æ¡ng thá»©c thá»‘ng nháº¥t. ThÃ´ng tin vá» cÃ¡c hÆ°á»›ng nghiÃªn cá»©u tiáº¿p theo khÃ´ng Ä‘Æ°á»£c cung cáº¥p trong Ä‘oáº¡n vÄƒn trÃ­ch dáº«n.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2601.02204](https://huggingface.co/papers/2601.02204) |
| ArXiv Abstract | [https://arxiv.org/abs/2601.02204](https://arxiv.org/abs/2601.02204) |
| PDF Download | [https://arxiv.org/pdf/2601.02204.pdf](https://arxiv.org/pdf/2601.02204.pdf) |
| Github Repository | [https://github.com/ByteVisionLab/NextFlow](https://github.com/ByteVisionLab/NextFlow) |

--- 

## 4. DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer

**TÃ¡c giáº£:** Xu Guo, Fulong Ye, Xinghui Li, Pengqi Tu, Pengze Zhang, Qichao Sun, Songtao Zhao, Xiangwang Hou, Qian He

**Xuáº¥t báº£n lÃºc:** 2026-01-04

### Main Problem:
Video Face Swapping (VFS) hiá»‡n táº¡i gáº·p khÃ³ khÄƒn trong viá»‡c duy trÃ¬ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng danh tÃ­nh (identity similarity), báº£o toÃ n thuá»™c tÃ­nh (attribute preservation) nhÆ° tÆ° tháº¿, biá»ƒu cáº£m, Ã¡nh sÃ¡ng, ná»n vÃ  thÃ´ng tin Ä‘á»™ng, cÅ©ng nhÆ° tÃ­nh nháº¥t quÃ¡n thá»i gian (temporal consistency). CÃ¡c phÆ°Æ¡ng phÃ¡p Image Face Swapping (IFS) tiÃªn tiáº¿n, dÃ¹ Ä‘áº¡t hiá»‡u suáº¥t tá»‘t vá» danh tÃ­nh vÃ  thuá»™c tÃ­nh, khi Ã¡p dá»¥ng trá»±c tiáº¿p cho video thÆ°á»ng gÃ¢y ra hiá»‡n tÆ°á»£ng nháº¥p nhÃ¡y (flickering) vÃ  rung láº¯c (jittering). CÃ¡c phÆ°Æ¡ng phÃ¡p VFS hiá»‡n cÃ³ cáº£i thiá»‡n tÃ­nh nháº¥t quÃ¡n thá»i gian nhÆ°ng váº«n cÃ²n kÃ©m xa IFS vá» kháº£ nÄƒng báº£o toÃ n danh tÃ­nh vÃ  thuá»™c tÃ­nh. NgoÃ i ra, cÃ³ sá»± háº¡n cháº¿ vá» cÃ¡c bá»™ dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ (benchmarks) toÃ n diá»‡n cho VFS.

### Main Idea:
BÃ i nghiÃªn cá»©u Ä‘á» xuáº¥t DreamID-V, má»™t framework toÃ n diá»‡n nháº±m thu háº¹p khoáº£ng cÃ¡ch giá»¯a IFS vÃ  VFS Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c trao Ä‘á»•i khuÃ´n máº·t video Ä‘á»™ chÃ¢n thá»±c cao. Giáº£i phÃ¡p bao gá»“m ba trá»¥ cá»™t chÃ­nh:
1.  **SyncID-Pipe:** Má»™t pipeline dá»¯ liá»‡u má»›i giÃºp chuyá»ƒn giao Æ°u Ä‘iá»ƒm cá»§a IFS sang lÄ©nh vá»±c video. NÃ³ tiá»n huáº¥n luyá»‡n má»™t Identity-Anchored Video Synthesizer (IVS) Ä‘iá»u khiá»ƒn báº±ng tÆ° tháº¿, sá»­ dá»¥ng cÆ¡ cháº¿ Adaptive Pose-Attention Ä‘á»ƒ Ä‘Æ°a thÃ´ng tin tÆ° tháº¿ vÃ o cÃ¡c mÃ´ hÃ¬nh ná»n táº£ng video First-Last-Frame. IVS sau Ä‘Ã³ Ä‘Æ°á»£c káº¿t há»£p vá»›i cÃ¡c mÃ´ hÃ¬nh IFS Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c bá»™ dá»¯ liá»‡u "bidirectional ID quadruplets" nháº±m cung cáº¥p giÃ¡m sÃ¡t rÃµ rÃ ng trong huáº¥n luyá»‡n. Pipeline nÃ y cÅ©ng bao gá»“m chiáº¿n lÆ°á»£c thÃ­ch á»©ng biá»ƒu cáº£m vÃ  cÆ¡ cháº¿ tÃ¡i táº¡o ná»n Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh phÃ¹ há»£p cá»§a dá»¯ liá»‡u.
2.  **DreamID-V Framework:** LÃ  framework trao Ä‘á»•i khuÃ´n máº·t video Ä‘áº§u tiÃªn dá»±a trÃªn Diffusion Transformer (DiT). NÃ³ sá»­ dá»¥ng module Modality-Aware Conditioning (MC) cá»‘t lÃµi Ä‘á»ƒ Ä‘Æ°a cÃ¡c Ä‘iá»u kiá»‡n Ä‘a phÆ°Æ¡ng thá»©c má»™t cÃ¡ch phÃ¢n biá»‡t, cho phÃ©p tÃ¡ch rá»i Ä‘iá»u kiá»‡n vÃ  há»£p nháº¥t tÃ­nh nÄƒng.
3.  **CÃ¡c CÆ¡ cháº¿ Huáº¥n luyá»‡n NÃ¢ng cao:** Äá»ƒ tÄƒng cÆ°á»ng tÃ­nh chÃ¢n thá»±c cá»§a hÃ¬nh áº£nh vÃ  tÃ­nh nháº¥t quÃ¡n danh tÃ­nh trong cÃ¡c ká»‹ch báº£n khÃ³ khÄƒn, framework thiáº¿t káº¿ cÆ¡ cháº¿ Synthetic-to-Real Curriculum vÃ  chiáº¿n lÆ°á»£c Identity-Coherence Reinforcement Learning.
NgoÃ i ra, bÃ i bÃ¡o giá»›i thiá»‡u **IDBench-V**, má»™t bá»™ dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ toÃ n diá»‡n má»›i bao gá»“m nhiá»u cáº£nh Ä‘a dáº¡ng cho nhiá»‡m vá»¥ trao Ä‘á»•i khuÃ´n máº·t video.

### Main Results:
Dá»±a trÃªn Ä‘oáº¡n trÃ­ch dáº«n, cÃ¡c káº¿t quáº£ chÃ­nh Ä‘Æ°á»£c cÃ´ng bá»‘ lÃ :
*   DreamID-V vÆ°á»£t trá»™i hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n Ä‘áº¡i (state-of-the-art) khÃ¡c trong cÃ¡c thá»­ nghiá»‡m rá»™ng rÃ£i, cáº£ vá» Ä‘á»‹nh lÆ°á»£ng vÃ  Ä‘á»‹nh tÃ­nh.
*   Framework thá»ƒ hiá»‡n tÃ­nh linh hoáº¡t Ä‘áº·c biá»‡t, cÃ³ thá»ƒ dá»… dÃ ng thÃ­ch á»©ng vá»›i nhiá»u nhiá»‡m vá»¥ liÃªn quan Ä‘áº¿n trao Ä‘á»•i khuÃ´n máº·t khÃ¡c nhau.
*   IDBench-V Ä‘Æ°á»£c giá»›i thiá»‡u nhÆ° má»™t bá»™ dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ toÃ n diá»‡n, phá»¥c vá»¥ cho viá»‡c Ä‘Ã¡nh giÃ¡ cÃ¡c phÆ°Æ¡ng phÃ¡p VFS trong cÃ¡c Ä‘iá»u kiá»‡n Ä‘a dáº¡ng.

### Conclusion & Future Works:
BÃ i nghiÃªn cá»©u Ä‘Ã£ phÃ¡t triá»ƒn má»™t framework toÃ n diá»‡n, bao gá»“m má»™t pipeline dá»¯ liá»‡u má»›i (SyncID-Pipe), má»™t kiáº¿n trÃºc mÃ´ hÃ¬nh Ä‘á»™t phÃ¡ (DreamID-V dá»±a trÃªn DiT) vÃ  má»™t bá»™ dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ má»›i (IDBench-V), Ä‘á»ƒ giáº£i quyáº¿t nhá»¯ng thÃ¡ch thá»©c cá»‘ há»¯u trong Video Face Swapping. DreamID-V Ä‘Ã£ chá»©ng minh hiá»‡u suáº¥t táº¡o video vÆ°á»£t trá»™i so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n Ä‘áº¡i vÃ  thá»ƒ hiá»‡n tÃ­nh linh hoáº¡t Ä‘Ã¡ng ká»ƒ, cÃ³ kháº£ nÄƒng thÃ­ch á»©ng linh hoáº¡t vá»›i cÃ¡c nhiá»‡m vá»¥ liÃªn quan Ä‘áº¿n trao Ä‘á»•i khuÃ´n máº·t khÃ¡c. Äoáº¡n trÃ­ch dáº«n khÃ´ng Ä‘i sÃ¢u vÃ o cÃ¡c cÃ´ng viá»‡c tÆ°Æ¡ng lai cá»¥ thá»ƒ, nhÆ°ng framework Ä‘Ã£ Ä‘áº·t ná»n mÃ³ng vá»¯ng cháº¯c cho viá»‡c phÃ¡t triá»ƒn VFS cháº¥t lÆ°á»£ng cao hÆ¡n vÃ  má»Ÿ ra nhiá»u hÆ°á»›ng nghiÃªn cá»©u tiá»m nÄƒng.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2601.01425](https://huggingface.co/papers/2601.01425) |
| ArXiv Abstract | [https://arxiv.org/abs/2601.01425](https://arxiv.org/abs/2601.01425) |
| PDF Download | [https://arxiv.org/pdf/2601.01425.pdf](https://arxiv.org/pdf/2601.01425.pdf) |
| Github Repository | [https://github.com/bytedance/DreamID-V](https://github.com/bytedance/DreamID-V) |

--- 

## 5. VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation

**TÃ¡c giáº£:** Shikun Sun, Liao Qu, Huichao Zhang, Yiheng Liu, Yangyang Song, Xian Li, Xu Wang, Yi Jiang, Daniel K. Du, Xinglong Wu, Jia Jia

**Xuáº¥t báº£n lÃºc:** 2026-01-05

### Main Problem:
Váº¥n Ä‘á» cá»‘t lÃµi mÃ  bÃ i bÃ¡o Ä‘á» cáº­p lÃ  "xung Ä‘á»™t chÃ­nh sÃ¡ch khÃ´ng Ä‘á»“ng bá»™" (asynchronous policy conflicts) trong cÃ¡c mÃ´ hÃ¬nh Visual Autoregressive (VAR) khi Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a báº±ng Há»c TÄƒng CÆ°á»ng (RL). CÃ¡c mÃ´ hÃ¬nh VAR hoáº¡t Ä‘á»™ng vá»›i cáº¥u trÃºc Ä‘áº§u vÃ o khÃ´ng Ä‘á»“ng nháº¥t (lÆ°á»›i token rá»i ráº¡c vá»›i hÃ¬nh dáº¡ng khÃ´ng gian thay Ä‘á»•i vÃ  sá»‘ lÆ°á»£ng token truy váº¥n biáº¿n Ä‘á»™ng) qua cÃ¡c bÆ°á»›c táº¡o áº£nh khÃ¡c nhau. Sá»± khÃ´ng Ä‘á»“ng nháº¥t nÃ y dáº«n Ä‘áº¿n quÃ¡ trÃ¬nh huáº¥n luyá»‡n RL khÃ´ng á»•n Ä‘á»‹nh, há»™i tá»¥ cháº­m vÃ  cÄƒn chá»‰nh dÆ°á»›i má»©c tá»‘i Æ°u, Ä‘áº·c biá»‡t khi Ã¡p dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p nhÆ° Group Relative Policy Optimization (GRPO) trá»±c tiáº¿p vÃ o cÃ¡c mÃ´ hÃ¬nh VAR chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh hÃ¬nh áº£nh.

### Main Idea:
BÃ i nghiÃªn cá»©u Ä‘á» xuáº¥t má»™t framework má»›i Ä‘á»ƒ tÄƒng cÆ°á»ng Group Relative Policy Optimization (GRPO) nháº±m giáº£i quyáº¿t má»™t cÃ¡ch rÃµ rÃ ng cÃ¡c xung Ä‘á»™t chÃ­nh sÃ¡ch khÃ´ng Ä‘á»“ng bá»™ trong mÃ´ hÃ¬nh VAR. Framework nÃ y tÃ­ch há»£p ba thÃ nh pháº§n hiá»‡p Ä‘á»“ng:
1.  **Value as Middle Return (VMR):** Má»™t pháº§n thÆ°á»Ÿng trung gian á»•n Ä‘á»‹nh Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ hÆ°á»›ng dáº«n quÃ¡ trÃ¬nh táº¡o áº£nh á»Ÿ giai Ä‘oáº¡n Ä‘áº§u, cung cáº¥p pháº£n há»“i dÃ y Ä‘áº·c, Ä‘á»™ biáº¿n thiÃªn tháº¥p vÃ  Ä‘áº£m báº£o tÃ­nh tá»‘i Æ°u cá»§a táº­p há»£p chÃ­nh sÃ¡ch. NÃ³ phÃ¢n tÃ¡ch má»¥c tiÃªu RL toÃ n chuá»—i thÃ nh má»™t bÃ i toÃ¡n tá»‘i Æ°u hÃ³a hai giai Ä‘oáº¡n, giáº£m thiá»ƒu xung Ä‘á»™t giá»¯a cÃ¡c bÆ°á»›c.
2.  **Per-Action Normalization Weighting (PANW):** Má»™t lÆ°á»£c Ä‘á»“ Ä‘iá»u chá»‰nh trá»ng sá»‘ Ä‘á»™ng theo bÆ°á»›c thá»i gian Ä‘á»ƒ phÃ¢n bá»• tÃ­n dá»¥ng chÃ­nh xÃ¡c. NÃ³ chuáº©n hÃ³a Ä‘Ã³ng gÃ³p cá»§a má»—i bÆ°á»›c báº±ng sá»‘ lÆ°á»£ng token truy váº¥n, giÃºp cÃ¢n báº±ng gradient trÃªn cÃ¡c tá»· lá»‡ khÃ¡c nhau vÃ  cáº£i thiá»‡n sá»± á»•n Ä‘á»‹nh.
3.  **Mask Propagation (MP):** Má»™t thuáº­t toÃ¡n lan truyá»n mask má»›i, láº¥y cáº£m há»©ng tá»« nguyÃªn táº¯c Reward Feedback Learning (ReFL), Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ cÃ´ láº­p hiá»‡u á»©ng tá»‘i Æ°u hÃ³a cáº£ vá» máº·t khÃ´ng gian vÃ  thá»i gian Ä‘á»‘i vá»›i cÃ¡c token cÃ³ trÃ¡ch nhiá»‡m chÃ­nh táº¡o ra pháº§n thÆ°á»Ÿng cuá»‘i cÃ¹ng.

### Main Results:
CÃ¡c phÃ¡t hiá»‡n chÃ­nh cho tháº¥y framework Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘Ã£ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ cháº¥t lÆ°á»£ng máº«u vÃ  Ä‘á»™ phÃ¹ há»£p vá»›i má»¥c tiÃªu so vá»›i baseline GRPO thÃ´ng thÆ°á»ng. Cá»¥ thá»ƒ, phÆ°Æ¡ng phÃ¡p nÃ y:
-   Thá»ƒ hiá»‡n sá»± á»•n Ä‘á»‹nh vÆ°á»£t trá»™i trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n vÃ  tÄƒng tá»‘c Ä‘á»™ há»™i tá»¥ so vá»›i GRPO vanilla.
-   Äáº¡t Ä‘Æ°á»£c nhá»¯ng cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ vá» cháº¥t lÆ°á»£ng máº«u vÃ  Ä‘á»™ phÃ¹ há»£p vá»›i má»¥c tiÃªu trÃªn cÃ¡c benchmark hiá»ƒn thá»‹ vÄƒn báº£n.
-   Äáº¡t Ä‘Æ°á»£c káº¿t quáº£ vÆ°á»£t trá»™i so vá»›i Ä‘iá»ƒm khá»Ÿi Ä‘áº§u TokenFlow-T2I vÃ  Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tiÃªn tiáº¿n (state-of-the-art) so vá»›i cÃ¡c baseline táº­p trung vÃ o diffusion.
-   BÃ i nghiÃªn cá»©u cung cáº¥p cháº©n Ä‘oÃ¡n vÃ  hÃ¬nh thá»©c hÃ³a cÃ¡c xung Ä‘á»™t chÃ­nh sÃ¡ch khÃ´ng Ä‘á»“ng bá»™ trong RL cho VAR, chá»©ng minh vai trÃ² quan trá»ng cá»§a VMR, PANW vÃ  MP trong viá»‡c cÃ¢n báº±ng gradient vÃ  tÄƒng cÆ°á»ng phÃ¢n bá»• tÃ­n dá»¥ng theo khÃ´ng gian-thá»i gian.

### Conclusion & Future Works:
BÃ i nghiÃªn cá»©u káº¿t luáº­n ráº±ng viá»‡c cáº¥u trÃºc Ä‘Ãºng Ä‘áº¯n má»¥c tiÃªu Há»c TÄƒng CÆ°á»ng vÃ  cÃ¢n báº±ng cÃ¡c cáº­p nháº­t trÃªn cÃ¡c bÆ°á»›c khÃ´ng Ä‘á»“ng nháº¥t lÃ  ráº¥t quan trá»ng Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c sá»± cÄƒn chá»‰nh Ä‘Ã¡ng tin cáº­y giá»¯a vÄƒn báº£n vÃ  hÃ¬nh áº£nh trong cÃ¡c mÃ´ hÃ¬nh Visual Autoregressive. Máº·c dÃ¹ vÄƒn báº£n Ä‘Æ°á»£c trÃ­ch dáº«n khÃ´ng Ä‘á» cáº­p rÃµ rÃ ng Ä‘áº¿n cÃ¡c hÆ°á»›ng nghiÃªn cá»©u trong tÆ°Æ¡ng lai, bÃ i bÃ¡o Ä‘Ã£ thiáº¿t láº­p má»™t framework RL há»‡ thá»‘ng Ä‘áº§u tiÃªn cho VAR chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh hÃ¬nh áº£nh, nháº¥n máº¡nh ráº±ng viá»‡c sá»­ dá»¥ng RL trong cÃ¡c mÃ´ hÃ¬nh AR dá»± Ä‘oÃ¡n theo thang Ä‘o tiáº¿p theo váº«n cÃ²n lÃ  má»™t lÄ©nh vá»±c Ã­t Ä‘Æ°á»£c khÃ¡m phÃ¡, Ä‘áº·c biá»‡t lÃ  viá»‡c giáº£i quyáº¿t cÃ¡c thÃ¡ch thá»©c cáº¥u trÃºc cá»‘t lÃµi do viá»‡c táº¡o token song song Ä‘a tá»· lá»‡ gÃ¢y ra.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2601.02256](https://huggingface.co/papers/2601.02256) |
| ArXiv Abstract | [https://arxiv.org/abs/2601.02256](https://arxiv.org/abs/2601.02256) |
| PDF Download | [https://arxiv.org/pdf/2601.02256.pdf](https://arxiv.org/pdf/2601.02256.pdf) |
| Github Repository | N/A |

--- 

## 6. GARDO: Reinforcing Diffusion Models without Reward Hacking

**TÃ¡c giáº£:** Haoran He, Yuxiao Ye, Jie Liu, Jiajun Liang, Zhiyong Wang, Ziyang Yuan, Xintao Wang, Hangyu Mao, Pengfei Wan, Ling Pan

**Xuáº¥t báº£n lÃºc:** 2025-12-30

### Main Problem:
Váº¥n Ä‘á» cá»‘t lÃµi lÃ  hiá»‡n tÆ°á»£ng "reward hacking" (hack pháº§n thÆ°á»Ÿng) khi tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh khuáº¿ch tÃ¡n (diffusion models) thÃ´ng qua há»c tÄƒng cÆ°á»ng (RL) trá»±c tuyáº¿n. Äiá»u nÃ y phÃ¡t sinh do viá»‡c sá»­ dá»¥ng pháº§n thÆ°á»Ÿng proxy (proxy reward) khÃ´ng hoÃ n háº£o, chá»‰ pháº£n Ã¡nh má»™t pháº§n má»¥c tiÃªu thá»±c sá»±, dáº«n Ä‘áº¿n viá»‡c cÃ¡c mÃ´ hÃ¬nh tá»‘i Æ°u hÃ³a Ä‘iá»ƒm sá»‘ proxy nhÆ°ng cháº¥t lÆ°á»£ng hÃ¬nh áº£nh thá»±c táº¿ suy giáº£m vÃ  sá»± Ä‘a dáº¡ng trong táº¡o máº«u bá»‹ máº¥t. CÃ¡c giáº£i phÃ¡p hiá»‡n cÃ³ sá»­ dá»¥ng KL regularization Ä‘á»ƒ ngÄƒn cháº·n "reward hacking" láº¡i lÃ m giáº£m hiá»‡u quáº£ láº¥y máº«u vÃ  cáº£n trá»Ÿ kháº£ nÄƒng khÃ¡m phÃ¡ cÃ¡c vÃ¹ng cÃ³ pháº§n thÆ°á»Ÿng cao má»›i.

### Main Idea:
BÃ i nghiÃªn cá»©u Ä‘á» xuáº¥t má»™t khuÃ´n khá»• Ä‘a nÄƒng mang tÃªn GARDO (Gated and Adaptive Regularization with Diversity-aware Optimization) Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c yÃªu cáº§u cáº¡nh tranh vá» hiá»‡u quáº£ láº¥y máº«u, khÃ¡m phÃ¡ vÃ  giáº£m thiá»ƒu "reward hacking". CÃ¡c Ã½ tÆ°á»Ÿng chÃ­nh cá»§a GARDO bao gá»“m:
1.  **Gated Regularization (Äiá»u hÃ²a cÃ³ cá»•ng):** KhÃ´ng Ã¡p dá»¥ng Ä‘iá»u hÃ²a phá»• biáº¿n cho táº¥t cáº£ cÃ¡c máº«u, mÃ  chá»‰ chá»n lá»c xá»­ pháº¡t má»™t táº­p há»£p con cÃ¡c máº«u cÃ³ Ä‘á»™ khÃ´ng cháº¯c cháº¯n cao vá» pháº§n thÆ°á»Ÿng. Äá»™ khÃ´ng cháº¯c cháº¯n nÃ y Ä‘Æ°á»£c Ä‘á»‹nh lÆ°á»£ng báº±ng sá»± khÃ´ng Ä‘á»“ng nháº¥t giá»¯a má»™t táº­p há»£p cÃ¡c hÃ m pháº§n thÆ°á»Ÿng.
2.  **Adaptive Regularization (Äiá»u hÃ²a thÃ­ch á»©ng):** CÆ¡ cháº¿ Ä‘iá»u hÃ²a thÃ­ch á»©ng, trong Ä‘Ã³ mÃ´ hÃ¬nh tham chiáº¿u Ä‘Æ°á»£c cáº­p nháº­t Ä‘á»‹nh ká»³ Ä‘á»ƒ phÃ¹ há»£p vá»›i kháº£ nÄƒng cá»§a chÃ­nh sÃ¡ch trá»±c tuyáº¿n, Ä‘áº£m báº£o má»¥c tiÃªu Ä‘iá»u hÃ²a luÃ´n phÃ¹ há»£p vÃ  há»— trá»£ khÃ¡m phÃ¡ liÃªn tá»¥c.
3.  **Diversity-aware Optimization (Tá»‘i Æ°u hÃ³a nháº­n biáº¿t Ä‘a dáº¡ng):** Khuáº¿ch Ä‘áº¡i pháº§n thÆ°á»Ÿng cho cÃ¡c máº«u cháº¥t lÆ°á»£ng cao Ä‘á»“ng thá»i thá»ƒ hiá»‡n sá»± Ä‘a dáº¡ng cao, khuyáº¿n khÃ­ch bao phá»§ cÃ¡c cháº¿ Ä‘á»™ (mode coverage) mÃ  khÃ´ng lÃ m máº¥t á»•n Ä‘á»‹nh quÃ¡ trÃ¬nh tá»‘i Æ°u hÃ³a.

### Main Results:
CÃ¡c thá»­ nghiá»‡m rá»™ng rÃ£i trÃªn nhiá»u pháº§n thÆ°á»Ÿng proxy khÃ¡c nhau vÃ  cÃ¡c chá»‰ sá»‘ Ä‘o lÆ°á»ng chÆ°a tá»«ng tháº¥y (hold-out unseen metrics) cho tháº¥y GARDO:
*   Giáº£m thiá»ƒu thÃ nh cÃ´ng hiá»‡n tÆ°á»£ng "reward hacking".
*   NÃ¢ng cao tÃ­nh Ä‘a dáº¡ng trong viá»‡c táº¡o ra máº«u.
*   KhÃ´ng lÃ m giáº£m hiá»‡u quáº£ láº¥y máº«u (sample efficiency).
*   KhÃ´ng cáº£n trá»Ÿ kháº£ nÄƒng khÃ¡m phÃ¡ (exploration).
Äiá»u nÃ y lÃ m ná»•i báº­t tÃ­nh hiá»‡u quáº£ vÃ  máº¡nh máº½ cá»§a phÆ°Æ¡ng phÃ¡p.

### Conclusion & Future Works:
BÃ i nghiÃªn cá»©u káº¿t luáº­n ráº±ng GARDO lÃ  má»™t phÆ°Æ¡ng phÃ¡p hiá»‡u quáº£ vÃ  máº¡nh máº½, cÃ³ kháº£ nÄƒng cÃ¢n báº±ng thÃ nh cÃ´ng giá»¯a cÃ¡c yÃªu cáº§u cáº¡nh tranh vá» hiá»‡u quáº£ láº¥y máº«u, khÃ¡m phÃ¡, Ä‘a dáº¡ng, Ä‘á»“ng thá»i giáº£m thiá»ƒu "reward hacking" trong quÃ¡ trÃ¬nh tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh táº¡o áº£nh. (Äoáº¡n vÄƒn trÃ­ch dáº«n khÃ´ng Ä‘á» cáº­p rÃµ rÃ ng Ä‘áº¿n cÃ¡c hÆ°á»›ng nghiÃªn cá»©u trong tÆ°Æ¡ng lai).

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.24138](https://huggingface.co/papers/2512.24138) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.24138](https://arxiv.org/abs/2512.24138) |
| PDF Download | [https://arxiv.org/pdf/2512.24138.pdf](https://arxiv.org/pdf/2512.24138.pdf) |
| Github Repository | [https://github.com/tinnerhrhe/GARDO](https://github.com/tinnerhrhe/GARDO) |

--- 

## 7. VINO: A Unified Visual Generator with Interleaved OmniModal Context

**TÃ¡c giáº£:** Junyi Chen, Tong He, Zhoujie Fu, Pengfei Wan, Kun Gai, Weicai Ye

**Xuáº¥t báº£n lÃºc:** 2026-01-05

### Main Problem:
CÃ¡c pipeline táº¡o hÃ¬nh áº£nh vÃ  video hiá»‡n táº¡i bá»‹ phÃ¢n máº£nh, Ä‘Ã²i há»i cÃ¡c mÃ´ hÃ¬nh riÃªng biá»‡t cho tá»«ng nhiá»‡m vá»¥ nhÆ° táº¡o tá»« vÄƒn báº£n sang hÃ¬nh áº£nh, tá»« vÄƒn báº£n sang video vÃ  chá»‰nh sá»­a hÃ¬nh áº£nh. Máº·c dÃ¹ cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n Ä‘a phÆ°Æ¡ng thá»©c (multimodal LLMs) cung cáº¥p kháº£ nÄƒng nháº­n thá»©c há»£p nháº¥t, chÃºng váº«n dá»±a vÃ o cÃ¡c mÃ´ hÃ¬nh khuáº¿ch tÃ¡n hoáº·c bá»™ giáº£i mÃ£ bÃªn ngoÃ i Ä‘á»ƒ táº¡o hÃ¬nh áº£nh Ä‘á»™ phÃ¢n giáº£i cao. NgoÃ i ra, cÃ¡c mÃ´ hÃ¬nh hiá»‡n táº¡i gáº·p khÃ³ khÄƒn trong viá»‡c xá»­ lÃ½ Ä‘á»“ng thá»i cÃ¡c tÃ­n hiá»‡u Ä‘a phÆ°Æ¡ng thá»©c khÃ¡c nhau, dáº«n Ä‘áº¿n xung Ä‘á»™t ngá»¯ nghÄ©a hoáº·c Ä‘iá»u kiá»‡n khÃ´ng nháº¥t quÃ¡n, Ä‘áº·c biá»‡t khi cÃ³ sá»± khÃ¡c biá»‡t giá»¯a cÃ¡c lá»‡nh mÃ´ táº£ phong phÃº vÃ  cÃ¡c hÆ°á»›ng dáº«n chá»‰nh sá»­a ngáº¯n gá»n.

### Main Idea:
VINO Ä‘á» xuáº¥t má»™t trÃ¬nh táº¡o hÃ¬nh áº£nh há»£p nháº¥t cÃ³ kháº£ nÄƒng táº¡o vÃ  chá»‰nh sá»­a hÃ¬nh áº£nh vÃ  video trong má»™t khuÃ´n khá»• duy nháº¥t báº±ng cÃ¡ch sá»­ dá»¥ng má»™t backbone khuáº¿ch tÃ¡n chia sáº» Ä‘Æ°á»£c Ä‘iá»u kiá»‡n hÃ³a bá»Ÿi vÄƒn báº£n, hÃ¬nh áº£nh vÃ  video. NÃ³ káº¿t há»£p má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯-thá»‹ giÃ¡c (VLM) vá»›i má»™t Bá»™ biáº¿n Ä‘á»•i khuáº¿ch tÃ¡n Ä‘a phÆ°Æ¡ng thá»©c (MMDiT), nÆ¡i cÃ¡c Ä‘áº§u vÃ o Ä‘a phÆ°Æ¡ng thá»©c Ä‘Æ°á»£c mÃ£ hÃ³a thÃ nh cÃ¡c token Ä‘iá»u kiá»‡n xen káº½ vÃ  sau Ä‘Ã³ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ hÆ°á»›ng dáº«n quÃ¡ trÃ¬nh khuáº¿ch tÃ¡n. Kiáº¿n trÃºc nÃ y bao gá»“m cÃ¡c token truy váº¥n cÃ³ thá»ƒ há»c Ä‘Æ°á»£c táº¡i Ä‘áº§u vÃ o VLM Ä‘á»ƒ cáº£i thiá»‡n kháº£ nÄƒng Ä‘iá»u kiá»‡n vÃ  á»•n Ä‘á»‹nh, cÃ¹ng vá»›i cÆ¡ cháº¿ Ä‘Ã¡nh dáº¥u ranh giá»›i token Ä‘á»ƒ liÃªn káº¿t nháº¥t quÃ¡n cÃ¡c biá»ƒu diá»…n ngá»¯ nghÄ©a (VLM) vÃ  tiá»m áº©n (VAE) cá»§a cÃ¡c tham chiáº¿u trá»±c quan. Má»™t chiáº¿n lÆ°á»£c huáº¥n luyá»‡n Ä‘a giai Ä‘oáº¡n cÅ©ng Ä‘Æ°á»£c giá»›i thiá»‡u Ä‘á»ƒ dáº§n dáº§n má»Ÿ rá»™ng má»™t mÃ´ hÃ¬nh táº¡o video cÆ¡ báº£n thÃ nh má»™t trÃ¬nh táº¡o Ä‘a tÃ¡c vá»¥ cÃ³ kháº£ nÄƒng xá»­ lÃ½ cáº£ Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra hÃ¬nh áº£nh vÃ  video.

### Main Results:
VINO thá»ƒ hiá»‡n cháº¥t lÆ°á»£ng hÃ¬nh áº£nh máº¡nh máº½, tuÃ¢n thá»§ hÆ°á»›ng dáº«n má»™t cÃ¡ch trung thá»±c, cáº£i thiá»‡n kháº£ nÄƒng báº£o toÃ n tham chiáº¿u vÃ  thuá»™c tÃ­nh, vÃ  cho phÃ©p chá»‰nh sá»­a Ä‘a danh tÃ­nh dá»… kiá»ƒm soÃ¡t hÆ¡n. CÃ¡c cÆ¡ cháº¿ Ä‘á» xuáº¥t giÃºp cáº£i thiá»‡n kháº£ nÄƒng Ä‘iá»u kiá»‡n Ä‘a phÆ°Æ¡ng thá»©c, á»•n Ä‘á»‹nh quÃ¡ trÃ¬nh tá»‘i Æ°u hÃ³a vÃ  giáº£m thiá»ƒu viá»‡c hoÃ¡n Ä‘á»•i danh tÃ­nh cÅ©ng nhÆ° rÃ² rá»‰ thuá»™c tÃ­nh trong cÃ¡c cáº£nh tham chiáº¿u Ä‘a phÆ°Æ¡ng thá»©c phá»©c táº¡p. MÃ´ hÃ¬nh cung cáº¥p má»™t con Ä‘Æ°á»ng kháº£ thi hÆ¡n Ä‘á»ƒ táº¡o hÃ¬nh áº£nh há»£p nháº¥t vÃ  cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng so vá»›i cÃ¡c pipeline khuáº¿ch tÃ¡n chuyÃªn biá»‡t hiá»‡n cÃ³.

### Conclusion & Future Works:
BÃ i bÃ¡o trÃ¬nh bÃ y VINO nhÆ° má»™t khuÃ´n khá»• kháº£ thi cho viá»‡c táº¡o hÃ¬nh áº£nh há»£p nháº¥t vÃ  cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng. NÃ³ nháº¥n máº¡nh tiá»m nÄƒng cá»§a tÃ­nh toÃ¡n xen káº½, trong ngá»¯ cáº£nh nhÆ° má»™t ná»n táº£ng cho viá»‡c táº¡o hÃ¬nh áº£nh Ä‘a má»¥c Ä‘Ã­ch, gá»£i má»Ÿ hÆ°á»›ng phÃ¡t triá»ƒn cho cÃ¡c há»‡ thá»‘ng táº¡o hÃ¬nh áº£nh tá»•ng quÃ¡t hÆ¡n trong tÆ°Æ¡ng lai.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2601.02358](https://huggingface.co/papers/2601.02358) |
| ArXiv Abstract | [https://arxiv.org/abs/2601.02358](https://arxiv.org/abs/2601.02358) |
| PDF Download | [https://arxiv.org/pdf/2601.02358.pdf](https://arxiv.org/pdf/2601.02358.pdf) |
| Github Repository | [https://github.com/SOTAMak1r/VINO-code](https://github.com/SOTAMak1r/VINO-code) |

--- 

## 8. InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams

**TÃ¡c giáº£:** Shuai Yuan, Yantai Yang, Xiaotian Yang, Xupeng Zhang, Zhonghao Zhao, Lingming Zhang, Zhipeng Zhang

**Xuáº¥t báº£n lÃºc:** 2026-01-05

### Main Problem:
Váº¥n Ä‘á» cá»‘t lÃµi lÃ  sá»± khÃ³ khÄƒn trong viá»‡c Ä‘áº¡t Ä‘Æ°á»£c kháº£ nÄƒng hiá»ƒu hÃ¬nh há»c 3D trá»±c quan quy mÃ´ lá»›n, liÃªn tá»¥c cho cÃ¡c luá»“ng dá»¯ liá»‡u vÃ´ táº­n. CÃ¡c mÃ´ hÃ¬nh ngoáº¡i tuyáº¿n hiá»‡n cÃ³, nhÆ° VGGT, khÃ´ng phÃ¹ há»£p cho cÃ¡c há»‡ thá»‘ng trá»±c tiáº¿p do tÃ­nh cháº¥t xá»­ lÃ½ theo lÃ´ vÃ  yÃªu cáº§u bá»™ nhá»› GPU lá»›n. CÃ¡c kiáº¿n trÃºc trá»±c tuyáº¿n, dÃ¹ Ä‘Æ°á»£c thiáº¿t káº¿ cho hoáº¡t Ä‘á»™ng thá»i gian thá»±c, láº¡i gáº·p pháº£i váº¥n Ä‘á» tÃ­ch lÅ©y bá»™ nhá»› KV khÃ´ng giá»›i háº¡n, dáº«n Ä‘áº¿n chi phÃ­ tÃ­nh toÃ¡n vÃ  bá»™ nhá»› cao khÃ´ng bá»n vá»¯ng, hoáº·c nÃ©n tráº¡ng thÃ¡i ngáº§m Ä‘á»‹nh lÃ m máº¥t thÃ´ng tin quan trá»ng vÃ  gÃ¢y ra hiá»‡n tÆ°á»£ng trÃ´i dáº¡t dá»¯ liá»‡u tháº£m khá»‘c qua cÃ¡c chuá»—i dÃ i. Má»™t thÃ¡ch thá»©c lá»›n ná»¯a lÃ  sá»± dÆ° thá»«a token cáº¥p Ä‘á»™ cao trong bá»™ nhá»› KV cá»§a cÃ¡c quá»¹ Ä‘áº¡o camera liÃªn tá»¥c, khiáº¿n kÃ­ch thÆ°á»›c bá»™ nhá»› tÄƒng nhanh. CÃ¡c phÆ°Æ¡ng phÃ¡p cáº¯t tá»‰a truyá»n thá»‘ng khÃ´ng thá»ƒ Ã¡p dá»¥ng Ä‘Æ°á»£c vá»›i cÃ¡c kernel tá»‘i Æ°u hÃ³a nhÆ° FlashAttention vÃ¬ chÃºng dá»±a vÃ o viá»‡c truy cáº­p trá»ng sá»‘ chÃº Ã½, Ä‘iá»u mÃ  cÃ¡c kernel nÃ y bá» qua Ä‘á»ƒ Ä‘áº¡t tá»‘c Ä‘á»™.

### Main Idea:
InfiniteVGGT Ä‘á» xuáº¥t má»™t kiáº¿n trÃºc transformer hÃ¬nh há»c trá»±c quan nhÃ¢n quáº£ má»›i vá»›i khÃ¡i niá»‡m "bá»™ nhá»› cuá»™n". Giáº£i phÃ¡p nÃ y sá»­ dá»¥ng bá»™ nhá»› KV cÃ³ giá»›i háº¡n nhÆ°ng thÃ­ch á»©ng vÃ  luÃ´n thá»ƒ hiá»‡n tá»‘t. InfiniteVGGT Ã¡p dá»¥ng má»™t chiáº¿n lÆ°á»£c cáº¯t tá»‰a khÃ´ng cáº§n huáº¥n luyá»‡n vÃ  khÃ´ng phá»¥ thuá»™c vÃ o cÆ¡ cháº¿ chÃº Ã½, dá»±a trÃªn Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine cá»§a khÃ³a lÃ m tiÃªu chÃ­ quan trá»ng cá»§a token, Ä‘á»ƒ loáº¡i bá» thÃ´ng tin lá»—i thá»i vÃ  dÆ° thá»«a má»™t cÃ¡ch thÃ´ng minh, tá»« Ä‘Ã³ "cuá»™n" bá»™ nhá»› vá» phÃ­a trÆ°á»›c vá»›i má»—i khung hÃ¬nh má»›i. PhÆ°Æ¡ng phÃ¡p nÃ y hoÃ n toÃ n tÆ°Æ¡ng thÃ­ch vá»›i FlashAttention. Bá»™ nhá»› cuá»™n nÃ y hoáº¡t Ä‘á»™ng báº±ng cÃ¡ch liÃªn tá»¥c vÃ  Ä‘á»™ng lÃ m má»›i ná»™i dung thÃ´ng qua má»™t chiáº¿n lÆ°á»£c duy trÃ¬ Ä‘a cáº¥p, táº­p trung vÃ o viá»‡c báº£o toÃ n cÃ¡c token riÃªng láº» thay vÃ¬ xÃ³a toÃ n bá»™ khung hÃ¬nh, vÃ  Ä‘Æ°á»£c quáº£n lÃ½ bá»Ÿi má»™t ngÃ¢n sÃ¡ch token Ä‘á»™ng, phÃ¢n bá»• theo tá»«ng lá»›p kiáº¿n trÃºc, Ä‘áº£m báº£o dáº¥u chÃ¢n bá»™ nhá»› GPU bá»‹ giá»›i háº¡n. NghiÃªn cá»©u cÅ©ng giá»›i thiá»‡u benchmark Long3D Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ nghiÃªm ngáº·t kháº£ nÄƒng Æ°á»›c tÃ­nh hÃ¬nh há»c 3D liÃªn tá»¥c trÃªn cÃ¡c chuá»—i dÃ i Ä‘áº¿n khoáº£ng 10.000 khung hÃ¬nh.

### Main Results:
InfiniteVGGT Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t vÆ°á»£t trá»™i so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p streaming hiá»‡n cÃ³ vá» Ä‘á»™ á»•n Ä‘á»‹nh dÃ i háº¡n. NÃ³ cho phÃ©p streaming vá»›i chÃ¢n trá»i vÃ´ háº¡n Ä‘á»“ng thá»i thá»±c hiá»‡n tÃ¡i táº¡o máº¡nh máº½, vÃ´ háº¡n mÃ  khÃ´ng gáº·p pháº£i tÃ¬nh tráº¡ng trÃ n bá»™ nhá»›. NghiÃªn cá»©u Ä‘Ã£ chá»©ng minh kháº£ nÄƒng Ä‘á»™c Ä‘Ã¡o nÃ y trÃªn cÃ¡c benchmark chuá»—i dÃ i. NgoÃ i ra, viá»‡c giá»›i thiá»‡u benchmark Long3D Ä‘Ã£ cung cáº¥p má»™t ná»n táº£ng Ä‘Ã¡nh giÃ¡ Ä‘á»‹nh nghÄ©a cho nghiÃªn cá»©u tÆ°Æ¡ng lai vá» hiá»ƒu vÃ  tÃ¡i táº¡o hÃ¬nh há»c 3D dÃ i háº¡n, láº¥p Ä‘áº§y má»™t khoáº£ng trá»‘ng quan trá»ng trong lÄ©nh vá»±c nÃ y. Benchmark nÃ y láº§n Ä‘áº§u tiÃªn cho phÃ©p Ä‘Ã¡nh giÃ¡ nghiÃªm ngáº·t Æ°á»›c tÃ­nh hÃ¬nh há»c 3D liÃªn tá»¥c trÃªn cÃ¡c chuá»—i khoáº£ng 10.000 khung hÃ¬nh.

### Conclusion & Future Works:
NghiÃªn cá»©u nÃ y giá»›i thiá»‡u InfiniteVGGT, má»™t kiáº¿n trÃºc bá»™ nhá»› vÃ´ háº¡n cho viá»‡c hiá»ƒu hÃ¬nh há»c 3D liÃªn tá»¥c, Ä‘Æ°á»£c xÃ¢y dá»±ng trÃªn má»™t há»‡ thá»‘ng bá»™ nhá»› tÆ°á»ng minh, Ä‘á»™ng vÃ  cÃ³ thá»ƒ giáº£i thÃ­ch Ä‘Æ°á»£c. InfiniteVGGT Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tiÃªn tiáº¿n trÃªn cÃ¡c benchmark chuá»—i dÃ i vÃ  cÃ³ kháº£ nÄƒng Ä‘á»™c Ä‘Ã¡o lÃ  tÃ¡i táº¡o máº¡nh máº½, vÃ´ háº¡n mÃ  khÃ´ng bá»‹ trÃ n bá»™ nhá»›. Cuá»‘i cÃ¹ng, benchmark Long3D má»›i Ä‘Æ°á»£c giá»›i thiá»‡u nháº±m má»¥c Ä‘Ã­ch Ä‘Ã¡nh giÃ¡ nghiÃªm ngáº·t hiá»‡u suáº¥t dÃ i háº¡n, cung cáº¥p má»™t ná»n táº£ng Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng cho nghiÃªn cá»©u tÆ°Æ¡ng lai trong lÄ©nh vá»±c hiá»ƒu hÃ¬nh há»c 3D dÃ i háº¡n.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2601.02281](https://huggingface.co/papers/2601.02281) |
| ArXiv Abstract | [https://arxiv.org/abs/2601.02281](https://arxiv.org/abs/2601.02281) |
| PDF Download | [https://arxiv.org/pdf/2601.02281.pdf](https://arxiv.org/pdf/2601.02281.pdf) |
| Github Repository | [https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT](https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT) |

--- 

## 9. Recursive Language Models

**TÃ¡c giáº£:** Alex L. Zhang, Tim Kraska, Omar Khattab

**Xuáº¥t báº£n lÃºc:** 2025-12-31

### Main Problem:
CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) hiá»‡n Ä‘áº¡i cÃ³ giá»›i háº¡n vá» Ä‘á»™ dÃ i ngá»¯ cáº£nh vÃ  cháº¥t lÆ°á»£ng suy giáº£m Ä‘Ã¡ng ká»ƒ khi ngá»¯ cáº£nh dÃ i hÆ¡n ("context rot"), ngay cáº£ trong giá»›i háº¡n cho phÃ©p cá»§a chÃºng. Äiá»u nÃ y gÃ¢y khÃ³ khÄƒn cho viá»‡c xá»­ lÃ½ cÃ¡c nháº¯c nhá»Ÿ (prompts) dÃ i tÃ¹y Ã½, Ä‘áº·c biá»‡t lÃ  trong cÃ¡c tÃ¡c vá»¥ yÃªu cáº§u hÃ ng triá»‡u token. CÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n cÃ³ nhÆ° tÃ³m táº¯t (compaction) thÆ°á»ng khÃ´ng Ä‘á»§ biá»ƒu cáº£m vÃ  bá» qua cÃ¡c chi tiáº¿t quan trá»ng.

### Main Idea:
BÃ i bÃ¡o Ä‘á» xuáº¥t MÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘á»‡ quy (Recursive Language Models - RLMs) nhÆ° má»™t chiáº¿n lÆ°á»£c suy luáº­n tá»•ng quÃ¡t Ä‘á»ƒ má»Ÿ rá»™ng Ä‘Ã¡ng ká»ƒ Ä‘á»™ dÃ i ngá»¯ cáº£nh hiá»‡u quáº£ cá»§a LLMs. Ã tÆ°á»Ÿng chÃ­nh lÃ  coi cÃ¡c nháº¯c nhá»Ÿ dÃ i nhÆ° má»™t pháº§n cá»§a "mÃ´i trÆ°á»ng bÃªn ngoÃ i" mÃ  LLM cÃ³ thá»ƒ tÆ°Æ¡ng tÃ¡c má»™t cÃ¡ch tÆ°á»£ng trÆ°ng, thay vÃ¬ náº¡p trá»±c tiáº¿p vÃ o máº¡ng lÆ°á»›i tháº§n kinh. RLMs khá»Ÿi táº¡o má»™t mÃ´i trÆ°á»ng láº­p trÃ¬nh REPL (Read-Eval-Print Loop) nÆ¡i prompt Ä‘Æ°á»£c Ä‘áº·t lÃ m giÃ¡ trá»‹ cá»§a má»™t biáº¿n. LLM sau Ä‘Ã³ cÃ³ thá»ƒ viáº¿t mÃ£ Ä‘á»ƒ kiá»ƒm tra, phÃ¢n tÃ¡ch nháº¯c nhá»Ÿ, quan sÃ¡t cÃ¡c hiá»‡u á»©ng phá»¥ tá»« viá»‡c thá»±c thi mÃ£ vÃ  tá»± gá»i láº¡i má»™t cÃ¡ch Ä‘á»‡ quy trÃªn cÃ¡c Ä‘oáº¡n mÃ£ con.

### Main Results:
RLMs thÃ nh cÃ´ng trong viá»‡c xá»­ lÃ½ cÃ¡c Ä‘áº§u vÃ o cÃ³ Ä‘á»™ dÃ i lá»›n hÆ¡n Ä‘áº¿n hai báº­c so vá»›i cá»­a sá»• ngá»¯ cáº£nh cá»§a mÃ´ hÃ¬nh cÆ¡ sá»Ÿ (lÃªn Ä‘áº¿n hÆ¡n 10 triá»‡u token).
- So vá»›i cÃ¡c LLM cÆ¡ sá»Ÿ vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p xá»­ lÃ½ ngá»¯ cáº£nh dÃ i thÃ´ng thÆ°á»ng (tÃ³m táº¯t, tÃ¡c nhÃ¢n cÃ´ng cá»¥ truy xuáº¥t, tÃ¡c nhÃ¢n táº¡o mÃ£), RLMs vÆ°á»£t trá»™i vá» cháº¥t lÆ°á»£ng trÃªn bá»‘n tÃ¡c vá»¥ ngá»¯ cáº£nh dÃ i Ä‘a dáº¡ng, bao gá»“m Deep Research, Information Aggregation, Code Repository Understanding vÃ  má»™t tÃ¡c vá»¥ suy luáº­n cáº·p tá»•ng há»£p.
- RLMs duy trÃ¬ hiá»‡u suáº¥t máº¡nh máº½ vá»›i sá»± suy giáº£m Ã­t nghiÃªm trá»ng hÆ¡n Ä‘Ã¡ng ká»ƒ khi Ä‘á»™ dÃ i Ä‘áº§u vÃ o vÃ  Ä‘á»™ phá»©c táº¡p tÃ¡c vá»¥ tÄƒng lÃªn (nhÆ° minh há»a trÃªn S-NIAH, OOLONG vÃ  OOLONG-Pairs).
- Chi phÃ­ cho má»—i truy váº¥n cá»§a RLMs tÆ°Æ¡ng Ä‘Æ°Æ¡ng hoáº·c tháº­m chÃ­ ráº» hÆ¡n so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c. VÃ­ dá»¥, trÃªn BrowseComp-Plus (1K), RLM (GPT-5) cÃ³ chi phÃ­ trung bÃ¬nh lÃ  0.99 USD vÃ  vÆ°á»£t trá»™i hÆ¡n cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ tÃ³m táº¯t vÃ  truy xuáº¥t hÆ¡n 29%.

### Conclusion & Future Works:
RLMs cung cáº¥p má»™t giáº£i phÃ¡p hiá»‡u quáº£ vÃ  cÃ³ thá»ƒ má»Ÿ rá»™ng Ä‘á»ƒ LLMs xá»­ lÃ½ cÃ¡c ngá»¯ cáº£nh cá»±c dÃ i, vÆ°á»£t qua cÃ¡c giá»›i háº¡n cá»§a cá»­a sá»• ngá»¯ cáº£nh truyá»n thá»‘ng vÃ  hiá»‡u suáº¥t kÃ©m dáº§n ("context rot"). PhÆ°Æ¡ng phÃ¡p nÃ y mang láº¡i hiá»‡u suáº¥t vÆ°á»£t trá»™i vÃ  chi phÃ­ cáº¡nh tranh. VÄƒn báº£n trÃ­ch dáº«n khÃ´ng Ä‘á» cáº­p rÃµ rÃ ng Ä‘áº¿n cÃ¡c hÆ°á»›ng nghiÃªn cá»©u trong tÆ°Æ¡ng lai.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.24601](https://huggingface.co/papers/2512.24601) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.24601](https://arxiv.org/abs/2512.24601) |
| PDF Download | [https://arxiv.org/pdf/2512.24601.pdf](https://arxiv.org/pdf/2512.24601.pdf) |
| Github Repository | [https://github.com/alexzhang13/rlm/tree/main](https://github.com/alexzhang13/rlm/tree/main) |

--- 

## 10. Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling

**TÃ¡c giáº£:** Falcon LLM Team, Iheb Chaabane, Puneesh Khanna, Suhail Mohmad, Slim Frikha, Shi Hu, Abdalgader Abubaker, Reda Alami, Mikhail Lubinets, Mohamed El Amine Seddik, Hakim Hacid

**Xuáº¥t báº£n lÃºc:** 2026-01-05

### Main Problem:
CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) gáº·p khÃ³ khÄƒn trong viá»‡c cÃ¢n báº±ng hiá»‡u suáº¥t suy luáº­n cao vá»›i chi phÃ­ suy luáº­n (inference cost) tháº¥p, Ä‘áº·c biá»‡t khi Ã¡p dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p Test-Time Scaling (TTS) vá»‘n tá»‘n kÃ©m tÃ i nguyÃªn. Váº¥n Ä‘á» cá»‘t lÃµi lÃ  lÃ m tháº¿ nÃ o Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c kháº£ nÄƒng suy luáº­n cáº¡nh tranh vá»›i cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ nhá» (SLM) mÃ  váº«n Ä‘áº£m báº£o hiá»‡u quáº£ vá» chi phÃ­ vÃ  tÃ i nguyÃªn.

### Main Idea:
BÃ i nghiÃªn cá»©u giá»›i thiá»‡u Falcon-H1R, má»™t mÃ´ hÃ¬nh 7B tham sá»‘ Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho suy luáº­n, sá»­ dá»¥ng kiáº¿n trÃºc lai Transformer-Mamba Ä‘á»ƒ Ä‘áº¡t hiá»‡u suáº¥t suy luáº­n cáº¡nh tranh vá»›i cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n. MÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c huáº¥n luyá»‡n thÃ´ng qua Supervised Fine-Tuning (SFT) trÃªn dá»¯ liá»‡u Ä‘Æ°á»£c tuyá»ƒn chá»n ká»¹ lÆ°á»¡ng vÃ  Reinforcement Learning (RL) vá»›i phÆ°Æ¡ng phÃ¡p GRPO. Falcon-H1R Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ nÃ¢ng cao hiá»‡u quáº£ Test-Time Scaling (TTS) báº±ng cÃ¡ch káº¿t há»£p suy luáº­n nhanh, hiá»‡u quáº£ token vÃ  Ä‘á»™ chÃ­nh xÃ¡c cao, Ä‘áº·c biá»‡t khi tÃ­ch há»£p phÆ°Æ¡ng phÃ¡p DeepConf Ä‘á»ƒ tá»‘i Æ°u hÃ³a viá»‡c táº¡o chuá»—i suy luáº­n song song vÃ  dá»«ng sá»›m.

### Main Results:
Falcon-H1R-7B Ä‘áº¡t hiá»‡u suáº¥t suy luáº­n vÆ°á»£t trá»™i hoáº·c ngang báº±ng cÃ¡c mÃ´ hÃ¬nh SOTA lá»›n hÆ¡n tá»« 2 Ä‘áº¿n 7 láº§n, thá»ƒ hiá»‡n hiá»‡u quáº£ tham sá»‘ cao. MÃ´ hÃ¬nh Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c máº¡nh máº½ trÃªn nhiá»u bá»™ benchmark suy luáº­n, bao gá»“m 88.1% trÃªn AIME24, 83.1% trÃªn AIME25, 64.9% trÃªn HMMT25, 36.3% trÃªn AMO-Bench vÃ  68.6% trÃªn LiveCodeBenchv6, cáº¡nh tranh vá»›i cÃ¡c mÃ´ hÃ¬nh SOTA lá»›n hÆ¡n nhÆ° GPT-OSS-20B vÃ  Qwen3-32B. Khi Ã¡p dá»¥ng Test-Time Scaling vá»›i DeepConf, Falcon-H1R-7B cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ cáº£ Ä‘á»™ chÃ­nh xÃ¡c vÃ  hiá»‡u quáº£ chi phÃ­. VÃ­ dá»¥, trÃªn AIME25, mÃ´ hÃ¬nh Ä‘áº¡t 96.7% Ä‘á»™ chÃ­nh xÃ¡c Ä‘á»“ng thá»i giáº£m 38% sá»‘ lÆ°á»£ng token sá»­ dá»¥ng so vá»›i DeepSeek-R1-0528-Qwen3-8B. Äiá»u nÃ y Ä‘áº¿n tá»« viá»‡c tá»‘i Æ°u hÃ³a Ä‘á»“ng thá»i Ä‘á»™ chÃ­nh xÃ¡c cao, hiá»‡u quáº£ token vÃ  tá»‘c Ä‘á»™ suy luáº­n trong mÃ´i trÆ°á»ng tÆ° duy song song.

### Conclusion & Future Works:
Falcon-H1R chá»©ng minh ráº±ng cÃ¡c mÃ´ hÃ¬nh nhá» gá»n, thÃ´ng qua cÃ¡c lá»±a chá»n kiáº¿n trÃºc lai má»¥c tiÃªu vÃ  chiáº¿n lÆ°á»£c huáº¥n luyá»‡n cáº©n tháº­n (bao gá»“m SFT vÃ  RL), cÃ³ thá»ƒ mang láº¡i kháº£ nÄƒng suy luáº­n máº¡nh máº½, cÃ³ thá»ƒ má»Ÿ rá»™ng vÃ  hiá»‡u quáº£ vá» máº·t chi phÃ­. MÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c coi lÃ  má»™t ná»n táº£ng hiá»‡u quáº£ cho cÃ¡c tÃ¡c vá»¥ suy luáº­n Ä‘Ã²i há»i cáº£ Ä‘á»™ chÃ­nh xÃ¡c cao vÃ  kháº£ nÄƒng má»Ÿ rá»™ng, Ä‘áº·c biá»‡t trong cÃ¡c ká»‹ch báº£n cáº§n táº¡o ra chuá»—i suy luáº­n dÃ i vÃ  Test-Time Scaling song song.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2601.02346](https://huggingface.co/papers/2601.02346) |
| ArXiv Abstract | [https://arxiv.org/abs/2601.02346](https://arxiv.org/abs/2601.02346) |
| PDF Download | [https://arxiv.org/pdf/2601.02346.pdf](https://arxiv.org/pdf/2601.02346.pdf) |
| Github Repository | N/A |

--- 

## 11. SimpleMem: Efficient Lifelong Memory for LLM Agents

**TÃ¡c giáº£:** Jiaqi Liu, Yaofeng Su, Peng Xia, Siwei Han, Zeyu Zheng, Cihang Xie, Mingyu Ding, Huaxiu Yao

**Xuáº¥t báº£n lÃºc:** 2026-01-05

### Main Problem:
CÃ¡c tÃ¡c nhÃ¢n LLM hiá»‡n táº¡i gáº·p khÃ³ khÄƒn trong tÆ°Æ¡ng tÃ¡c dÃ i háº¡n do giá»›i háº¡n cá»­a sá»• ngá»¯ cáº£nh vÃ  há»‡ thá»‘ng bá»™ nhá»› khÃ´ng hiá»‡u quáº£, dáº«n Ä‘áº¿n sá»± dÆ° thá»«a thÃ´ng tin Ä‘Ã¡ng ká»ƒ vÃ  chi phÃ­ token cao. CÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n cÃ³ hoáº·c lÆ°u trá»¯ toÃ n bá»™ lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c, gÃ¢y ra tÃ¬nh tráº¡ng "phá»“ng ngá»¯ cáº£nh" vÃ  suy giáº£m hiá»‡u suáº¥t, hoáº·c dá»±a vÃ o suy luáº­n láº·p láº¡i Ä‘á»ƒ lá»c nhiá»…u, nhÆ°ng láº¡i tá»‘n kÃ©m vá» máº·t tÃ­nh toÃ¡n vÃ  token. Váº¥n Ä‘á» cá»‘t lÃµi lÃ  hiá»‡u quáº£ truy xuáº¥t kÃ©m vÃ  sá»­ dá»¥ng token tháº¥p.

### Main Idea:
BÃ i nghiÃªn cá»©u giá»›i thiá»‡u SimpleMem, má»™t khuÃ´n khá»• bá»™ nhá»› hiá»‡u quáº£ cho cÃ¡c tÃ¡c nhÃ¢n LLM dá»±a trÃªn nÃ©n ngá»¯ nghÄ©a khÃ´ng máº¥t mÃ¡t. SimpleMem giáº£i quyáº¿t váº¥n Ä‘á» báº±ng má»™t quy trÃ¬nh ba giai Ä‘oáº¡n:
1.  **Semantic Structured Compression**: Ãp dá»¥ng cÆ¡ cháº¿ lá»c nháº­n biáº¿t entropy Ä‘á»ƒ cháº¯t lá»c cÃ¡c tÆ°Æ¡ng tÃ¡c phi cáº¥u trÃºc thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ bá»™ nhá»› nÃ©n, Ä‘Æ°á»£c láº­p chá»‰ má»¥c Ä‘a chiá»u (nhÃºng ngá»¯ nghÄ©a dÃ y Ä‘áº·c, Ä‘áº·c Ä‘iá»ƒm tá»« vá»±ng thÆ°a thá»›t vÃ  siÃªu dá»¯ liá»‡u kÃ½ hiá»‡u).
2.  **Recursive Memory Consolidation**: Má»™t quÃ¡ trÃ¬nh khÃ´ng Ä‘á»“ng bá»™ tÃ­ch há»£p cÃ¡c Ä‘Æ¡n vá»‹ liÃªn quan vÃ o cÃ¡c biá»ƒu diá»…n trá»«u tÆ°á»£ng cáº¥p cao hÆ¡n Ä‘á»ƒ giáº£m sá»± dÆ° thá»«a vÃ  duy trÃ¬ cáº¥u trÃºc bá»™ nhá»› nhá» gá»n, láº¥y cáº£m há»©ng tá»« sá»± cá»§ng cá»‘ bá»™ nhá»› sinh há»c.
3.  **Adaptive Query-Aware Retrieval**: Äiá»u chá»‰nh Ä‘á»™ng pháº¡m vi truy xuáº¥t dá»±a trÃªn Ä‘á»™ phá»©c táº¡p cá»§a truy váº¥n Ä‘á»ƒ xÃ¢y dá»±ng ngá»¯ cáº£nh chÃ­nh xÃ¡c vÃ  hiá»‡u quáº£ token cho quÃ¡ trÃ¬nh suy luáº­n tiáº¿p theo.

### Main Results:
SimpleMem thá»ƒ hiá»‡n hiá»‡u suáº¥t vÆ°á»£t trá»™i so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p cÆ¡ sá»Ÿ:
*   Äáº¡t Ä‘Æ°á»£c cáº£i thiá»‡n F1 trung bÃ¬nh 26.4% so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p cÆ¡ sá»Ÿ nhÆ° Mem0.
*   Giáº£m má»©c tiÃªu thá»¥ token thá»i gian suy luáº­n tá»›i 30 láº§n so vá»›i cÃ¡c mÃ´ hÃ¬nh ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§.
*   Thá»ƒ hiá»‡n sá»± cÃ¢n báº±ng vÆ°á»£t trá»™i giá»¯a hiá»‡u suáº¥t vÃ  hiá»‡u quáº£, Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c cao vá»›i má»©c tiÃªu thá»¥ token tá»‘i thiá»ƒu (khoáº£ng 550 token).
*   VÆ°á»£t trá»™i hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n cÃ³ vá» Ä‘á»™ chÃ­nh xÃ¡c, hiá»‡u quáº£ truy xuáº¥t vÃ  chi phÃ­ suy luáº­n.

### Conclusion & Future Works:
SimpleMem thiáº¿t láº­p má»™t tiÃªu chuáº©n má»›i vá» hiá»‡u quáº£ bá»™ nhá»› cho cÃ¡c tÃ¡c nhÃ¢n LLM thÃ´ng qua nÃ©n ngá»¯ nghÄ©a cÃ³ cáº¥u trÃºc, tá»• chá»©c bá»™ nhá»› nguyÃªn táº¯c, cá»§ng cá»‘ vÃ  truy xuáº¥t thÃ­ch á»©ng. Khung nÃ y cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u quáº£ thÃ´ng tin dÆ°á»›i ngÃ¢n sÃ¡ch ngá»¯ cáº£nh vÃ  token cá»‘ Ä‘á»‹nh. VÄƒn báº£n trÃ­ch dáº«n khÃ´ng Ä‘á» cáº­p cá»¥ thá»ƒ Ä‘áº¿n cÃ¡c hÆ°á»›ng nghiÃªn cá»©u trong tÆ°Æ¡ng lai.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2601.02553](https://huggingface.co/papers/2601.02553) |
| ArXiv Abstract | [https://arxiv.org/abs/2601.02553](https://arxiv.org/abs/2601.02553) |
| PDF Download | [https://arxiv.org/pdf/2601.02553.pdf](https://arxiv.org/pdf/2601.02553.pdf) |
| Github Repository | [https://github.com/aiming-lab/SimpleMem](https://github.com/aiming-lab/SimpleMem) |

--- 

## 12. Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes

**TÃ¡c giáº£:** Jing Tan, Zhaoyang Zhang, Yantao Shen, Jiarui Cai, Shuo Yang, Jiajun Wu, Wei Xia, Zhuowen Tu, Stefano Soatto

**Xuáº¥t báº£n lÃºc:** 2026-01-05

### Main Problem:
Váº¥n Ä‘á» cá»‘t lÃµi mÃ  bÃ i bÃ¡o Ä‘á» cáº­p lÃ  viá»‡c khÃ³ khÄƒn trong thao tÃ¡c khÃ´ng gian Ä‘á»‘i tÆ°á»£ng trong cÃ¡c cáº£nh áº£nh báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn. CÃ¡c há»‡ thá»‘ng táº¡o Ä‘a phÆ°Æ¡ng thá»©c hiá»‡n cÃ³, Ä‘áº·c biá»‡t lÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p chá»‰nh sá»­a dá»±a trÃªn vÄƒn báº£n, gáº·p khÃ³ khÄƒn trong viá»‡c thá»±c hiá»‡n cÃ¡c phÃ©p biáº¿n Ä‘á»•i hÃ¬nh há»c á»Ÿ cáº¥p Ä‘á»™ Ä‘á»‘i tÆ°á»£ng nhÆ° dá»‹ch chuyá»ƒn, xoay hoáº·c thay Ä‘á»•i kÃ­ch thÆ°á»›c. Äiá»u nÃ y lÃ  do sá»± khan hiáº¿m cá»§a dá»¯ liá»‡u giÃ¡m sÃ¡t theo cáº·p vÃ  giá»›i háº¡n cá»§a viá»‡c tá»‘i Æ°u hÃ³a cáº¥p Ä‘á»™ pixel, khiáº¿n viá»‡c kiá»ƒm soÃ¡t khÃ´ng gian khÃ´ng chÃ­nh xÃ¡c vÃ  Ä‘Ã²i há»i sá»± can thiá»‡p thá»§ cÃ´ng hoáº·c chuyÃªn mÃ´n vá» Ä‘á»“ há»a.

### Main Idea:
BÃ i bÃ¡o giá»›i thiá»‡u TALK2MOVE, má»™t khung lÃ m viá»‡c dá»±a trÃªn há»c tÄƒng cÆ°á»ng (RL) sá»­ dá»¥ng mÃ´ hÃ¬nh khuáº¿ch tÃ¡n Ä‘á»ƒ thá»±c hiá»‡n cÃ¡c phÃ©p biáº¿n Ä‘á»•i khÃ´ng gian cáº¥p Ä‘á»™ Ä‘á»‘i tÆ°á»£ng theo hÆ°á»›ng dáº«n báº±ng vÄƒn báº£n. Giáº£i phÃ¡p nÃ y sá»­ dá»¥ng Group Relative Policy Optimization (GRPO) Ä‘á»ƒ khÃ¡m phÃ¡ cÃ¡c hÃ nh Ä‘á»™ng hÃ¬nh há»c thÃ´ng qua cÃ¡c lÆ°á»£t triá»ƒn khai Ä‘a dáº¡ng Ä‘Æ°á»£c táº¡o tá»« áº£nh Ä‘áº§u vÃ o vÃ  cÃ¡c biáº¿n thá»ƒ vÄƒn báº£n nháº¹, loáº¡i bá» nhu cáº§u vá» dá»¯ liá»‡u theo cáº·p tá»‘n kÃ©m. TALK2MOVE thiáº¿t káº¿ má»™t mÃ´ hÃ¬nh pháº§n thÆ°á»Ÿng khÃ´ng gian hÆ°á»›ng Ä‘á»‘i tÆ°á»£ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ trá»±c tiáº¿p hÃ nh vi dá»‹ch chuyá»ƒn, xoay vÃ  thay Ä‘á»•i kÃ­ch thÆ°á»›c, giÃºp cÃ¡c biáº¿n Ä‘á»•i dá»… hiá»ƒu vÃ  nháº¥t quÃ¡n. Äá»ƒ tÄƒng hiá»‡u quáº£ há»c táº­p, phÆ°Æ¡ng phÃ¡p nÃ y cÅ©ng sá»­ dá»¥ng Ä‘Ã¡nh giÃ¡ bÆ°á»›c ngoÃ i chÃ­nh sÃ¡ch (off-policy step evaluation) vÃ  láº¥y máº«u bÆ°á»›c chá»§ Ä‘á»™ng (active step sampling) Ä‘á»ƒ táº­p trung vÃ o cÃ¡c giai Ä‘oáº¡n biáº¿n Ä‘á»•i thÃ´ng tin, Ä‘á»“ng thá»i giá»›i thiá»‡u cÆ¡ cháº¿ thoÃ¡t sá»›m Ä‘á»ƒ tÄƒng tá»‘c quÃ¡ trÃ¬nh táº¡o lÆ°á»£t triá»ƒn khai.

### Main Results:
TALK2MOVE Ä‘áº¡t Ä‘Æ°á»£c cÃ¡c phÃ©p biáº¿n Ä‘á»•i Ä‘á»‘i tÆ°á»£ng chÃ­nh xÃ¡c, nháº¥t quÃ¡n vÃ  trung thá»±c vá» ngá»¯ nghÄ©a. CÃ¡c thá»­ nghiá»‡m trÃªn cÃ¡c bá»™ dá»¯ liá»‡u Ä‘Æ°á»£c chá»n lá»c Ä‘Ã£ chá»©ng minh ráº±ng TALK2MOVE Ä‘áº¡t káº¿t quáº£ vÆ°á»£t trá»™i so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p chá»‰nh sá»­a Ä‘Æ°á»£c hÆ°á»›ng dáº«n báº±ng vÄƒn báº£n hiá»‡n cÃ³ vá» cáº£ Ä‘á»™ chÃ­nh xÃ¡c khÃ´ng gian vÃ  tÃ­nh máº¡ch láº¡c cá»§a cáº£nh. PhÆ°Æ¡ng phÃ¡p nÃ y hiá»‡u quáº£ vá» dá»¯ liá»‡u so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn SFT, giáº£m Ä‘Ã¡ng ká»ƒ sá»± phá»¥ thuá»™c vÃ o cÃ¡c chÃº thÃ­ch theo cáº·p tá»‘n kÃ©m vÃ  cáº£i thiá»‡n hiá»‡u quáº£ Ä‘Ã o táº¡o lÃªn gáº¥p 2 láº§n thÃ´ng qua viá»‡c láº¥y máº«u bÆ°á»›c chá»§ Ä‘á»™ng.

### Conclusion & Future Works:
TALK2MOVE lÃ  khung lÃ m viá»‡c dá»±a trÃªn há»c tÄƒng cÆ°á»ng Ä‘áº§u tiÃªn giáº£i quyáº¿t bÃ i toÃ¡n biáº¿n Ä‘á»•i hÃ¬nh há»c cáº¥p Ä‘á»™ Ä‘á»‘i tÆ°á»£ng Ä‘Æ°á»£c hÆ°á»›ng dáº«n báº±ng vÄƒn báº£n, cung cáº¥p má»™t giao diá»‡n trá»±c quan vÃ  dá»… tiáº¿p cáº­n cho ngÆ°á»i dÃ¹ng. Máº·c dÃ¹ Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c nhá»¯ng káº¿t quáº£ vÆ°á»£t trá»™i vá» Ä‘á»™ chÃ­nh xÃ¡c khÃ´ng gian vÃ  tÃ­nh máº¡ch láº¡c, cÃ¡c hÆ°á»›ng nghiÃªn cá»©u trong tÆ°Æ¡ng lai cÃ³ thá»ƒ bao gá»“m viá»‡c má»Ÿ rá»™ng quy mÃ´ bá»™ dá»¯ liá»‡u cho cÃ¡c biáº¿n Ä‘á»•i hÃ¬nh há»c vÃ  tiáº¿p tá»¥c cáº£i thiá»‡n hiá»‡u quáº£ tÃ­nh toÃ¡n cá»§a quÃ¡ trÃ¬nh Ä‘Ã o táº¡o GRPO.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2601.02356](https://huggingface.co/papers/2601.02356) |
| ArXiv Abstract | [https://arxiv.org/abs/2601.02356](https://arxiv.org/abs/2601.02356) |
| PDF Download | [https://arxiv.org/pdf/2601.02356.pdf](https://arxiv.org/pdf/2601.02356.pdf) |
| Github Repository | [https://github.com/sparkstj/Talk2Move](https://github.com/sparkstj/Talk2Move) |

--- 

## 13. Confidence Estimation for LLMs in Multi-turn Interactions

**TÃ¡c giáº£:** Caiqi Zhang, Ruihan Yang, Xiaochen Zhu, Chengzu Li, Tiancheng Hu, Yijiang River Dong, Deqing Yang, Nigel Collier

**Xuáº¥t báº£n lÃºc:** 2026-01-05

### Main Problem:
NghiÃªn cá»©u hiá»‡n táº¡i vá» Æ°á»›c lÆ°á»£ng Ä‘á»™ tin cáº­y cá»§a cÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLM) chá»§ yáº¿u táº­p trung vÃ o cÃ¡c cÃ i Ä‘áº·t tÆ°Æ¡ng tÃ¡c má»™t lÆ°á»£t (single-turn). Äá»™ng lá»±c cá»§a Ä‘á»™ tin cáº­y cá»§a mÃ´ hÃ¬nh trong cÃ¡c cuá»™c há»™i thoáº¡i Ä‘a lÆ°á»£t, nÆ¡i ngá»¯ cáº£nh tÃ­ch lÅ©y vÃ  sá»± mÆ¡ há»“ dáº§n Ä‘Æ°á»£c giáº£i quyáº¿t, váº«n chÆ°a Ä‘Æ°á»£c khÃ¡m phÃ¡ rá»™ng rÃ£i. Viá»‡c Æ°á»›c lÆ°á»£ng Ä‘á»™ tin cáº­y Ä‘Ã¡ng tin cáº­y trong cÃ i Ä‘áº·t Ä‘a lÆ°á»£t lÃ  ráº¥t quan trá»ng cho nhiá»u á»©ng dá»¥ng háº¡ nguá»“n nhÆ° cÃ¡c tÃ¡c nhÃ¢n tá»± trá»‹ vÃ  há»‡ thá»‘ng cÃ³ sá»± tham gia cá»§a con ngÆ°á»i, nhÆ°ng cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n táº¡i khÃ´ng hiá»‡u quáº£ trong viá»‡c theo dÃµi sá»± tiáº¿n triá»ƒn nÃ y.

### Main Idea:
BÃ i nghiÃªn cá»©u nÃ y trÃ¬nh bÃ y má»™t nghiÃªn cá»©u há»‡ thá»‘ng Ä‘áº§u tiÃªn vá» Æ°á»›c lÆ°á»£ng Ä‘á»™ tin cáº­y trong cÃ¡c tÆ°Æ¡ng tÃ¡c Ä‘a lÆ°á»£t. CÃ´ng trÃ¬nh thiáº¿t láº­p má»™t khuÃ´n khá»• Ä‘Ã¡nh giÃ¡ chÃ­nh thá»©c dá»±a trÃªn hai tiÃªu chÃ­ chÃ­nh: hiá»‡u chuáº©n tá»«ng lÆ°á»£t (per-turn calibration) vÃ  tÃ­nh Ä‘Æ¡n Ä‘iá»‡u cá»§a Ä‘á»™ tin cáº­y (monotonicity of confidence) khi cÃ³ thÃªm thÃ´ng tin. Äá»ƒ thá»±c hiá»‡n Ä‘iá»u nÃ y, cÃ¡c tÃ¡c giáº£ giá»›i thiá»‡u cÃ¡c chá»‰ sá»‘ má»›i, bao gá»“m Lá»—i hiá»‡u chuáº©n dá»± kiáº¿n Ä‘Æ°á»£c chuáº©n hÃ³a theo Ä‘á»™ dÃ i thÃ´ng tin (InfoECE), vÃ  má»™t mÃ´ hÃ¬nh "Hinter-Guesser" má»›i Ä‘á»ƒ táº¡o cÃ¡c bá»™ dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ cÃ³ kiá»ƒm soÃ¡t. BÃ i bÃ¡o Ä‘á» xuáº¥t P(SUFFICIENT), má»™t phÆ°Æ¡ng phÃ¡p dÃ² há»i dá»±a trÃªn logit, Ä‘Ã¡nh giÃ¡ Ä‘á»™ tin cáº­y báº±ng cÃ¡ch há»i liá»‡u thÃ´ng tin hiá»‡n táº¡i cÃ³ Ä‘á»§ Ä‘á»ƒ suy ra ráº±ng má»™t cÃ¢u tráº£ lá»i lÃ  duy nháº¥t vÃ  chÃ­nh xÃ¡c hay khÃ´ng, Ä‘áº·c biá»‡t há»¯u Ã­ch trong cÃ¡c tÃ¬nh huá»‘ng cÃ¢u há»i ban Ä‘áº§u khÃ´ng rÃµ rÃ ng. CÃ¡c phÆ°Æ¡ng phÃ¡p Æ°á»›c lÆ°á»£ng Ä‘á»™ tin cáº­y khÃ¡c Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ bao gá»“m phÆ°Æ¡ng phÃ¡p verbalized (VANILLA-VERB, COT-VERB) vÃ  phÆ°Æ¡ng phÃ¡p self-consistency (SC).

### Main Results:
CÃ¡c thá»­ nghiá»‡m cho tháº¥y cÃ¡c ká»¹ thuáº­t Ä‘á»™ tin cáº­y Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i gáº·p khÃ³ khÄƒn trong viá»‡c duy trÃ¬ hiá»‡u chuáº©n hoáº·c thá»ƒ hiá»‡n tÃ­nh Ä‘Æ¡n Ä‘iá»‡u nháº¥t quÃ¡n khi cÃ¡c cuá»™c há»™i thoáº¡i tiáº¿n triá»ƒn. PhÆ°Æ¡ng phÃ¡p P(SUFFICIENT) Ä‘Æ°á»£c Ä‘á» xuáº¥t chá»©ng tá» hiá»‡u suáº¥t tá»‘t hÆ¡n má»™t cÃ¡ch tÆ°Æ¡ng Ä‘á»‘i vá» cáº£ hiá»‡u chuáº©n vÃ  tÃ­nh Ä‘Æ¡n Ä‘iá»‡u, máº·c dÃ¹ nhiá»‡m vá»¥ nÃ y váº«n cÃ²n nhiá»u khÃ´ng gian Ä‘á»ƒ cáº£i thiá»‡n. CÃ¡c mÃ´ hÃ¬nh thá»ƒ hiá»‡n tÃ­nh Ä‘Æ¡n Ä‘iá»‡u máº¡nh máº½ hÆ¡n khi Ä‘á»™ tin cáº­y Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ dá»±a trÃªn cÃ¢u tráº£ lá»i Ä‘Ãºng thá»±c táº¿ chá»© khÃ´ng pháº£i cÃ¢u tráº£ lá»i táº¡m thá»i cá»§a mÃ´ hÃ¬nh. P(SUFFICIENT) hiá»‡u quáº£ hÆ¡n trong viá»‡c phÃ¢n biá»‡t cÃ¡c thÃ´ng tin tÄƒng thÃªm cÃ³ Ã½ nghÄ©a so vá»›i cÃ¡c Ä‘oáº¡n há»™i thoáº¡i khÃ´ng cung cáº¥p giÃ¡ trá»‹ thÃ´ng tin. PhÃ¢n tÃ­ch cÅ©ng tiáº¿t lá»™ ráº±ng tÃ­n hiá»‡u Ä‘á»™ tin cáº­y cÃ³ hÃ nh vi ráº¥t khÃ¡c nhau giá»¯a Ä‘á»‘i thoáº¡i Ä‘a lÆ°á»£t vÃ  tÃ³m táº¯t má»™t lÆ°á»£t, nháº¥n máº¡nh táº§m quan trá»ng cá»§a cáº¥u trÃºc tÆ°Æ¡ng tÃ¡c cá»§a Ä‘á»‘i thoáº¡i Ä‘á»‘i vá»›i viá»‡c Æ°á»›c lÆ°á»£ng Ä‘á»™ tin cáº­y cá»§a mÃ´ hÃ¬nh.

### Conclusion & Future Works:
CÃ´ng trÃ¬nh nÃ y cung cáº¥p má»™t phÆ°Æ¡ng phÃ¡p luáº­n ná»n táº£ng Ä‘á»ƒ phÃ¡t triá»ƒn cÃ¡c tÃ¡c nhÃ¢n há»™i thoáº¡i Ä‘Ã¡ng tin cáº­y vÃ  Ä‘Ã¡ng tin cáº­y hÆ¡n. CÃ¡c phÃ¡t hiá»‡n lÃ m ná»•i báº­t ráº±ng Ä‘á»™ tin cáº­y trong tÆ°Æ¡ng tÃ¡c Ä‘a lÆ°á»£t lÃ  má»™t má»¥c tiÃªu riÃªng biá»‡t vÃ  cáº§n thiáº¿t Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c hÃ nh vi LLM Ä‘Ã¡ng tin cáº­y vÃ  Ä‘á»‹nh hÆ°á»›ng quyáº¿t Ä‘á»‹nh. Máº·c dÃ¹ P(SUFFICIENT) cáº£i thiá»‡n hiá»‡u suáº¥t, nhiá»‡m vá»¥ Æ°á»›c lÆ°á»£ng Ä‘á»™ tin cáº­y trong tÆ°Æ¡ng tÃ¡c Ä‘a lÆ°á»£t váº«n cÃ²n xa má»›i Ä‘Æ°á»£c giáº£i quyáº¿t, gá»£i Ã½ nhiá»u hÆ°á»›ng nghiÃªn cá»©u tiáº¿p theo Ä‘á»ƒ phÃ¡t triá»ƒn cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡u quáº£ hÆ¡n.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2601.02179](https://huggingface.co/papers/2601.02179) |
| ArXiv Abstract | [https://arxiv.org/abs/2601.02179](https://arxiv.org/abs/2601.02179) |
| PDF Download | [https://arxiv.org/pdf/2601.02179.pdf](https://arxiv.org/pdf/2601.02179.pdf) |
| Github Repository | N/A |

--- 

## 14. KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs

**TÃ¡c giáº£:** Yixuan Tang, Yi Yang

**Xuáº¥t báº£n lÃºc:** 2026-01-03

### Main Problem:
Váº¥n Ä‘á» cá»‘t lÃµi mÃ  bÃ i bÃ¡o Ä‘á» cáº­p lÃ  hai thÃ¡ch thá»©c cáº¥u trÃºc khi sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) chá»‰ cÃ³ bá»™ giáº£i mÃ£ (decoder-only LLM) lÃ m xÆ°Æ¡ng sá»‘ng nhÃºng vÄƒn báº£n trong mÃ´i trÆ°á»ng khÃ´ng cáº§n huáº¥n luyá»‡n (training-free):
1.  **Sá»± báº¥t Ä‘á»‘i xá»©ng thÃ´ng tin:** CÆ¡ cháº¿ chÃº Ã½ nhÃ¢n quáº£ (causal attention) háº¡n cháº¿ cÃ¡c token á»Ÿ vá»‹ trÃ­ Ä‘áº§u truy cáº­p vÃ o ngá»¯ cáº£nh sau Ä‘Ã³ trong chuá»—i, khiáº¿n chÃºng khÃ´ng nháº­n thá»©c Ä‘Æ°á»£c toÃ n bá»™ ná»™i dung.
2.  **ThiÃªn vá»‹ trong biá»ƒu diá»…n:** Má»¥c tiÃªu dá»± Ä‘oÃ¡n token tiáº¿p theo (next-token prediction objective) khiáº¿n cÃ¡c biá»ƒu diá»…n bá»‹ thiÃªn vá» táº¡o sinh hÆ¡n lÃ  nÃ©n ngá»¯ nghÄ©a, lÃ m giáº£m cháº¥t lÆ°á»£ng nhÃºng.

### Main Idea:
BÃ i bÃ¡o Ä‘á» xuáº¥t KV-Embedding, má»™t khuÃ´n khá»• khÃ´ng cáº§n huáº¥n luyá»‡n nháº±m giáº£i quyáº¿t cÃ¡c háº¡n cháº¿ trÃªn thÃ´ng qua viá»‡c Ä‘á»‹nh tuyáº¿n láº¡i tráº¡ng thÃ¡i KV ná»™i bá»™ trong LLM:
1.  **Äá»‹nh tuyáº¿n láº¡i tráº¡ng thÃ¡i Key-Value (KV) ná»™i bá»™:** PhÆ°Æ¡ng phÃ¡p nÃ y khai thÃ¡c quan sÃ¡t ráº±ng cÃ¡c tráº¡ng thÃ¡i Key vÃ  Value (KV) cá»§a token cuá»‘i cÃ¹ng á»Ÿ má»—i lá»›p mÃ£ hÃ³a má»™t cÃ¡i nhÃ¬n nÃ©n cá»§a chuá»—i. Báº±ng cÃ¡ch Ä‘á»‹nh tuyáº¿n láº¡i cÃ¡c tráº¡ng thÃ¡i nÃ y vÃ  tiá»n xá»­ lÃ½ chÃºng nhÆ° má»™t tiá»n tá»‘ ná»™i bá»™ vÃ o cÆ¡ cháº¿ chÃº Ã½, táº¥t cáº£ cÃ¡c token cÃ³ thá»ƒ truy cáº­p ngá»¯ cáº£nh cáº¥p Ä‘á»™ chuá»—i trong má»™t láº§n truyá»n tiáº¿n duy nháº¥t.
2.  **Chiáº¿n lÆ°á»£c lá»±a chá»n lá»›p tá»± Ä‘á»™ng:** Äá»ƒ Ä‘áº£m báº£o kháº£ nÄƒng Ã¡p dá»¥ng khÃ´ng phá»¥ thuá»™c vÃ o mÃ´ hÃ¬nh, bÃ i bÃ¡o giá»›i thiá»‡u má»™t chiáº¿n lÆ°á»£c lá»±a chá»n lá»›p tá»± Ä‘á»™ng dá»±a trÃªn chiá»u ná»™i táº¡i (intrinsic dimensionality), giÃºp xÃ¡c Ä‘á»‹nh cÃ¡c lá»›p tá»‘i Æ°u Ä‘á»ƒ Ä‘á»‹nh tuyáº¿n láº¡i KV, nÆ¡i cÃ¡c biá»ƒu diá»…n thá»ƒ hiá»‡n Ä‘á»™ nÃ©n tá»‘i Ä‘a.
3.  **Nháº¯c nhá»Ÿ hÆ°á»›ng nÃ©n ngá»¯ nghÄ©a:** Sá»­ dá»¥ng má»™t template nháº¯c nhá»Ÿ Ä‘áº·c biá»‡t Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ Ä‘á»‹nh hÆ°á»›ng biá»ƒu diá»…n cá»§a token cuá»‘i cÃ¹ng vá» phÃ­a cháº¯t lá»c báº£n cháº¥t ngá»¯ nghÄ©a cá»§a Ä‘áº§u vÃ o, tá»« Ä‘Ã³ giáº£m thiá»ƒu thiÃªn vá»‹ dá»± Ä‘oÃ¡n.

### Main Results:
-   **Hiá»‡u suáº¥t vÆ°á»£t trá»™i:** KV-Embedding vÆ°á»£t trá»™i hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ´ng cáº§n huáº¥n luyá»‡n hiá»‡n cÃ³ tá»›i 10% trÃªn bá»™ benchmark MTEB, sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh ná»n táº£ng nhÆ° Qwen, Mistral vÃ  Llama.
-   **Hiá»‡u suáº¥t á»•n Ä‘á»‹nh vá»›i ngá»¯ cáº£nh dÃ i:** PhÆ°Æ¡ng phÃ¡p duy trÃ¬ hiá»‡u suáº¥t máº¡nh máº½ trÃªn cÃ¡c chuá»—i dÃ i tá»›i 4.096 token trÃªn benchmark LoCoV1, má»™t ká»‹ch báº£n mÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p cÆ¡ sá»Ÿ thÆ°á»ng suy giáº£m do sá»± loÃ£ng ngá»¯ cáº£nh.
-   **KhÃ´ng gian nhÃºng cáº£i thiá»‡n:** PhÃ¢n tÃ­ch thÃªm xÃ¡c nháº­n ráº±ng phÆ°Æ¡ng phÃ¡p táº¡o ra má»™t khÃ´ng gian nhÃºng Ä‘áº³ng hÆ°á»›ng hÆ¡n vá»›i sá»± liÃªn káº¿t vÃ  Ä‘á»“ng nháº¥t Ä‘Æ°á»£c cáº£i thiá»‡n.
-   **Giáº£i phÃ¡p hiá»‡u quáº£ vÃ  cÃ³ thá»ƒ má»Ÿ rá»™ng:** CÃ¡c káº¿t quáº£ chá»©ng minh ráº±ng viá»‡c thao tÃ¡c tráº¡ng thÃ¡i ná»™i bá»™ mang láº¡i má»™t giáº£i phÃ¡p hiá»‡u quáº£ vÃ  cÃ³ thá»ƒ má»Ÿ rá»™ng Ä‘á»ƒ táº­n dá»¥ng LLM lÃ m mÃ´ hÃ¬nh nhÃºng vÄƒn báº£n.

### Conclusion & Future Works:
KV-Embedding thiáº¿t láº­p má»™t tráº¡ng thÃ¡i nghá»‡ thuáº­t má»›i cho viá»‡c nhÃºng vÄƒn báº£n khÃ´ng cáº§n huáº¥n luyá»‡n thÃ´ng qua viá»‡c Ä‘á»‹nh tuyáº¿n láº¡i KV ná»™i bá»™. Khung cÃ´ng tÃ¡c nÃ y duy trÃ¬ kháº£ nÄƒng Ã¡p dá»¥ng khÃ´ng phá»¥ thuá»™c vÃ o mÃ´ hÃ¬nh nhá» chiáº¿n lÆ°á»£c lá»±a chá»n lá»›p tá»± Ä‘á»™ng dá»±a trÃªn chiá»u ná»™i táº¡i. CÃ¡c tÃ¡c giáº£ hy vá»ng cÃ´ng trÃ¬nh nÃ y khuyáº¿n khÃ­ch khÃ¡m phÃ¡ sÃ¢u hÆ¡n vá» cÃ¡c cÆ¡ cháº¿ ná»™i bá»™ cá»§a LLM Ä‘á»ƒ há»c biá»ƒu diá»…n, chá»©ng minh ráº±ng thao tÃ¡c tráº¡ng thÃ¡i ná»™i bá»™ lÃ  má»™t giáº£i phÃ¡p thay tháº¿ hiá»‡u quáº£ cho viá»‡c sá»­a Ä‘á»•i Ä‘áº§u vÃ o.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2601.01046](https://huggingface.co/papers/2601.01046) |
| ArXiv Abstract | [https://arxiv.org/abs/2601.01046](https://arxiv.org/abs/2601.01046) |
| PDF Download | [https://arxiv.org/pdf/2601.01046.pdf](https://arxiv.org/pdf/2601.01046.pdf) |
| Github Repository | N/A |

--- 

## 15. CPPO: Contrastive Perception for Vision Language Policy Optimization

**TÃ¡c giáº£:** Ahmad Rezaei, Mohsen Gholami, Saeed Ranjbar Alvar, Kevin Cannons, Mohammad Asiful Hossain, Zhou Weimin, Shunbo Zhou, Yong Zhang, Mohammad Akbari

**Xuáº¥t báº£n lÃºc:** 2026-01-01

### Main Problem:
CÃ¡c mÃ´ hÃ¬nh VLM (Vision-Language Models) thÆ°á»ng thá»ƒ hiá»‡n hiá»‡u suáº¥t suy luáº­n Ä‘a phÆ°Æ¡ng thá»©c yáº¿u hÆ¡n so vá»›i cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ thuáº§n tÃºy. Viá»‡c má»Ÿ rá»™ng há»c tÄƒng cÆ°á»ng (RL) sang suy luáº­n Ä‘a phÆ°Æ¡ng thá»©c Ä‘Ã²i há»i cáº£i thiá»‡n cáº£ khÃ­a cáº¡nh nháº­n thá»©c (perception) vÃ  suy luáº­n (reasoning). CÃ¡c phÆ°Æ¡ng phÃ¡p trÆ°á»›c Ä‘Ã¢y Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y chá»§ yáº¿u dá»±a vÃ o pháº§n thÆ°á»Ÿng nháº­n thá»©c tÆ°á»ng minh nhÆ°ng gáº·p khÃ³ khÄƒn trong viá»‡c tÃ¡ch biá»‡t cÃ¡c token nháº­n thá»©c khá»i cÃ¡c token suy luáº­n. Cá»¥ thá»ƒ, cÃ¡c thÃ¡ch thá»©c bao gá»“m viá»‡c cáº§n thÃªm cÃ¡c mÃ´ hÃ¬nh LLM, dá»¯ liá»‡u ground-truth, Ã©p buá»™c tÃ¡ch biá»‡t giá»¯a nháº­n thá»©c vÃ  suy luáº­n trong Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh, hoáº·c Ã¡p dá»¥ng pháº§n thÆ°á»Ÿng má»™t cÃ¡ch khÃ´ng chá»n lá»c cho táº¥t cáº£ cÃ¡c token. Nhá»¯ng cÃ¡ch tiáº¿p cáº­n nÃ y cÃ³ thá»ƒ lÃ m giÃ¡n Ä‘oáº¡n quÃ¡ trÃ¬nh suy luáº­n tá»± nhiÃªn, tá»‘n kÃ©m vá» máº·t tÃ­nh toÃ¡n, khÃ´ng má»Ÿ rá»™ng Ä‘Æ°á»£c, dá»… bá»‹ "reward hacking", hoáº·c sá»­ dá»¥ng cÃ¡c hÃ m máº¥t mÃ¡t khÃ´ng á»•n Ä‘á»‹nh (vÃ­ dá»¥: KL divergence khÃ´ng giá»›i háº¡n) vÃ  Ã¡p dá»¥ng chÃºng khÃ´ng Ä‘á»“ng Ä‘á»u cho táº¥t cáº£ cÃ¡c token.

### Main Idea:
CPPO (Contrastive Perception Policy Optimization) lÃ  má»™t phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ³a chÃ­nh sÃ¡ch dá»±a trÃªn há»c tÄƒng cÆ°á»ng Ä‘á»ƒ tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh VLM. CPPO giáº£i quyáº¿t váº¥n Ä‘á» báº±ng cÃ¡ch phÃ¡t hiá»‡n cÃ¡c token nháº­n thá»©c thÃ´ng qua sá»± thay Ä‘á»•i entropy trong cÃ¡c Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh dÆ°á»›i cÃ¡c hÃ¬nh áº£nh Ä‘áº§u vÃ o bá»‹ nhiá»…u loáº¡n. Cá»¥ thá»ƒ, cÃ¡c token cÃ³ entropy tÄƒng máº¡nh nháº¥t dÆ°á»›i sá»± nhiá»…u loáº¡n loáº¡i bá» thÃ´ng tin Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh lÃ  token nháº­n thá»©c, vÃ¬ phÃ¢n phá»‘i cá»§a chÃºng thá»ƒ hiá»‡n thÃ´ng tin tÆ°Æ¡ng há»— cao nháº¥t vá»›i hÃ¬nh áº£nh. CPPO sau Ä‘Ã³ má»Ÿ rá»™ng hÃ m má»¥c tiÃªu cá»§a RL vá»›i má»™t hÃ m máº¥t mÃ¡t nháº­n thá»©c tÆ°Æ¡ng pháº£n (Contrastive Perception Loss - CPL). CPL yÃªu cáº§u sá»± nháº¥t quÃ¡n dÆ°á»›i cÃ¡c nhiá»…u loáº¡n giá»¯ thÃ´ng tin vÃ  sá»± nháº¡y cáº£m dÆ°á»›i cÃ¡c nhiá»…u loáº¡n loáº¡i bá» thÃ´ng tin. CPL lÃ  má»™t hÃ m máº¥t mÃ¡t tÆ°Æ¡ng pháº£n khÃ´ng cÃ³ giÃ¡m sÃ¡t, khÃ´ng yÃªu cáº§u giÃ¡m sÃ¡t Chain-of-Thought (CoT) bá»• sung hoáº·c cÃ¡c mÃ´ hÃ¬nh Ä‘á»™c quyá»n. NÃ³ Ä‘Æ°á»£c Ã¡p dá»¥ng dÆ°á»›i dáº¡ng hÃ m máº¥t mÃ¡t tÆ°Æ¡ng pháº£n InfoNCE, sá»­ dá»¥ng phÃ¢n phá»‘i xÃ¡c suáº¥t token vá»›i hÃ¬nh áº£nh gá»‘c lÃ m neo, hÃ¬nh áº£nh nhiá»…u loáº¡n giá»¯ thÃ´ng tin lÃ m máº«u tÃ­ch cá»±c vÃ  hÃ¬nh áº£nh nhiá»…u loáº¡n loáº¡i bá» thÃ´ng tin lÃ m máº«u tiÃªu cá»±c. Quan trá»ng lÃ , CPL chá»‰ Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c token nháº­n thá»©c tá»« cÃ¡c "rollout" Ä‘Ãºng, Ä‘áº£m báº£o ráº±ng cÃ¡c neo tÆ°Æ¡ng á»©ng vá»›i cÃ¡c token nháº­n thá»©c chÃ­nh xÃ¡c vÃ  Ä‘Ã£ Ä‘Æ°á»£c xÃ¡c minh.

### Main Results:
- CPPO vÆ°á»£t trá»™i hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p thÆ°á»Ÿng nháº­n thá»©c trÆ°á»›c Ä‘Ã¢y.
- CPPO trÃ¡nh Ä‘Æ°á»£c viá»‡c sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh bá»• sung, lÃ m cho quÃ¡ trÃ¬nh Ä‘Ã o táº¡o hiá»‡u quáº£ vÃ  cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng hÆ¡n.
- CPPO thÃ nh cÃ´ng trong viá»‡c tÃ¡ch biá»‡t vÃ  cáº£i thiá»‡n kháº£ nÄƒng nháº­n thá»©c vÃ  suy luáº­n cá»§a chÃ­nh sÃ¡ch VLM.
- CPPO giá»›i thiá»‡u má»™t phÆ°Æ¡ng phÃ¡p phÃ¡t hiá»‡n token nháº­n thá»©c dá»±a trÃªn entropy, cho phÃ©p chÃ­nh sÃ¡ch VLM tá»± xÃ¡c Ä‘á»‹nh cÃ¡c token nháº­n thá»©c cá»§a mÃ¬nh báº±ng cÃ¡ch phÃ¢n tÃ­ch phÃ¢n phá»‘i Ä‘áº§u ra cá»§a nÃ³.
- CPPO Ä‘á» xuáº¥t CPL, má»™t hÃ m máº¥t mÃ¡t tÆ°Æ¡ng pháº£n khÃ´ng giÃ¡m sÃ¡t, dÃ nh riÃªng cho nháº­n thá»©c Ä‘á»ƒ tá»‘i Æ°u hÃ³a chÃ­nh sÃ¡ch VLM.

### Conclusion & Future Works:
BÃ i nghiÃªn cá»©u giá»›i thiá»‡u CPPO nhÆ° má»™t giáº£i phÃ¡p dá»±a trÃªn há»c tÄƒng cÆ°á»ng Ä‘á»ƒ tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh VLM, cáº£i thiá»‡n kháº£ nÄƒng suy luáº­n Ä‘a phÆ°Æ¡ng thá»©c báº±ng cÃ¡ch cung cáº¥p cÆ¡ cháº¿ hiá»‡u quáº£ Ä‘á»ƒ disentangle vÃ  tá»‘i Æ°u hÃ³a cáº£ nháº­n thá»©c vÃ  suy luáº­n. CPPO vÆ°á»£t trá»™i so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p trÆ°á»›c Ä‘Ã³ nhá» kháº£ nÄƒng tá»± Ä‘á»™ng xÃ¡c Ä‘á»‹nh token nháº­n thá»©c vÃ  Ã¡p dá»¥ng hÃ m máº¥t mÃ¡t tÆ°Æ¡ng pháº£n khÃ´ng giÃ¡m sÃ¡t má»™t cÃ¡ch cÃ³ má»¥c tiÃªu, giÃºp viá»‡c Ä‘Ã o táº¡o hiá»‡u quáº£ vÃ  dá»… má»Ÿ rá»™ng. Máº·c dÃ¹ vÄƒn báº£n trÃ­ch dáº«n khÃ´ng Ä‘á» cáº­p rÃµ rÃ ng Ä‘áº¿n cÃ¡c cÃ´ng trÃ¬nh tÆ°Æ¡ng lai, phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c Ä‘á» xuáº¥t vá» hÃ m máº¥t mÃ¡t tÆ°Æ¡ng pháº£n cáº¥p token Ä‘Æ°á»£c Ã¡p dá»¥ng cá»¥ thá»ƒ cho cÃ¡c token phá»¥ thuá»™c vÃ o thá»‹ giÃ¡c má»Ÿ ra má»™t hÆ°á»›ng nghiÃªn cá»©u má»›i trong viá»‡c tá»‘i Æ°u hÃ³a chÃ­nh sÃ¡ch VLM.

### Overview Figure

![Overview Figure](papers\2026-01-06\2601.00501_overview.png)

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2601.00501](https://huggingface.co/papers/2601.00501) |
| ArXiv Abstract | [https://arxiv.org/abs/2601.00501](https://arxiv.org/abs/2601.00501) |
| PDF Download | [https://arxiv.org/pdf/2601.00501.pdf](https://arxiv.org/pdf/2601.00501.pdf) |
| Github Repository | N/A |

--- 

## 16. DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies

**TÃ¡c giáº£:** Renke Wang, Zhenyu Zhang, Ying Tai, Jian Yang

**Xuáº¥t báº£n lÃºc:** 2026-01-05

### Main Problem:
Váº¥n Ä‘á» chÃ­nh trong viá»‡c phá»¥c há»“i lÆ°á»›i ngÆ°á»i tá»« nhiá»u gÃ³c nhÃ¬n lÃ  cÃ¡c bá»™ dá»¯ liá»‡u tháº¿ giá»›i thá»±c chá»©a cÃ¡c chÃº thÃ­ch ground-truth khÃ´ng hoÃ n háº£o, gÃ¢y sai lá»‡ch cho quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh. NgÆ°á»£c láº¡i, dá»¯ liá»‡u tá»•ng há»£p máº·c dÃ¹ cÃ³ sá»± giÃ¡m sÃ¡t chÃ­nh xÃ¡c nhÆ°ng láº¡i pháº£i Ä‘á»‘i máº·t vá»›i khoáº£ng cÃ¡ch miá»n (domain gap) Ä‘Ã¡ng ká»ƒ so vá»›i dá»¯ liá»‡u thá»±c. Sá»± khan hiáº¿m dá»¯ liá»‡u Ä‘a gÃ³c nhÃ¬n Ä‘Æ°á»£c chÃº thÃ­ch cÃ²n lÃ m tráº§m trá»ng thÃªm váº¥n Ä‘á» tá»•ng quÃ¡t hÃ³a giá»¯a cÃ¡c bá»™ dá»¯ liá»‡u. CÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n cÃ³ dá»±a trÃªn dá»¯ liá»‡u tá»•ng há»£p thÆ°á»ng sá»­ dá»¥ng ká»¹ thuáº­t ngáº«u nhiÃªn hÃ³a miá»n (domain randomization) nhÆ°ng váº«n khÃ³ kháº¯c phá»¥c hoÃ n toÃ n khoáº£ng cÃ¡ch miá»n, Ä‘áº·c biá»‡t Ä‘á»‘i vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p há»“i quy trá»±c tiáº¿p.

### Main Idea:
BÃ i nghiÃªn cá»©u Ä‘á» xuáº¥t DiffProxy, má»™t framework má»›i láº¡ Ä‘á»ƒ táº¡o ra cÃ¡c "proxy" ngÆ°á»i nháº¥t quÃ¡n tá»« nhiá»u gÃ³c nhÃ¬n cho viá»‡c phá»¥c há»“i lÆ°á»›i. Giáº£i phÃ¡p cá»‘t lÃµi cá»§a DiffProxy lÃ  táº­n dá»¥ng cÃ¡c Æ°u tiÃªn táº¡o sinh dá»±a trÃªn mÃ´ hÃ¬nh khuáº¿ch tÃ¡n (diffusion-based generative priors) Ä‘á»ƒ báº¯c cáº§u giá»¯a viá»‡c huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u tá»•ng há»£p vÃ  kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a trong tháº¿ giá»›i thá»±c. CÃ¡c Ä‘á»•i má»›i chÃ­nh bao gá»“m:
1.  Má»™t cÆ¡ cháº¿ Ä‘a Ä‘iá»u kiá»‡n (multi-conditional mechanism) Ä‘á»ƒ táº¡o ra cÃ¡c proxy ngÆ°á»i nháº¥t quÃ¡n tá»« nhiá»u gÃ³c nhÃ¬n vÃ  Ä‘Æ°á»£c cÄƒn chá»‰nh tá»«ng pixel.
2.  Má»™t module tinh chá»‰nh bÃ n tay (hand refinement module) tÃ­ch há»£p cÃ¡c gá»£i Ã½ trá»±c quan linh hoáº¡t Ä‘á»ƒ nÃ¢ng cao chi tiáº¿t cá»¥c bá»™, Ä‘áº·c biá»‡t á»Ÿ cáº¥p Ä‘á»™ ngÃ³n tay.
3.  Má»™t phÆ°Æ¡ng phÃ¡p nhÃ¢n tá»· lá»‡ trong thá»i gian kiá»ƒm tra nháº­n biáº¿t Ä‘á»™ khÃ´ng cháº¯c cháº¯n (uncertainty-aware test-time scaling) Ä‘á»ƒ tÄƒng cÆ°á»ng sá»± máº¡nh máº½ cho cÃ¡c trÆ°á»ng há»£p thÃ¡ch thá»©c trong quÃ¡ trÃ¬nh tá»‘i Æ°u hÃ³a.
Framework Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»™c quyá»n trÃªn dá»¯ liá»‡u tá»•ng há»£p quy mÃ´ lá»›n vÃ  hoáº¡t Ä‘á»™ng theo hai giai Ä‘oáº¡n: (i) Táº¡o Proxy ngÆ°á»i, táº¡o ra cÃ¡c tÆ°Æ¡ng á»©ng máº­t Ä‘á»™ pixel-to-surface; (ii) Phá»¥c há»“i lÆ°á»›i ngÆ°á»i, báº±ng cÃ¡ch khá»›p mÃ´ hÃ¬nh SMPL-X vá»›i cÃ¡c proxy nÃ y thÃ´ng qua tá»‘i Æ°u hÃ³a tÃ¡i chiáº¿u. MÃ´ hÃ¬nh khuáº¿ch tÃ¡n cÅ©ng cho phÃ©p Æ°á»›c tÃ­nh Ä‘á»™ khÃ´ng cháº¯c cháº¯n trÃªn tá»«ng pixel Ä‘á»ƒ Ä‘iá»u chá»‰nh trá»ng sá»‘ tá»‘i Æ°u hÃ³a.

### Main Results:
DiffProxy, Ä‘Æ°á»£c huáº¥n luyá»‡n hoÃ n toÃ n trÃªn dá»¯ liá»‡u tá»•ng há»£p vÃ  khÃ´ng yÃªu cáº§u báº¥t ká»³ cáº·p chÃº thÃ­ch hÃ¬nh áº£nh-lÆ°á»›i thá»±c nÃ o, Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t hiá»‡n Ä‘áº¡i (state-of-the-art) trÃªn nÄƒm bá»™ benchmark tháº¿ giá»›i thá»±c. PhÆ°Æ¡ng phÃ¡p nÃ y thá»ƒ hiá»‡n kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a zero-shot máº¡nh máº½, Ä‘áº·c biá»‡t trong cÃ¡c ká»‹ch báº£n thÃ¡ch thá»©c vá»›i sá»± che khuáº¥t (occlusions) vÃ  cÃ¡c gÃ³c nhÃ¬n má»™t pháº§n (partial views).

### Conclusion & Future Works:
DiffProxy lÃ  má»™t phÆ°Æ¡ng phÃ¡p tiÃªn phong táº­n dá»¥ng cÃ¡c mÃ´ hÃ¬nh khuáº¿ch tÃ¡n Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘á»ƒ táº¡o ra cÃ¡c tÆ°Æ¡ng á»©ng máº­t Ä‘á»™ nháº¥t quÃ¡n tá»« nhiá»u gÃ³c nhÃ¬n, tá»« Ä‘Ã³ giáº£i quyáº¿t cÃ¡c thÃ¡ch thá»©c vá» chÃº thÃ­ch sai lá»‡ch vÃ  khoáº£ng cÃ¡ch miá»n trong phá»¥c há»“i lÆ°á»›i ngÆ°á»i. Báº±ng cÃ¡ch huáº¥n luyá»‡n Ä‘á»™c quyá»n trÃªn dá»¯ liá»‡u tá»•ng há»£p quy mÃ´ lá»›n, DiffProxy Ä‘áº¡t Ä‘Æ°á»£c kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a máº¡nh máº½ vÃ  hiá»‡u suáº¥t vÆ°á»£t trá»™i trÃªn cÃ¡c bá»™ dá»¯ liá»‡u thá»±c táº¿ mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u thá»±c. (KhÃ´ng cÃ³ thÃ´ng tin cá»¥ thá»ƒ vá» cÃ¡c hÆ°á»›ng nghiÃªn cá»©u tiáº¿p theo trong Ä‘oáº¡n trÃ­ch nÃ y).

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2601.02267](https://huggingface.co/papers/2601.02267) |
| ArXiv Abstract | [https://arxiv.org/abs/2601.02267](https://arxiv.org/abs/2601.02267) |
| PDF Download | [https://arxiv.org/pdf/2601.02267.pdf](https://arxiv.org/pdf/2601.02267.pdf) |
| Github Repository | [https://github.com/wrk226/DiffProxy](https://github.com/wrk226/DiffProxy) |

--- 

## 17. COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs

**TÃ¡c giáº£:** Dasol Choi, DongGeon Lee, Brigitta Jesica Kartono, Helena Berndt, Taeyoun Kwon, Joonwon Jang, Haon Park, Hwanjo Yu, Minsuk Kahng

**Xuáº¥t báº£n lÃºc:** 2026-01-05

### Main Problem:
1.  CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) Ä‘ang Ä‘Æ°á»£c triá»ƒn khai trong cÃ¡c á»©ng dá»¥ng doanh nghiá»‡p quan trá»ng (nhÆ° y táº¿, tÃ i chÃ­nh), nÆ¡i viá»‡c tuÃ¢n thá»§ cÃ¡c chÃ­nh sÃ¡ch cá»¥ thá»ƒ cá»§a tá»• chá»©c lÃ  vÃ´ cÃ¹ng cáº§n thiáº¿t.
2.  CÃ¡c Ä‘Ã¡nh giÃ¡ an toÃ n hiá»‡n cÃ³ chá»‰ táº­p trung vÃ o cÃ¡c má»‘i nguy háº¡i chung (nhÆ° Ä‘á»™c háº¡i, báº¡o lá»±c, phÃ¡t ngÃ´n thÃ¹ Ä‘á»‹ch) vÃ  khÃ´ng cÃ³ cÃ¡c giao thá»©c Ä‘Ã¡nh giÃ¡ tiÃªu chuáº©n cho viá»‡c tuÃ¢n thá»§ chÃ­nh sÃ¡ch Ä‘áº·c thÃ¹ cá»§a tá»«ng tá»• chá»©c.
3.  Äiá»u nÃ y táº¡o ra má»™t lá»— há»•ng trong viá»‡c Ä‘áº£m báº£o LLM tuÃ¢n thá»§ cÃ¡c quy táº¯c "allowlist" (Ä‘Æ°á»£c phÃ©p) vÃ  "denylist" (bá»‹ cáº¥m) do tá»• chá»©c Ä‘á»‹nh nghÄ©a, dáº«n Ä‘áº¿n rá»§i ro vá» thÃ´ng tin sai lá»‡ch, vi pháº¡m quy Ä‘á»‹nh vÃ  tá»•n háº¡i cho ngÆ°á»i dÃ¹ng.

### Main Idea:
1.  Äá» xuáº¥t COMPASS (Company/Organization Policy Alignment Assessment), má»™t khuÃ´n khá»• cÃ³ há»‡ thá»‘ng vÃ  cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng Ä‘áº§u tiÃªn Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ sá»± tuÃ¢n thá»§ chÃ­nh sÃ¡ch cá»¥ thá»ƒ cá»§a tá»• chá»©c trong cÃ¡c LLM.
2.  COMPASS nháº­n Ä‘áº§u vÃ o lÃ  táº­p há»£p cÃ¡c chÃ­nh sÃ¡ch allowlist vÃ  denylist cá»§a má»™t tá»• chá»©c (Ä‘Æ°á»£c diá»…n Ä‘áº¡t báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn) cÃ¹ng vá»›i mÃ´ táº£ ngá»¯ cáº£nh tá»• chá»©c.
3.  KhuÃ´n khá»• nÃ y tá»± Ä‘á»™ng táº¡o ra cÃ¡c truy váº¥n Ä‘Ã¡nh giÃ¡ bao gá»“m:
    *   Truy váº¥n cÆ¡ báº£n (base queries) Ä‘á»ƒ kiá»ƒm tra tuÃ¢n thá»§ thÃ´ng thÆ°á»ng.
    *   Truy váº¥n trÆ°á»ng há»£p biÃªn (edge queries) Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ kiá»ƒm tra giá»›i háº¡n cá»§a chÃ­nh sÃ¡ch, bao gá»“m cáº£ cÃ¡c trÆ°á»ng há»£p Ä‘á»‘i khÃ¡ng (adversarial) Ä‘á»ƒ phÃ¡t hiá»‡n tá»« chá»‘i sai (false positive) hoáº·c khÃ´ng tá»« chá»‘i (false negative).
4.  Sau Ä‘Ã³, COMPASS sá»­ dá»¥ng má»™t LLM lÃ m trá»ng tÃ i Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ pháº£n há»“i cá»§a chatbot vÃ  xÃ¡c Ä‘á»‹nh xem chÃºng cÃ³ tuÃ¢n thá»§ chÃ­nh sÃ¡ch hay khÃ´ng.

### Main Results:
1.  Ãp dá»¥ng COMPASS cho tÃ¡m ká»‹ch báº£n ngÃ nh Ä‘a dáº¡ng, táº¡o ra vÃ  xÃ¡c thá»±c 5.920 truy váº¥n kiá»ƒm tra cáº£ tuÃ¢n thá»§ Ä‘á»‹nh ká»³ vÃ  kháº£ nÄƒng chá»‘ng chá»‹u Ä‘á»‘i khÃ¡ng.
2.  ÄÃ¡nh giÃ¡ báº£y mÃ´ hÃ¬nh hiá»‡n Ä‘áº¡i cho tháº¥y má»™t sá»± báº¥t Ä‘á»‘i xá»©ng cÆ¡ báº£n: cÃ¡c mÃ´ hÃ¬nh xá»­ lÃ½ Ä‘Ã¡ng tin cáº­y cÃ¡c yÃªu cáº§u há»£p lá»‡ (allowlist) vá»›i Ä‘á»™ chÃ­nh xÃ¡c trÃªn 95 pháº§n trÄƒm.
3.  Tuy nhiÃªn, cÃ¡c mÃ´ hÃ¬nh tháº¥t báº¡i nghiÃªm trá»ng trong viá»‡c thá»±c thi cÃ¡c lá»‡nh cáº¥m (denylist), chá»‰ tá»« chá»‘i 13-40 pháº§n trÄƒm cÃ¡c vi pháº¡m denylist Ä‘á»‘i khÃ¡ng.
4.  Khoáº£ng cÃ¡ch nÃ y gia tÄƒng Ä‘Ã¡ng ká»ƒ trong cÃ¡c Ä‘iá»u kiá»‡n Ä‘á»‘i khÃ¡ng, vá»›i má»™t sá»‘ mÃ´ hÃ¬nh tá»« chá»‘i dÆ°á»›i 5 pháº§n trÄƒm cÃ¡c trÆ°á»ng há»£p biÃªn vi pháº¡m chÃ­nh sÃ¡ch.
5.  Nhá»¯ng káº¿t quáº£ nÃ y chá»©ng minh ráº±ng cÃ¡c LLM hiá»‡n táº¡i thiáº¿u sá»± máº¡nh máº½ cáº§n thiáº¿t cho cÃ¡c triá»ƒn khai nháº¡y cáº£m vá» chÃ­nh sÃ¡ch.

### Conclusion:
1.  CÃ¡c LLM hiá»‡n táº¡i thiáº¿u Ä‘á»™ bá»n vá»¯ng cáº§n thiáº¿t cho cÃ¡c triá»ƒn khai quan trá»ng vá» chÃ­nh sÃ¡ch.
2.  COMPASS Ä‘Æ°á»£c thiáº¿t láº­p nhÆ° má»™t khuÃ´n khá»• Ä‘Ã¡nh giÃ¡ thiáº¿t yáº¿u cho an toÃ n AI cáº¥p tá»• chá»©c.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2601.01836](https://huggingface.co/papers/2601.01836) |
| ArXiv Abstract | [https://arxiv.org/abs/2601.01836](https://arxiv.org/abs/2601.01836) |
| PDF Download | [https://arxiv.org/pdf/2601.01836.pdf](https://arxiv.org/pdf/2601.01836.pdf) |
| Github Repository | [https://github.com/AIM-Intelligence/COMPASS](https://github.com/AIM-Intelligence/COMPASS) |

--- 

## 18. SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving

**TÃ¡c giáº£:** Chaofan Tao, Jierun Chen, Yuxin Jiang, Kaiqi Kou, Shaowei Wang, Ruoyu Wang, Xiaohui Li, Sidi Yang, Yiming Du, Jianbo Dai, Zhiming Mao, Xinyu Wang, Lifeng Shang, Haoli Bai

**Xuáº¥t báº£n lÃºc:** 2026-01-04

### Main Problem:
Váº¥n Ä‘á» cá»‘t lÃµi mÃ  bÃ i bÃ¡o Ä‘á» cáº­p lÃ  lÃ m tháº¿ nÃ o Ä‘á»ƒ nÃ¢ng cao giá»›i háº¡n cá»§a phÆ°Æ¡ng phÃ¡p tinh chá»‰nh cÃ³ giÃ¡m sÃ¡t (SFT) Ä‘Æ¡n giáº£n vÃ  nháº¹ nhÃ ng Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c váº¥n Ä‘á» ká»¹ thuáº­t pháº§n má»m (SWE), thay vÃ¬ phá»¥ thuá»™c vÃ o cÃ¡c phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n phá»©c táº¡p vÃ  tá»‘n kÃ©m tÃ i nguyÃªn (nhÆ° mid-training, há»c tÄƒng cÆ°á»ng hoáº·c káº¿t há»£p cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y). NgoÃ i ra, cÃ¡c ná»— lá»±c SFT trÆ°á»›c Ä‘Ã¢y thÆ°á»ng thiáº¿u bá»™ dá»¯ liá»‡u cháº¥t lÆ°á»£ng cao bao gá»“m mÃ´i trÆ°á»ng thá»±c thi vÃ  cÃ¡c trÆ°á»ng há»£p lá»—i thá»±c táº¿ cáº§n thiáº¿t Ä‘á»ƒ táº¡o ra cÃ¡c lá»™ trÃ¬nh Ä‘a dáº¡ng vÃ  cháº¥t lÆ°á»£ng.

### Main Idea:
BÃ i bÃ¡o Ä‘á» xuáº¥t SWE-Lego, má»™t phÆ°Æ¡ng phÃ¡p tinh chá»‰nh cÃ³ giÃ¡m sÃ¡t (SFT) Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ Ä‘áº¡t hiá»‡u suáº¥t tiÃªn tiáº¿n nháº¥t trong giáº£i quyáº¿t cÃ¡c váº¥n Ä‘á» ká»¹ thuáº­t pháº§n má»m. SWE-Lego bao gá»“m ba khá»‘i xÃ¢y dá»±ng chÃ­nh:
1.  **Bá»™ dá»¯ liá»‡u SWE-Lego:** Má»™t bá»™ sÆ°u táº­p 32 nghÃ¬n trÆ°á»ng há»£p nhiá»‡m vá»¥ cháº¥t lÆ°á»£ng cao vÃ  18 nghÃ¬n lá»™ trÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c xÃ¡c thá»±c, káº¿t há»£p dá»¯ liá»‡u thá»±c táº¿ vÃ  tá»•ng há»£p Ä‘á»ƒ bá»• sung cho nhau vá» cháº¥t lÆ°á»£ng vÃ  sá»‘ lÆ°á»£ng. Dá»¯ liá»‡u nÃ y Ä‘Æ°á»£c xÃ¢y dá»±ng tá»« hÆ¡n 3000 kho lÆ°u trá»¯ thá»±c táº¿ vÃ  Ä‘Æ°á»£c xÃ¡c thá»±c nghiÃªm ngáº·t Ä‘á»ƒ ngÄƒn cháº·n cÃ¡c ká»¹ thuáº­t gian láº­n (nhÆ° Git hacking).
2.  **Quy trÃ¬nh SFT tinh chá»‰nh:** Bao gá»“m "che lá»—i tá»«ng bÆ°á»›c" (loáº¡i trá»« cÃ¡c token liÃªn quan Ä‘áº¿n lá»—i thá»±c thi khá»i tÃ­nh toÃ¡n máº¥t mÃ¡t) vÃ  "há»c theo Ä‘á»™ khÃ³" (huáº¥n luyá»‡n mÃ´ hÃ¬nh trÃªn cÃ¡c tÃ¡c vá»¥ dá»… hÆ¡n trÆ°á»›c khi chuyá»ƒn sang cÃ¡c tÃ¡c vá»¥ khÃ³ hÆ¡n).
3.  **Má»Ÿ rá»™ng táº¡i thá»i Ä‘iá»ƒm kiá»ƒm tra (TTS):** NghiÃªn cá»©u cÃ¡c chiáº¿n lÆ°á»£c TTS hiá»‡u quáº£ nháº¥t cho cÃ¡c tÃ¡c vá»¥ SWE báº±ng cÃ¡ch má»Ÿ rá»™ng tuáº§n tá»± (tÄƒng sá»‘ lÆ°á»£t tÆ°Æ¡ng tÃ¡c tá»‘i Ä‘a) vÃ  má»Ÿ rá»™ng song song (nhiá»u láº§n cháº¡y vá»›i lá»±a chá»n báº±ng trÃ¬nh xÃ¡c minh), Ä‘á»“ng thá»i tÃ¬m ra cÃ¡c chiáº¿n lÆ°á»£c tá»‘i Æ°u.

### Main Results:
*   CÃ¡c mÃ´ hÃ¬nh SWE-Lego Ä‘áº¡t hiá»‡u suáº¥t tiÃªn tiáº¿n nháº¥t trong sá»‘ cÃ¡c mÃ´ hÃ¬nh mÃ£ nguá»“n má»Ÿ cÃ³ kÃ­ch thÆ°á»›c tÆ°Æ¡ng Ä‘Æ°Æ¡ng trÃªn SWE-bench Verified.
*   SWE-Lego-Qwen3-8B Ä‘áº¡t 42.2% chá»‰ vá»›i SFT vÃ  tÄƒng lÃªn 49.6% vá»›i TTS@16.
*   SWE-Lego-Qwen3-32B Ä‘áº¡t 52.6% chá»‰ vá»›i SFT vÃ  tÄƒng lÃªn 58.8% vá»›i TTS@16.
*   Káº¿t quáº£ nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n dá»±a trÃªn Ä‘Ã¡nh giÃ¡ khÃ´ng gian láº­n Git, trÃ¡nh cÃ¡c káº¿t quáº£ bá»‹ thá»•i phá»“ng.
*   PhÃ¢n tÃ­ch chi tiáº¿t hiá»‡u suáº¥t cho tháº¥y bá»™ dá»¯ liá»‡u SWE-Lego mang láº¡i má»©c tÄƒng Ä‘Ã¡ng ká»ƒ 25.6%, quy trÃ¬nh SFT tinh chá»‰nh Ä‘Ã³ng gÃ³p thÃªm 3.8% vÃ  TTS Ä‘Ã³ng gÃ³p 6.2%, tá»•ng cá»™ng nÃ¢ng mÃ´ hÃ¬nh Qwen3-32B tá»« 23.2% lÃªn 58.8%.

### Conclusion & Future Works:
SWE-Lego chá»©ng minh ráº±ng má»™t quy trÃ¬nh SFT Ä‘Æ°á»£c thiáº¿t káº¿ tá»‰ má»‰ cÃ³ thá»ƒ cáº¡nh tranh hoáº·c vÆ°á»£t trá»™i hÆ¡n hiá»‡u suáº¥t cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n phá»©c táº¡p vÃ  tá»‘n kÃ©m hÆ¡n. BÃ i nghiÃªn cá»©u hy vá»ng SWE-Lego cÃ³ thá»ƒ cung cáº¥p má»™t mÃ´ hÃ¬nh háº­u huáº¥n luyá»‡n cÃ³ thá»ƒ tÃ¡i sáº£n xuáº¥t vÃ  nháº¹ nhÃ ng Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c tÃ¡c nhÃ¢n SWE hiá»‡u quáº£. Dá»¯ liá»‡u vÃ  cÃ¡c mÃ´ hÃ¬nh liÃªn quan cá»§a cÃ´ng trÃ¬nh nÃ y sáº½ Ä‘Æ°á»£c má»Ÿ nguá»“n cho nghiÃªn cá»©u tÆ°Æ¡ng lai trong cá»™ng Ä‘á»“ng.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2601.01426](https://huggingface.co/papers/2601.01426) |
| ArXiv Abstract | [https://arxiv.org/abs/2601.01426](https://arxiv.org/abs/2601.01426) |
| PDF Download | [https://arxiv.org/pdf/2601.01426.pdf](https://arxiv.org/pdf/2601.01426.pdf) |
| Github Repository | [https://github.com/SWE-Lego/SWE-Lego](https://github.com/SWE-Lego/SWE-Lego) |

--- 

## 19. Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion

**TÃ¡c giáº£:** Yi Zhou, Xuechao Zou, Shun Zhang, Kai Li, Shiying Wang, Jingming Chen, Congyan Lang, Tengfei Cao, Pin Tao, Yuanchun Shi

**Xuáº¥t báº£n lÃºc:** 2025-12-28

### Main Problem:
PhÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a áº£nh vá»‡ tinh bÃ¡n giÃ¡m sÃ¡t (semi-supervised remote sensing image semantic segmentation) gáº·p pháº£i váº¥n Ä‘á» cá»‘t lÃµi lÃ  "pseudo-label drift", nÆ¡i "confirmation bias" dáº«n Ä‘áº¿n sá»± tÃ­ch lÅ©y lá»—i trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c giÃ¡m sÃ¡t hoÃ n toÃ n Ä‘Ã²i há»i chÃº thÃ­ch cáº¥p pixel tá»‘n kÃ©m vÃ  máº¥t thá»i gian, dáº«n Ä‘áº¿n tÃ¬nh tráº¡ng khan hiáº¿m nhÃ£n dá»¯ liá»‡u. CÃ¡c phÆ°Æ¡ng phÃ¡p bÃ¡n giÃ¡m sÃ¡t hiá»‡n cÃ³ dá»… bá»‹ trÃ´i nhÃ£n giáº£ vÃ  gáº·p khÃ³ khÄƒn trong viá»‡c phÃ¢n biá»‡t cÃ¡c danh má»¥c ngá»¯ nghÄ©a tÆ°Æ¡ng tá»± hoáº·c xÃ¡c Ä‘á»‹nh ranh giá»›i Ä‘á»‘i tÆ°á»£ng chÃ­nh xÃ¡c, Ä‘áº·c biá»‡t trong cÃ¡c cáº£nh áº£nh vá»‡ tinh phá»©c táº¡p vÃ  khi nhÃ£n dá»¯ liá»‡u cá»±c ká»³ khan hiáº¿m.

### Main Idea:
BÃ i nghiÃªn cá»©u Ä‘á» xuáº¥t Co2S, má»™t khuÃ´n khá»• phÃ¢n Ä‘oáº¡n áº£nh vá»‡ tinh bÃ¡n giÃ¡m sÃ¡t á»•n Ä‘á»‹nh. Co2S káº¿t há»£p má»™t cÃ¡ch hiá»‡p Ä‘á»“ng cÃ¡c thÃ´ng tin tiÃªn nghiá»‡m tá»« cÃ¡c mÃ´ hÃ¬nh thá»‹ giÃ¡c-ngÃ´n ngá»¯ (vision-language models) vÃ  cÃ¡c mÃ´ hÃ¬nh tá»± giÃ¡m sÃ¡t (self-supervised models). Cá»¥ thá»ƒ, kiáº¿n trÃºc bao gá»“m hai mÃ´ hÃ¬nh ná»n táº£ng thá»‹ giÃ¡c dá»±a trÃªn ViT khÃ´ng Ä‘á»“ng nháº¥t, Ä‘Æ°á»£c khá»Ÿi táº¡o vá»›i CLIP vÃ  DINOv3 Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c, nháº±m giáº£m thiá»ƒu sá»± tÃ­ch lÅ©y lá»—i vÃ  pseudo-label drift. Co2S giá»›i thiá»‡u cÆ¡ cháº¿ "explicit-implicit semantic co-guidance" sá»­ dá»¥ng nhÃºng vÄƒn báº£n vÃ  cÃ¡c truy váº¥n cÃ³ thá»ƒ há»c Ä‘Æ°á»£c Ä‘á»ƒ cung cáº¥p hÆ°á»›ng dáº«n cáº¥p Ä‘á»™ lá»›p tÆ°á»ng minh vÃ  ngáº§m Ä‘á»‹nh. NgoÃ i ra, chiáº¿n lÆ°á»£c "global-local feature collaborative fusion" Ä‘Æ°á»£c phÃ¡t triá»ƒn Ä‘á»ƒ káº¿t há»£p thÃ´ng tin ngá»¯ cáº£nh toÃ n cá»¥c tá»« CLIP vá»›i cÃ¡c chi tiáº¿t cá»¥c bá»™ tá»« DINOv3, cho phÃ©p mÃ´ hÃ¬nh táº¡o ra káº¿t quáº£ phÃ¢n Ä‘oáº¡n cÃ³ Ä‘á»™ chÃ­nh xÃ¡c cao.

### Main Results:
CÃ¡c thá»­ nghiá»‡m rá»™ng rÃ£i trÃªn sÃ¡u bá»™ dá»¯ liá»‡u áº£nh vá»‡ tinh phá»• biáº¿n khÃ¡c nhau Ä‘Ã£ chá»©ng minh tÃ­nh Æ°u viá»‡t cá»§a phÆ°Æ¡ng phÃ¡p Co2S. Co2S liÃªn tá»¥c Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t hÃ ng Ä‘áº§u trÃªn cÃ¡c giao thá»©c phÃ¢n vÃ¹ng khÃ¡c nhau vÃ  trong cÃ¡c ká»‹ch báº£n Ä‘a dáº¡ng. Khung nÃ y cho tháº¥y hiá»‡u quáº£ vÆ°á»£t trá»™i so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p bÃ¡n giÃ¡m sÃ¡t hiá»‡n cÃ³, Ä‘áº·c biá»‡t trong cÃ¡c trÆ°á»ng há»£p chÃº thÃ­ch cá»±c ká»³ khan hiáº¿m.

### Conclusion & Future Works:
Co2S lÃ  má»™t khuÃ´n khá»• á»•n Ä‘á»‹nh cho phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a áº£nh vá»‡ tinh bÃ¡n giÃ¡m sÃ¡t, giáº£i quyáº¿t váº¥n Ä‘á» pseudo-label drift thÃ´ng qua viá»‡c káº¿t há»£p priors tá»« cÃ¡c mÃ´ hÃ¬nh thá»‹ giÃ¡c-ngÃ´n ngá»¯ vÃ  tá»± giÃ¡m sÃ¡t, sá»­ dá»¥ng cÆ¡ cháº¿ co-guidance vÃ  co-fusion.
HÆ°á»›ng nghiÃªn cá»©u tiáº¿p theo khÃ´ng Ä‘Æ°á»£c Ä‘á» cáº­p trá»±c tiáº¿p trong Ä‘oáº¡n vÄƒn Ä‘Æ°á»£c trÃ­ch dáº«n.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.23035](https://huggingface.co/papers/2512.23035) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.23035](https://arxiv.org/abs/2512.23035) |
| PDF Download | [https://arxiv.org/pdf/2512.23035.pdf](https://arxiv.org/pdf/2512.23035.pdf) |
| Github Repository | [https://github.com/XavierJiezou/Co2S](https://github.com/XavierJiezou/Co2S) |

--- 

## 20. OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment

**TÃ¡c giáº£:** Ming Zhang, Kexin Tan, Yueyuan Huang, Yujiong Shen, Chunchun Ma, Li Ju, Xinran Zhang, Yuhui Wang, Wenqing Jing, Jingyi Deng, Huayu Sha, Binze Hu, Jingqi Tong, Changhao Jiang, Yage Geng, Yuankai Ying, Yue Zhang, Zhangyue Yin, Zhiheng Xi, Shihan Dou, Tao Gui, Qi Zhang, Xuanjing Huang

**Xuáº¥t báº£n lÃºc:** 2026-01-04

DÆ°á»›i Ä‘Ã¢y lÃ  tÃ³m táº¯t bÃ i nghiÃªn cá»©u dá»±a trÃªn tiÃªu Ä‘á» vÃ  Ä‘oáº¡n vÄƒn trÃ­ch dáº«n:

### Main Problem:
Váº¥n Ä‘á» cá»‘t lÃµi mÃ  bÃ i bÃ¡o Ä‘á» cáº­p lÃ  nhá»¯ng thÃ¡ch thá»©c trong viá»‡c Ä‘Ã¡nh giÃ¡ tÃ­nh má»›i cá»§a cÃ¡c bÃ i ná»™p há»c thuáº­t trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡ ngang hÃ ng. Viá»‡c bÃ¹ng ná»• cÃ¡c áº¥n pháº©m há»c thuáº­t Ä‘Ã£ Ä‘áº·t Ã¡p lá»±c lá»›n lÃªn há»‡ thá»‘ng Ä‘Ã¡nh giÃ¡, khiáº¿n ngÆ°á»i Ä‘Ã¡nh giÃ¡ thiáº¿u thá»i gian vÃ  nÄƒng lÆ°á»£ng Ä‘á»ƒ thá»±c hiá»‡n cÃ¡c Ä‘Ã¡nh giÃ¡ ká»¹ lÆ°á»¡ng, cÃ´ng báº±ng. Viá»‡c Ä‘Ã¡nh giÃ¡ tÃ­nh má»›i Ä‘áº·c biá»‡t khÃ³ khÄƒn do lÆ°á»£ng tÃ i liá»‡u khá»•ng lá»“, khÃ³ khÄƒn trong viá»‡c xÃ¡c minh tuyÃªn bá»‘ thÃ´ng qua phÃ¢n tÃ­ch chi tiáº¿t vÃ  tÃ­nh chá»§ quan trong phÃ¡n Ä‘oÃ¡n. CÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLM) hiá»‡n cÃ³ cÅ©ng cÃ²n háº¡n cháº¿, bao gá»“m viá»‡c LLM cÃ³ thá»ƒ "áº£o giÃ¡c" cÃ¡c tÃ i liá»‡u khÃ´ng tá»“n táº¡i, chá»‰ so sÃ¡nh tiÃªu Ä‘á» vÃ  tÃ³m táº¯t, hoáº·c bá»‹ giá»›i háº¡n bá»Ÿi cá»­a sá»• ngá»¯ cáº£nh.

### Main Idea:
BÃ i bÃ¡o giá»›i thiá»‡u OpenNovelty, má»™t há»‡ thá»‘ng Ä‘áº¡i diá»‡n Ä‘Æ°á»£c há»— trá»£ bá»Ÿi LLM, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ cung cáº¥p phÃ¢n tÃ­ch tÃ­nh má»›i minh báº¡ch, cÃ³ thá»ƒ truy xuáº¥t nguá»“n gá»‘c vÃ  dá»±a trÃªn báº±ng chá»©ng cho cÃ¡c bÃ i ná»™p há»c thuáº­t quy mÃ´ lá»›n. Triáº¿t lÃ½ thiáº¿t káº¿ cá»‘t lÃµi cá»§a OpenNovelty lÃ  lÃ m cho "TÃ­nh má»›i cÃ³ thá»ƒ xÃ¡c minh Ä‘Æ°á»£c" báº±ng cÃ¡ch khÃ´ng dá»±a vÃ o kiáº¿n thá»©c tham sá»‘ cá»§a LLM mÃ  thay vÃ o Ä‘Ã³, truy xuáº¥t cÃ¡c bÃ i bÃ¡o thá»±c táº¿ vÃ  thá»±c hiá»‡n so sÃ¡nh toÃ n vÄƒn á»Ÿ cáº¥p Ä‘á»™ Ä‘Ã³ng gÃ³p Ä‘á»ƒ Ä‘áº£m báº£o má»i phÃ¡n Ä‘oÃ¡n Ä‘á»u cÃ³ báº±ng chá»©ng. Há»‡ thá»‘ng nÃ y hoáº¡t Ä‘á»™ng qua bá»‘n giai Ä‘oáº¡n: (1) trÃ­ch xuáº¥t nhiá»‡m vá»¥ cá»‘t lÃµi vÃ  cÃ¡c tuyÃªn bá»‘ Ä‘Ã³ng gÃ³p Ä‘á»ƒ táº¡o truy váº¥n; (2) truy xuáº¥t cÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y cÃ³ liÃªn quan thÃ´ng qua cÃ´ng cá»¥ tÃ¬m kiáº¿m ngá»¯ nghÄ©a; (3) xÃ¢y dá»±ng má»™t phÃ¢n loáº¡i cÃ³ há»‡ thá»‘ng vá» cÃ¡c cÃ´ng trÃ¬nh liÃªn quan Ä‘áº¿n nhiá»‡m vá»¥ cá»‘t lÃµi vÃ  thá»±c hiá»‡n so sÃ¡nh toÃ n vÄƒn á»Ÿ cáº¥p Ä‘á»™ Ä‘Ã³ng gÃ³p; vÃ  (4) tá»•ng há»£p táº¥t cáº£ cÃ¡c phÃ¢n tÃ­ch thÃ nh má»™t bÃ¡o cÃ¡o tÃ­nh má»›i cÃ³ cáº¥u trÃºc vá»›i cÃ¡c trÃ­ch dáº«n vÃ  Ä‘oáº¡n báº±ng chá»©ng rÃµ rÃ ng.

### Main Results:
Há»‡ thá»‘ng OpenNovelty Ä‘Ã£ Ä‘Æ°á»£c triá»ƒn khai Ä‘á»ƒ phÃ¢n tÃ­ch hÆ¡n 500 bÃ i ná»™p ICLR 2026, vá»›i táº¥t cáº£ cÃ¡c bÃ¡o cÃ¡o tÃ­nh má»›i Ä‘Æ°á»£c cÃ´ng khai trÃªn trang web cá»§a dá»± Ã¡n. PhÃ¢n tÃ­ch sÆ¡ bá»™ cho tháº¥y há»‡ thá»‘ng cÃ³ kháº£ nÄƒng xÃ¡c Ä‘á»‹nh cÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y cÃ³ liÃªn quan, bao gá»“m cáº£ nhá»¯ng bÃ i bÃ¡o cÃ³ liÃªn quan cháº·t cháº½ mÃ  cÃ¡c tÃ¡c giáº£ cÃ³ thá»ƒ Ä‘Ã£ bá» qua. Báº±ng cÃ¡ch dá»±a táº¥t cáº£ cÃ¡c Ä‘Ã¡nh giÃ¡ tÃ­nh má»›i vÃ o cÃ¡c bÃ i bÃ¡o thá»±c táº¿ Ä‘Æ°á»£c truy xuáº¥t, kÃ¨m theo cÃ¡c trÃ­ch dáº«n rÃµ rÃ ng vÃ  Ä‘oáº¡n báº±ng chá»©ng, OpenNovelty trÃ¡nh Ä‘Æ°á»£c váº¥n Ä‘á» "áº£o giÃ¡c" thÆ°á»ng gáº·p trong cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn LLM Ä‘Æ¡n giáº£n. Khung phÃ¢n tÃ­ch tÃ­nh má»›i cá»§a OpenNovelty tÃ­ch há»£p viá»‡c trÃ­ch xuáº¥t Ä‘Ã³ng gÃ³p, truy xuáº¥t ngá»¯ nghÄ©a, xÃ¢y dá»±ng phÃ¢n loáº¡i dá»±a trÃªn LLM vÃ  so sÃ¡nh cáº¥p Ä‘á»™ Ä‘Ã³ng gÃ³p vÃ o má»™t quy trÃ¬nh tá»± Ä‘á»™ng hoÃ n toÃ n, cung cáº¥p cho ngÆ°á»i Ä‘Ã¡nh giÃ¡ má»™t ngá»¯ cáº£nh cÃ³ cáº¥u trÃºc Ä‘á»ƒ hiá»ƒu vá»‹ trÃ­ cá»§a má»—i bÃ i bÃ¡o trong lÄ©nh vá»±c nghiÃªn cá»©u.

### Conclusion & Future Works:
OpenNovelty hÆ°á»›ng tá»›i viá»‡c trang bá»‹ cho cá»™ng Ä‘á»“ng nghiÃªn cá»©u má»™t cÃ´ng cá»¥ cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng Ä‘á»ƒ thÃºc Ä‘áº©y quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡ ngang hÃ ng cÃ´ng báº±ng, nháº¥t quÃ¡n vÃ  dá»±a trÃªn báº±ng chá»©ng. NhÃ³m nghiÃªn cá»©u cÃ³ káº¿ hoáº¡ch má»Ÿ rá»™ng phÃ¢n tÃ­ch nÃ y lÃªn hÆ¡n 2.000 bÃ i ná»™p trong cÃ¡c giai Ä‘oáº¡n tiáº¿p theo. CÃ¡c háº¡n cháº¿ liÃªn quan Ä‘áº¿n viá»‡c trÃ­ch xuáº¥t cÃ´ng thá»©c toÃ¡n há»c vÃ  ná»™i dung hÃ¬nh áº£nh cÅ©ng sáº½ Ä‘Æ°á»£c giáº£i quyáº¿t trong cÃ¡c cáº£i tiáº¿n sau nÃ y.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2601.01576](https://huggingface.co/papers/2601.01576) |
| ArXiv Abstract | [https://arxiv.org/abs/2601.01576](https://arxiv.org/abs/2601.01576) |
| PDF Download | [https://arxiv.org/pdf/2601.01576.pdf](https://arxiv.org/pdf/2601.01576.pdf) |
| Github Repository | [https://github.com/january-blue/OpenNovelty](https://github.com/january-blue/OpenNovelty) |

--- 

## 21. Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery

**TÃ¡c giáº£:** Markus J. Buehler

**Xuáº¥t báº£n lÃºc:** 2025-12-30

### Main Problem:
Váº¥n Ä‘á» cá»‘t lÃµi mÃ  bÃ i nghiÃªn cá»©u Ä‘á» cáº­p lÃ  lÃ m tháº¿ nÃ o Ä‘á»ƒ hiá»ƒu vÃ  khai thÃ¡c cÃ¡c nguyÃªn táº¯c táº¡o sinh chung giá»¯a cáº¥u trÃºc váº­t cháº¥t vÃ  logic sÃ¡ng tÃ¡c Ã¢m nháº¡c Ä‘á»ƒ thÃºc Ä‘áº©y phÃ¢n tÃ­ch, sÃ¡ng táº¡o vÃ  khÃ¡m phÃ¡. Cá»¥ thá»ƒ, thÃ¡ch thá»©c lÃ  táº¡o ra sá»± Ä‘á»•i má»›i trong khoa há»c vÃ  nghá»‡ thuáº­t khi cÃ¡c rÃ ng buá»™c hiá»‡n cÃ³ khÃ´ng thá»ƒ Ä‘Æ°á»£c thá»a mÃ£n, Ä‘Ã²i há»i pháº£i má»Ÿ rá»™ng khÃ´ng gian cáº¥u hÃ¬nh kháº£ thi, vÃ  phÃ¡t triá»ƒn cÃ¡c há»‡ thá»‘ng TrÃ­ tuá»‡ NhÃ¢n táº¡o (AI) cÃ³ kháº£ nÄƒng phÃ¡t minh thay vÃ¬ chá»‰ ná»™i suy.

### Main Idea:
BÃ i bÃ¡o giá»›i thiá»‡u "materiomusic" nhÆ° má»™t khuÃ´n khá»• táº¡o sinh, liÃªn káº¿t cÃ¡c cáº¥u trÃºc phÃ¢n cáº¥p cá»§a váº­t cháº¥t vá»›i logic cáº¥u táº¡o cá»§a Ã¢m nháº¡c. Ã tÆ°á»Ÿng chÃ­nh lÃ  sá»­ dá»¥ng cÃ¡c Ã¡nh xáº¡ thuáº­n nghá»‹ch giá»¯a cÃ¡c cáº¥u trÃºc váº­t lÃ½ (vÃ­ dá»¥: phá»• phÃ¢n tá»­, máº¡ng lÆ°á»›i ba chiá»u) vÃ  cÃ¡c cáº¥u trÃºc Ã¢m nháº¡c (vÃ­ dá»¥: Ã¢m Ä‘iá»‡u, hÃ²a Ã¢m) theo cÃ¡ch Ä‘Æ°á»£c ná»n táº£ng váº­t lÃ½. Trong khuÃ´n khá»• nÃ y, Ã¢m thanh Ä‘Ã³ng vai trÃ² lÃ  má»™t cÃ´ng cá»¥ khoa há»c, nÆ¡i viá»‡c láº¯ng nghe trá»Ÿ thÃ nh má»™t cháº¿ Ä‘á»™ "nhÃ¬n tháº¥y" vÃ  sÃ¡ng tÃ¡c Ã¢m nháº¡c trá»Ÿ thÃ nh báº£n thiáº¿t káº¿ cho váº­t cháº¥t. KhÃ¡i niá»‡m "khiáº¿m khuyáº¿t cÃ³ chá»n lá»c" (selective imperfection) Ä‘Æ°á»£c Ä‘á» xuáº¥t nhÆ° má»™t cÆ¡ cháº¿ Ä‘á»ƒ khÃ´i phá»¥c sá»± cÃ¢n báº±ng giá»¯a tÃ­nh máº¡ch láº¡c vÃ  kháº£ nÄƒng thÃ­ch á»©ng, cho phÃ©p sá»± Ä‘á»•i má»›i xuáº¥t hiá»‡n khi cÃ¡c rÃ ng buá»™c khÃ´ng thá»ƒ Ä‘Æ°á»£c Ä‘Ã¡p á»©ng trong cÃ¡c má»©c Ä‘á»™ tá»± do hiá»‡n cÃ³.

### Main Results:
*   PhÃ¢n tÃ­ch Ä‘á»‹nh lÆ°á»£ng táº¥t cáº£ 2^12 thang Ã¢m nháº¡c cho tháº¥y cÃ¡c há»‡ thá»‘ng cÃ³ Ã½ nghÄ©a vÄƒn hÃ³a táº­p trung trong má»™t "hÃ nh lang" cÃ³ Ä‘á»™ entropy trung bÃ¬nh vÃ  khuyáº¿t táº­t trung bÃ¬nh, song song vá»›i Ä‘iá»ƒm tá»‘i Æ°u Hall-Petch trong khoa há»c váº­t liá»‡u, nÆ¡i máº­t Ä‘á»™ khuyáº¿t táº­t trung gian tá»‘i Ä‘a hÃ³a Ä‘á»™ bá»n cá»§a váº­t liá»‡u.
*   CÃ¡c mÃ´ hÃ¬nh AI dá»±a trÃªn báº§y Ä‘Ã n (swarm-based AI) cÃ³ thá»ƒ sÃ¡ng tÃ¡c Ã¢m nháº¡c thá»ƒ hiá»‡n cÃ¡c Ä‘áº·c Ä‘iá»ƒm cáº¥u trÃºc giá»‘ng con ngÆ°á»i, nhÆ° káº¿t ná»‘i máº¡ng lÆ°á»›i "tháº¿ giá»›i nhá»" (small-world connectivity), tÃ­ch há»£p module vÃ  sá»± máº¡ch láº¡c táº§m xa, cho tháº¥y má»™t lá»™ trÃ¬nh vÆ°á»£t ra ngoÃ i ná»™i suy Ä‘á»ƒ hÆ°á»›ng tá»›i sá»± phÃ¡t minh.
*   KhuÃ´n khá»• materiomusic há»— trá»£ thiáº¿t káº¿ protein de novo Ä‘a phÆ°Æ¡ng thá»©c vÃ  cÃ¡c há»‡ thá»‘ng AI cÃ³ kháº£ nÄƒng sÃ¡ng táº¡o thÃ´ng qua Ä‘á»™ng lá»±c há»c táº­p thá»ƒ.
*   CÃ¡c Ã¡nh xáº¡ Ä‘Æ°á»£c xÃ¢y dá»±ng lÃ  thuáº­n nghá»‹ch vÃ  báº£o toÃ n cáº¥u trÃºc ná»™i táº¡i, cho phÃ©p dá»‹ch cÃ¡c cáº¥u trÃºc váº­t lÃ½ thÃ nh Ã¢m thanh Ä‘á»ƒ khÃ¡m phÃ¡ vÃ  ngÆ°á»£c láº¡i, sá»­ dá»¥ng khÃ´ng gian Ã¢m nháº¡c Ä‘á»ƒ thiáº¿t káº¿ cÃ¡c dáº¡ng váº­t cháº¥t má»›i.

### Conclusion & Future Works:
BÃ i nghiÃªn cá»©u káº¿t luáº­n ráº±ng khoa há»c vÃ  nghá»‡ thuáº­t lÃ  nhá»¯ng hÃ nh Ä‘á»™ng táº¡o sinh trong viá»‡c "xÃ¢y dá»±ng tháº¿ giá»›i" dÆ°á»›i cÃ¡c rÃ ng buá»™c, vá»›i dao Ä‘á»™ng (vibration) Ä‘Ã³ng vai trÃ² lÃ  ngá»¯ phÃ¡p chung tá»• chá»©c cáº¥u trÃºc trÃªn nhiá»u quy mÃ´. "Khiáº¿m khuyáº¿t cÃ³ chá»n lá»c" phá»¥c vá»¥ nhÆ° thuáº­t toÃ¡n cho phÃ©p vÅ© trá»¥ tá»± sÃ¡ng tÃ¡c. HÆ°á»›ng nghiÃªn cá»©u tiáº¿p theo bao gá»“m viá»‡c phÃ¡t triá»ƒn materiomusic nhÆ° má»™t ngÃ´n ngá»¯ tá»± nhiÃªn cho sá»± sÃ¡ng táº¡o, thá»‘ng nháº¥t cÃ¡c phÃ¢n tá»­, váº­t liá»‡u vÃ  Ã¢m thanh, cÅ©ng nhÆ° tiáº¿p tá»¥c khai thÃ¡c AI tÃ¡c nhÃ¢n Ä‘á»ƒ trá»Ÿ thÃ nh Ä‘á»‘i tÃ¡c sÃ¡ng táº¡o, vÆ°á»£t ra ngoÃ i ná»™i suy Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c nhá»¯ng phÃ¡t minh má»›i.

### Overview Figure

![Overview Figure](papers\2026-01-06\2601.00863_overview.png)

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2601.00863](https://huggingface.co/papers/2601.00863) |
| ArXiv Abstract | [https://arxiv.org/abs/2601.00863](https://arxiv.org/abs/2601.00863) |
| PDF Download | [https://arxiv.org/pdf/2601.00863.pdf](https://arxiv.org/pdf/2601.00863.pdf) |
| Github Repository | [https://github.com/lamm-mit/MusicAnalysis](https://github.com/lamm-mit/MusicAnalysis) |

--- 

## 22. IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset

**TÃ¡c giáº£:** Kumar Abhishek, Jeremy Kawahara, Ghassan Hamarneh

**Xuáº¥t báº£n lÃºc:** 2025-12-25

### Main Problem:
Váº¥n Ä‘á» cá»‘t lÃµi mÃ  bÃ i bÃ¡o Ä‘á» cáº­p lÃ  sá»± thiáº¿u há»¥t cÃ¡c bá»™ dá»¯ liá»‡u phÃ¢n Ä‘oáº¡n tá»•n thÆ°Æ¡ng da (SLS) Ä‘a ngÆ°á»i chÃº thÃ­ch quy mÃ´ lá»›n, cÃ´ng khai vÃ  cÃ³ nhÃ£n ngÆ°á»i chÃº thÃ­ch, Ä‘áº·c biá»‡t cho áº£nh soi da (dermoscopic). CÃ¡c bá»™ dá»¯ liá»‡u SLS hiá»‡n cÃ³ thÆ°á»ng chá»‰ cung cáº¥p má»™t máº·t náº¡ phÃ¢n Ä‘oáº¡n cho má»—i áº£nh hoáº·c quÃ¡ nhá» (vÃ­ dá»¥: ISIC 2019-Seg chá»‰ cÃ³ 100 áº£nh vá»›i 3 phÃ¢n Ä‘oáº¡n má»—i áº£nh) Ä‘á»ƒ nghiÃªn cá»©u hiá»‡u quáº£ cÃ¡c tÃ¡c vá»¥ cá»¥ thá»ƒ cá»§a ngÆ°á»i chÃº thÃ­ch. Viá»‡c phÃ¢n Ä‘oáº¡n tá»•n thÆ°Æ¡ng da vá»‘n Ä‘Ã£ lÃ  má»™t nhiá»‡m vá»¥ thÃ¡ch thá»©c do cÃ¡c yáº¿u tá»‘ nhÆ° hiá»‡n váº­t hÃ¬nh áº£nh, sá»± Ä‘a dáº¡ng vá» kÃ­ch thÆ°á»›c, hÃ¬nh dáº¡ng, tÃ´ng mÃ u da, Ä‘á»™ tÆ°Æ¡ng pháº£n vÃ  ranh giá»›i tá»•n thÆ°Æ¡ng khÃ´ng rÃµ rÃ ng, táº¥t cáº£ Ä‘á»u áº£nh hÆ°á»Ÿng Ä‘áº¿n viá»‡c xÃ¡c Ä‘á»‹nh má»™t "chÃ¢n lÃ½ máº·t Ä‘áº¥t" duy nháº¥t.

### Main Idea:
BÃ i bÃ¡o giá»›i thiá»‡u ISIC MultiAnnot++ (IMA++), má»™t bá»™ dá»¯ liá»‡u phÃ¢n Ä‘oáº¡n tá»•n thÆ°Æ¡ng da Ä‘a ngÆ°á»i chÃº thÃ­ch cÃ´ng khai vÃ  quy mÃ´ lá»›n, Ä‘Æ°á»£c tá»•ng há»£p tá»« Kho lÆ°u trá»¯ ISIC. IMA++ Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ giáº£i quyáº¿t sá»± thiáº¿u há»¥t cÃ¡c bá»™ dá»¯ liá»‡u Ä‘a ngÆ°á»i chÃº thÃ­ch báº±ng cÃ¡ch cung cáº¥p nhiá»u phÃ¢n Ä‘oáº¡n cho má»—i áº£nh cÃ¹ng vá»›i siÃªu dá»¯ liá»‡u chi tiáº¿t, bao gá»“m trÃ¬nh Ä‘á»™ ká»¹ nÄƒng cá»§a ngÆ°á»i chÃº thÃ­ch vÃ  cÃ´ng cá»¥ phÃ¢n Ä‘oáº¡n Ä‘Æ°á»£c sá»­ dá»¥ng. Bá»™ dá»¯ liá»‡u nÃ y mÃ´ phá»ng cÃ¡c ká»‹ch báº£n chÃº thÃ­ch thá»±c táº¿, nÆ¡i nhiá»u ngÆ°á»i chÃº thÃ­ch Ä‘Ã³ng gÃ³p vÃ o má»™t táº­p há»£p con cÃ¡c hÃ¬nh áº£nh, táº¡o ra má»™t biá»ƒu Ä‘á»“ hai phÃ­a khÃ´ng Ä‘áº§y Ä‘á»§ giá»¯a táº­p há»£p áº£nh vÃ  táº­p há»£p ngÆ°á»i chÃº thÃ­ch. BÃ i bÃ¡o cÅ©ng cung cáº¥p cÃ¡c máº·t náº¡ phÃ¢n Ä‘oáº¡n Ä‘á»“ng thuáº­n Ä‘Æ°á»£c táº¡o ra báº±ng hai thuáº­t toÃ¡n Ä‘á»“ng thuáº­n cho cÃ¡c áº£nh cÃ³ nhiá»u phÃ¢n Ä‘oáº¡n.

### Main Results:
IMA++ lÃ  bá»™ dá»¯ liá»‡u phÃ¢n Ä‘oáº¡n tá»•n thÆ°Æ¡ng da (SLS) cÃ´ng khai lá»›n nháº¥t hiá»‡n cÃ³, vá»›i tá»•ng cá»™ng 17.684 máº·t náº¡ phÃ¢n Ä‘oáº¡n trÃªn 14.967 áº£nh soi da. Trong sá»‘ Ä‘Ã³, 2.394 áº£nh soi da cÃ³ tá»« 2 Ä‘áº¿n 5 phÃ¢n Ä‘oáº¡n má»—i áº£nh, Ä‘Æ°á»£c táº¡o bá»Ÿi 16 ngÆ°á»i chÃº thÃ­ch. Khi bá»• sung cÃ¡c máº·t náº¡ phÃ¢n Ä‘oáº¡n Ä‘á»“ng thuáº­n, tá»•ng sá»‘ phÃ¢n Ä‘oáº¡n trong bá»™ dá»¯ liá»‡u tÄƒng lÃªn 22.472. Bá»™ dá»¯ liá»‡u nÃ y náº¯m báº¯t má»™t pháº¡m vi rá»™ng cÃ¡c phong cÃ¡ch phÃ¢n Ä‘oáº¡n, pháº£n Ã¡nh sá»± khÃ¡c biá»‡t vá» sá»Ÿ thÃ­ch cá»§a ngÆ°á»i chÃº thÃ­ch, cÃ´ng cá»¥ Ä‘Æ°á»£c sá»­ dá»¥ng vÃ  má»©c Ä‘á»™ ká»¹ nÄƒng. IMA++ lÃ  bá»™ dá»¯ liá»‡u Ä‘áº§u tiÃªn vÃ  lá»›n nháº¥t cung cáº¥p thÃ´ng tin vá» cÃ´ng cá»¥ vÃ  trÃ¬nh Ä‘á»™ ká»¹ nÄƒng cá»§a ngÆ°á»i chÃº thÃ­ch, cho phÃ©p khÃ¡m phÃ¡ cÃ¡ch cÃ¡c yáº¿u tá»‘ nÃ y áº£nh hÆ°á»Ÿng Ä‘áº¿n sá»± biáº¿n thiÃªn cá»§a phÃ¢n Ä‘oáº¡n.

### Conclusion & Future Works:
IMA++ lÃ  má»™t bá»™ dá»¯ liá»‡u cÃ³ giÃ¡ trá»‹ cho cÃ¡c nhÃ  nghiÃªn cá»©u lÃ m viá»‡c trÃªn nhiá»u váº¥n Ä‘á» má»Ÿ, bao gá»“m phÃ¢n loáº¡i vÃ  phÃ¢n Ä‘oáº¡n tá»•n thÆ°Æ¡ng da. NÃ³ Ä‘áº·c biá»‡t há»¯u Ã­ch cho viá»‡c mÃ´ hÃ¬nh hÃ³a sá»Ÿ thÃ­ch phÃ¢n Ä‘oáº¡n Ä‘a ngÆ°á»i chÃº thÃ­ch, mÃ´ hÃ¬nh hÃ³a sá»± Ä‘á»“ng thuáº­n cá»§a chuyÃªn gia, tÃ¬m hiá»ƒu sá»± phÃ¢n bá»‘ cÃ¡c phÃ¢n Ä‘oáº¡n, khÃ¡m phÃ¡ cÃ¡c phong cÃ¡ch phÃ¢n Ä‘oáº¡n cÆ¡ báº£n tá»« máº·t náº¡ Ä‘a ngÆ°á»i chÃº thÃ­ch, vÃ  nghiÃªn cá»©u sá»± Ä‘á»“ng thuáº­n giá»¯a cÃ¡c chuyÃªn gia. HÆ¡n ná»¯a, nÃ³ há»— trá»£ phÃ¢n tÃ­ch hÃ¬nh áº£nh da Ä‘a phÆ°Æ¡ng thá»©c (áº£nh soi da vÃ  siÃªu dá»¯ liá»‡u phong phÃº) vÃ  Ä‘a nhiá»‡m vá»¥ (cháº©n Ä‘oÃ¡n, phÃ¢n Ä‘oáº¡n, dá»± Ä‘oÃ¡n IAA). CÃ¡c hÆ°á»›ng nghiÃªn cá»©u tiáº¿p theo Ä‘Ã£ sá»­ dá»¥ng hoáº·c cÃ³ thá»ƒ sá»­ dá»¥ng IMA++ bao gá»“m viá»‡c Ä‘Ã¡nh giÃ¡ viá»‡c khÃ¡m phÃ¡ cÃ¡c phong cÃ¡ch chÃº thÃ­ch duy nháº¥t vÃ  kiá»ƒm tra má»‘i liÃªn há»‡ thá»‘ng kÃª giá»¯a má»©c Ä‘á»™ Ä‘á»“ng thuáº­n phÃ¢n Ä‘oáº¡n giá»¯a cÃ¡c ngÆ°á»i chÃº thÃ­ch vÃ  má»©c Ä‘á»™ Ã¡c tÃ­nh cá»§a tá»•n thÆ°Æ¡ng da, cÅ©ng nhÆ° kháº£ nÄƒng dá»± Ä‘oÃ¡n má»©c Ä‘á»™ Ä‘á»“ng thuáº­n tá»« áº£nh trá»±c tiáº¿p.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.21472](https://huggingface.co/papers/2512.21472) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.21472](https://arxiv.org/abs/2512.21472) |
| PDF Download | [https://arxiv.org/pdf/2512.21472.pdf](https://arxiv.org/pdf/2512.21472.pdf) |
| Github Repository | [https://github.com/sfu-mial/IMAplusplus](https://github.com/sfu-mial/IMAplusplus) |

--- 

## 23. Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping

**TÃ¡c giáº£:** Saurabh Kaushik, Lalit Maurya, Beth Tellman

**Xuáº¥t báº£n lÃºc:** 2026-01-05

### Main Problem:
CÃ¡c mÃ´ hÃ¬nh Geo-Foundation Models (GFMs) hiá»‡n táº¡i, Ä‘áº·c biá»‡t lÃ  Prithvi GFM, gáº·p khÃ³ khÄƒn trong viá»‡c vÆ°á»£t trá»™i hÆ¡n baseline U-Net trong tÃ¡c vá»¥ láº­p báº£n Ä‘á»“ ngáº­p lá»¥t, do háº¡n cháº¿ trong viá»‡c náº¯m báº¯t cÃ¡c chi tiáº¿t cá»¥c bá»™ quan trá»ng. HÆ¡n ná»¯a, kiáº¿n trÃºc cá»§a Prithvi GFM nguyÃªn báº£n chá»‰ há»— trá»£ sÃ¡u kÃªnh Ä‘áº§u vÃ o, giá»›i háº¡n kháº£ nÄƒng á»©ng dá»¥ng cá»§a nÃ³ vá»›i dá»¯ liá»‡u Ä‘a kÃªnh vÃ  Ä‘a phÆ°Æ¡ng thá»©c.

### Main Idea:
BÃ i nghiÃªn cá»©u Ä‘á» xuáº¥t Prithvi-Complementary Adaptive Fusion Encoder (CAFE), má»™t mÃ´ hÃ¬nh káº¿t há»£p bá»™ mÃ£ hÃ³a Prithvi GFM Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vá»›i má»™t nhÃ¡nh CNN song song Ä‘Æ°á»£c tÄƒng cÆ°á»ng bá»Ÿi cÃ¡c Convolutional Attention Modules (CAM). Prithvi-CAFE sá»­ dá»¥ng cÃ¡c adapter Ä‘á»ƒ tinh chá»‰nh nhanh vÃ  hiá»‡u quáº£ Prithvi, Ä‘á»“ng thá»i thá»±c hiá»‡n há»£p nháº¥t Ä‘a cáº¥p, Ä‘a tá»· lá»‡ cÃ¡c Ä‘áº·c trÆ°ng cá»§a CNN. PhÆ°Æ¡ng phÃ¡p nÃ y cho phÃ©p xá»­ lÃ½ báº¥t ká»³ sá»‘ lÆ°á»£ng kÃªnh Ä‘áº§u vÃ o nÃ o, náº¯m báº¯t cÃ¡c chi tiáº¿t cá»¥c bá»™ quan trá»ng trong khi váº«n giá»¯ Ä‘Æ°á»£c cÃ¡c phá»¥ thuá»™c táº§m xa, báº±ng cÃ¡ch phÃ¢n chia cÃ¡c kÃªnh Ä‘áº§u vÃ o thÃ nh hai táº­p bá»• sung cho Transformer vÃ  CNN.

### Main Results:
Prithvi-CAFE Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ hÃ ng Ä‘áº§u (SoTA) trÃªn hai bá»™ dá»¯ liá»‡u láº­p báº£n Ä‘á»“ ngáº­p lá»¥t: Sen1Flood11 vÃ  FloodPlanet.
- TrÃªn dá»¯ liá»‡u thá»­ nghiá»‡m Sen1Flood11, Prithvi-CAFE Ä‘áº¡t IoU 83.41, vÆ°á»£t trá»™i so vá»›i Prithvi gá»‘c (IoU 82.50) vÃ  cÃ¡c GFM khÃ¡c (TerraMind 82.90, DOFA 81.54, spectralGPT 81.02).
- TrÃªn Ä‘á»‹a Ä‘iá»ƒm thá»­ nghiá»‡m tÃ¡ch biá»‡t cá»§a Sen1Flood11, Prithvi-CAFE Ä‘áº¡t IoU 81.37, vÆ°á»£t xa baseline U-Net (70.57) vÃ  Prithvi gá»‘c (72.42).
- TrÃªn FloodPlanet, Prithvi-CAFE Ä‘áº¡t IoU 64.70, cÅ©ng vÆ°á»£t trá»™i so vá»›i U-Net (60.14), Terramind (62.33), DOFA (59.15) vÃ  Prithvi 2.0 (61.91).

### Conclusion & Future Works:
MÃ´ hÃ¬nh Prithvi-CAFE Ä‘Æ¡n giáº£n nhÆ°ng hiá»‡u quáº£ cho tháº¥y tiá»m nÄƒng máº¡nh máº½ trong viá»‡c cáº£i thiá»‡n cÃ¡c tÃ¡c vá»¥ phÃ¢n Ä‘oáº¡n, Ä‘áº·c biá»‡t lÃ  khi dá»¯ liá»‡u Ä‘a kÃªnh vÃ  Ä‘a phÆ°Æ¡ng thá»©c cung cáº¥p thÃ´ng tin bá»• sung vÃ  cÃ¡c chi tiáº¿t cá»¥c bá»™ lÃ  ráº¥t quan trá»ng. NghiÃªn cá»©u nÃ y má»Ÿ rá»™ng kháº£ nÄƒng cá»§a Prithvi GFM vÆ°á»£t ra ngoÃ i sÃ¡u kÃªnh phá»• ban Ä‘áº§u Ä‘á»ƒ xá»­ lÃ½ hiá»‡u quáº£ báº¥t ká»³ sá»‘ lÆ°á»£ng kÃªnh nÃ o vÃ  táº¡o ra cÃ¡c báº£n Ä‘á»“ lÅ© lá»¥t Ä‘Ã¡ng tin cáº­y. MÃ£ nguá»“n cá»§a Prithvi-CAFE Ä‘Ã£ Ä‘Æ°á»£c cÃ´ng bá»‘ trÃªn GitHub.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2601.02315](https://huggingface.co/papers/2601.02315) |
| ArXiv Abstract | [https://arxiv.org/abs/2601.02315](https://arxiv.org/abs/2601.02315) |
| PDF Download | [https://arxiv.org/pdf/2601.02315.pdf](https://arxiv.org/pdf/2601.02315.pdf) |
| Github Repository | [https://github.com/Sk-2103/Prithvi-CAFE](https://github.com/Sk-2103/Prithvi-CAFE) |

--- 

## 24. Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents

**TÃ¡c giáº£:** Sourena Khanzadeh

**Xuáº¥t báº£n lÃºc:** 2026-01-05

### Main Problem:
Khi cÃ¡c tÃ¡c nhÃ¢n MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLM agents) ngÃ y cÃ ng Ä‘áº£m nháº­n cÃ¡c quyáº¿t Ä‘á»‹nh tá»± Ä‘á»™ng cÃ³ rá»§i ro cao, sá»± minh báº¡ch trong quy trÃ¬nh láº­p luáº­n cá»§a chÃºng trá»Ÿ thÃ nh má»™t má»‘i quan ngáº¡i an toÃ n cáº¥p bÃ¡ch. Máº·c dÃ¹ cÃ¡c dáº¥u váº¿t Chain-of-Thought (CoT) cho phÃ©p agent táº¡o ra cÃ¡c láº­p luáº­n dá»… Ä‘á»c, nhÆ°ng khÃ´ng rÃµ liá»‡u cÃ¡c dáº¥u váº¿t nÃ y cÃ³ pháº£i lÃ  cÃ¡c Ä‘á»™ng lá»±c táº¡o ra trung thá»±c (faithful generative drivers) cho Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh hay chá»‰ Ä‘Æ¡n thuáº§n lÃ  cÃ¡c biá»‡n minh háº­u kiá»ƒm (post-hoc rationalizations). Váº¥n Ä‘á» nÃ y Ä‘Æ°á»£c gá»i lÃ  "Khoáº£ng cÃ¡ch Trung thá»±c (Faithfulness Gap)" hoáº·c "Sá»± TÃ¡ch rá»i NhÃ¢n quáº£ (Causal Decoupling)", thá»ƒ hiá»‡n má»™t tháº¥t báº¡i cÆ¡ báº£n trong AI Giáº£i thÃ­ch Ä‘Æ°á»£c (XAI) khi "suy nghÄ©" ná»™i bá»™ cá»§a agent khÃ´ng liÃªn káº¿t nhÃ¢n quáº£ vá»›i hÃ nh Ä‘á»™ng cuá»‘i cÃ¹ng.

### Main Idea:
BÃ i nghiÃªn cá»©u giá»›i thiá»‡u Project Ariadne, má»™t khung XAI má»›i sá»­ dá»¥ng MÃ´ hÃ¬nh NhÃ¢n quáº£ Cáº¥u trÃºc (Structural Causal Models - SCMs) vÃ  logic pháº£n thá»±c táº¿ (counterfactual logic) Ä‘á»ƒ kiá»ƒm tra tÃ­nh toÃ n váº¹n nhÃ¢n quáº£ cá»§a láº­p luáº­n cá»§a agent. KhÃ´ng giá»‘ng nhÆ° cÃ¡c phÆ°Æ¡ng phÃ¡p giáº£i thÃ­ch hiá»‡n cÃ³ dá»±a vÃ o sá»± tÆ°Æ¡ng Ä‘á»“ng vÄƒn báº£n á»Ÿ má»©c bá» máº·t, Project Ariadne thá»±c hiá»‡n cÃ¡c "can thiá»‡p cá»©ng (hard interventions)" (do-calculus) vÃ o cÃ¡c nÃºt láº­p luáº­n trung gianâ€”há»‡ thá»‘ng Ä‘áº£o ngÆ°á»£c logic, phá»§ Ä‘á»‹nh cÃ¡c tiá»n Ä‘á» vÃ  Ä‘áº£o ngÆ°á»£c cÃ¡c tuyÃªn bá»‘ thá»±c táº¿â€”Ä‘á»ƒ Ä‘o lÆ°á»ng "Äá»™ nháº¡y NhÃ¢n quáº£ (Causal Sensitivity)" (phi) cá»§a cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng. Khung nÃ y Ä‘á»‹nh lÆ°á»£ng tÃ­nh trung thá»±c báº±ng "Äiá»ƒm nháº¡y cáº£m nhÃ¢n quáº£ (Causal Sensitivity Score)" dá»±a trÃªn sá»± khÃ¡c biá»‡t ngá»¯ nghÄ©a giá»¯a cÃ¢u tráº£ lá»i gá»‘c vÃ  cÃ¢u tráº£ lá»i pháº£n thá»±c táº¿ sau khi can thiá»‡p.

### Main Results:
CÃ¡c Ä‘Ã¡nh giÃ¡ thá»±c nghiá»‡m trÃªn cÃ¡c mÃ´ hÃ¬nh hiá»‡n Ä‘áº¡i cho tháº¥y má»™t "Khoáº£ng cÃ¡ch Trung thá»±c" dai dáº³ng. BÃ i nghiÃªn cá»©u xÃ¡c Ä‘á»‹nh vÃ  phÃ¡t hiá»‡n má»™t cháº¿ Ä‘á»™ tháº¥t báº¡i phá»• biáº¿n Ä‘Æ°á»£c gá»i lÃ  "Sá»± TÃ¡ch rá»i NhÃ¢n quáº£", trong Ä‘Ã³ cÃ¡c agent thá»ƒ hiá»‡n "máº­t Ä‘á»™ vi pháº¡m (violation density)" (rho) lÃªn Ä‘áº¿n 0.77 trong cÃ¡c lÄ©nh vá»±c thá»±c táº¿ vÃ  khoa há»c. Trong nhá»¯ng trÆ°á»ng há»£p nÃ y, cÃ¡c agent Ä‘Æ°a ra cÃ¡c káº¿t luáº­n giá»‘ng há»‡t nhau máº·c dÃ¹ cÃ³ logic ná»™i bá»™ mÃ¢u thuáº«n, chá»©ng minh ráº±ng dáº¥u váº¿t láº­p luáº­n cá»§a chÃºng hoáº¡t Ä‘á»™ng nhÆ° "NhÃ  hÃ¡t Láº­p luáº­n (Reasoning Theater)" trong khi viá»‡c ra quyáº¿t Ä‘á»‹nh bá»‹ chi phá»‘i bá»Ÿi cÃ¡c tiÃªn nghiá»‡m tham sá»‘ tiá»m áº©n. "Máº­t Ä‘á»™ vi pháº¡m" cao nháº¥t trong Láº­p luáº­n Khoa há»c (rho = 0.96), trong khi cÃ¡c nhiá»‡m vá»¥ Logic ToÃ¡n há»c cho tháº¥y Ä‘á»™ nháº¡y cao hÆ¡n Ä‘Ã¡ng ká»ƒ (phi trung bÃ¬nh = 0.329), cho tháº¥y tÃ­nh nhÃ¢n quáº£ sÃ¢u sáº¯c hÆ¡n.

### Conclusion & Future Works:
CÃ¡c phÃ¡t hiá»‡n cho tháº¥y cÃ¡c kiáº¿n trÃºc agent hiá»‡n táº¡i vá»‘n dÄ© dá»… bá»‹ giáº£i thÃ­ch khÃ´ng trung thá»±c. Project Ariadne cung cáº¥p má»™t khung cháº©n Ä‘oÃ¡n Ä‘á»ƒ phÃ¡t hiá»‡n cÃ¡c vi pháº¡m tÃ­nh trung thá»±c. BÃ i nghiÃªn cá»©u Ä‘á» xuáº¥t "Äiá»ƒm Ariadne (Ariadne Score)" nhÆ° má»™t tiÃªu chuáº©n má»›i Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ sá»± phÃ¹ há»£p giá»¯a logic Ä‘Ã£ nÃªu vÃ  hÃ nh Ä‘á»™ng cá»§a mÃ´ hÃ¬nh. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘á»‹nh hÆ°á»›ng cho cÃ¡c nghiÃªn cá»©u vÃ  phÃ¡t triá»ƒn trong tÆ°Æ¡ng lai nháº±m cáº£i thiá»‡n tÃ­nh trung thá»±c vÃ  minh báº¡ch cá»§a cÃ¡c agent LLM.

### Overview Figure

![Overview Figure](papers\2026-01-06\2601.02314_overview.png)

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2601.02314](https://huggingface.co/papers/2601.02314) |
| ArXiv Abstract | [https://arxiv.org/abs/2601.02314](https://arxiv.org/abs/2601.02314) |
| PDF Download | [https://arxiv.org/pdf/2601.02314.pdf](https://arxiv.org/pdf/2601.02314.pdf) |
| Github Repository | [https://github.com/skhanzad/AridadneXAI](https://github.com/skhanzad/AridadneXAI) |

--- 

## 25. M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models

**TÃ¡c giáº£:** Ju-Hsuan Weng, Jia-Wei Liao, Cheng-Fu Chou, Jun-Cheng Chen

**Xuáº¥t báº£n lÃºc:** 2025-12-28

### Main Problem:
CÃ¡c mÃ´ hÃ¬nh khuáº¿ch tÃ¡n vÄƒn báº£n thÃ nh hÃ¬nh áº£nh cÃ³ thá»ƒ táº¡o ra ná»™i dung cÃ³ háº¡i hoáº·c cÃ³ báº£n quyá»n. CÃ¡c phÆ°Æ¡ng phÃ¡p xÃ³a bá» khÃ¡i niá»‡m hiá»‡n cÃ³ chá»§ yáº¿u táº­p trung vÃ o viá»‡c xÃ³a khÃ¡i niá»‡m tá»« cÃ¡c lá»i nháº¯c vÄƒn báº£n, bá» qua cÃ¡c phÆ°Æ¡ng thá»©c nháº­p liá»‡u khÃ¡c nhÆ° nhÃºng há»c Ä‘Æ°á»£c vÃ  latent Ä‘áº£o ngÆ°á»£c. Nhá»¯ng phÆ°Æ¡ng thá»©c nÃ y trá»Ÿ thÃ nh bá» máº·t táº¥n cÃ´ng, nÆ¡i cÃ¡c khÃ¡i niá»‡m Ä‘Ã£ bá»‹ xÃ³a cÃ³ thá»ƒ tÃ¡i xuáº¥t hiá»‡n, vá»›i Tá»· lá»‡ tÃ¡i táº¡o khÃ¡i niá»‡m (CRR) vÆ°á»£t quÃ¡ 90% trong thiáº¿t láº­p há»™p tráº¯ng. Äiá»u nÃ y cho tháº¥y cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n táº¡i chá»§ yáº¿u lÃ m giÃ¡n Ä‘oáº¡n sá»± liÃªn káº¿t vÄƒn báº£n-hÃ¬nh áº£nh thay vÃ¬ loáº¡i bá» hoÃ n toÃ n cÃ¡c khÃ¡i niá»‡m.

### Main Idea:
BÃ i nghiÃªn cá»©u giá»›i thiá»‡u M-ErasureBench, má»™t khuÃ´n khá»• Ä‘Ã¡nh giÃ¡ Ä‘a phÆ°Æ¡ng thá»©c má»›i, toÃ n diá»‡n Ä‘á»ƒ cháº¥m Ä‘iá»ƒm cÃ¡c phÆ°Æ¡ng phÃ¡p xÃ³a bá» khÃ¡i niá»‡m trÃªn ba phÆ°Æ¡ng thá»©c nháº­p liá»‡u: lá»i nháº¯c vÄƒn báº£n, nhÃºng há»c Ä‘Æ°á»£c vÃ  latent Ä‘áº£o ngÆ°á»£c. Äá»‘i vá»›i hai phÆ°Æ¡ng thá»©c sau, nghiÃªn cá»©u Ä‘Ã¡nh giÃ¡ cáº£ truy cáº­p há»™p tráº¯ng vÃ  há»™p Ä‘en, táº¡o ra nÄƒm ká»‹ch báº£n Ä‘Ã¡nh giÃ¡. Äá»ƒ giáº£i quyáº¿t cÃ¡c lá»— há»•ng, bÃ i nghiÃªn cá»©u Ä‘á» xuáº¥t IRECE (Inference-time Robustness Enhancement for Concept Erasure), má»™t mÃ´-Ä‘un cáº¯m-vÃ -cháº¡y giÃºp Ä‘á»‹nh vá»‹ cÃ¡c khÃ¡i niá»‡m má»¥c tiÃªu thÃ´ng qua cÆ¡ cháº¿ chÃº Ã½ chÃ©o vÃ  lÃ m nhiá»…u cÃ¡c latent liÃªn quan trong quÃ¡ trÃ¬nh khá»­ nhiá»…u, mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n láº¡i.

### Main Results:
PhÃ¢n tÃ­ch cho tháº¥y cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n cÃ³ Ä‘áº¡t hiá»‡u suáº¥t xÃ³a bá» máº¡nh máº½ Ä‘á»‘i vá»›i lá»i nháº¯c vÄƒn báº£n nhÆ°ng pháº§n lá»›n tháº¥t báº¡i dÆ°á»›i cÃ¡c nhÃºng há»c Ä‘Æ°á»£c vÃ  latent Ä‘áº£o ngÆ°á»£c, vá»›i CRR vÆ°á»£t quÃ¡ 90% trong thiáº¿t láº­p há»™p tráº¯ng. IRECE Ä‘Ã£ chá»©ng minh ráº±ng nÃ³ khÃ´i phá»¥c kháº£ nÄƒng chá»‘ng chá»‹u má»™t cÃ¡ch nháº¥t quÃ¡n, giáº£m CRR tá»›i 40% trong ká»‹ch báº£n Ä‘áº£o ngÆ°á»£c latent há»™p tráº¯ng khÃ³ khÄƒn nháº¥t, Ä‘á»“ng thá»i duy trÃ¬ cháº¥t lÆ°á»£ng hÃ¬nh áº£nh. M-ErasureBench cung cáº¥p tiÃªu chuáº©n Ä‘Ã¡nh giÃ¡ toÃ n diá»‡n Ä‘áº§u tiÃªn vá» xÃ³a bá» khÃ¡i niá»‡m ngoÃ i lá»i nháº¯c vÄƒn báº£n.

### Conclusion & Future Works:
M-ErasureBench lÃ  tiÃªu chuáº©n Ä‘Ã¡nh giÃ¡ Ä‘a phÆ°Æ¡ng thá»©c toÃ n diá»‡n Ä‘áº§u tiÃªn cho viá»‡c xÃ³a bá» khÃ¡i niá»‡m trong cÃ¡c mÃ´ hÃ¬nh khuáº¿ch tÃ¡n. NghiÃªn cá»©u nÃ y lÃ m ná»•i báº­t cÃ¡c lá»— há»•ng cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n cÃ³ Ä‘á»‘i vá»›i cÃ¡c phÆ°Æ¡ng thá»©c táº¥n cÃ´ng thá»i gian suy luáº­n ngoÃ i vÄƒn báº£n. CÃ¹ng vá»›i IRECE, má»™t mÃ´-Ä‘un plug-and-play giÃºp tÄƒng cÆ°á»ng kháº£ nÄƒng chá»‘ng chá»‹u mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n láº¡i, nghiÃªn cá»©u cung cáº¥p cÃ¡c biá»‡n phÃ¡p báº£o vá»‡ thiáº¿t thá»±c Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh táº¡o sinh Ä‘Ã¡ng tin cáº­y hÆ¡n vÃ  an toÃ n hÆ¡n.

### CÃ¡c Ä‘Æ°á»ng dáº«n liÃªn quan

| Ná»n táº£ng | ÄÆ°á»ng dáº«n |
| :--- | :--- |
| Hugging Face | [https://huggingface.co/papers/2512.22877](https://huggingface.co/papers/2512.22877) |
| ArXiv Abstract | [https://arxiv.org/abs/2512.22877](https://arxiv.org/abs/2512.22877) |
| PDF Download | [https://arxiv.org/pdf/2512.22877.pdf](https://arxiv.org/pdf/2512.22877.pdf) |
| Github Repository | N/A |

